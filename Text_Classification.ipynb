{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "05c9264531b2438fa816dc1541003a50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_44ede988f74948b8b57ef76097282ba9",
              "IPY_MODEL_344ae37cc82942a1b16b968d75ce4947",
              "IPY_MODEL_6fc58efb927846cbb3c3b14a4b47c9bd"
            ],
            "layout": "IPY_MODEL_a52e49fcff834f8ca04a26176001240e"
          }
        },
        "44ede988f74948b8b57ef76097282ba9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c17287c8a35c4f53b4be1e847e83e71e",
            "placeholder": "​",
            "style": "IPY_MODEL_d93b990d4e0946a58e81f223673ff8c4",
            "value": "Dl Completed...: 100%"
          }
        },
        "344ae37cc82942a1b16b968d75ce4947": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6783dab12fac4ff68695ac4fa5f52e9f",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_470cd1d981f4439795202d814f5956c2",
            "value": 1
          }
        },
        "6fc58efb927846cbb3c3b14a4b47c9bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c24c6769a86448859063dae75dbf563f",
            "placeholder": "​",
            "style": "IPY_MODEL_cd4ad1fd129c4f67a4a58698b929fbf9",
            "value": " 1/1 [00:04&lt;00:00,  4.93s/ url]"
          }
        },
        "a52e49fcff834f8ca04a26176001240e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c17287c8a35c4f53b4be1e847e83e71e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d93b990d4e0946a58e81f223673ff8c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6783dab12fac4ff68695ac4fa5f52e9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "470cd1d981f4439795202d814f5956c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c24c6769a86448859063dae75dbf563f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd4ad1fd129c4f67a4a58698b929fbf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4863a3478a34c208483d27aacad2c4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2c2f459916de4668b7d34c5a64c7be2a",
              "IPY_MODEL_3131fae60b844dcdba69ff567ca33cbc",
              "IPY_MODEL_0f67c0801d564d9aa1e08ede8934ad62"
            ],
            "layout": "IPY_MODEL_0c6ff0f2baf64f1c88fbec5178ab93df"
          }
        },
        "2c2f459916de4668b7d34c5a64c7be2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db928d479c6648d8a3ff7c410860e7af",
            "placeholder": "​",
            "style": "IPY_MODEL_26fffc0f632c45a29a04859eaace4fc0",
            "value": "Dl Size...: 100%"
          }
        },
        "3131fae60b844dcdba69ff567ca33cbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a827664489614b5dbb39593f9bd153b6",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cfeb93228e5146f9af5edc2a3c25fa6b",
            "value": 1
          }
        },
        "0f67c0801d564d9aa1e08ede8934ad62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43a67878959348e9967989e71afba634",
            "placeholder": "​",
            "style": "IPY_MODEL_576f55c3a39647aea34e80f50b8f75d2",
            "value": " 80/80 [00:04&lt;00:00, 37.42 MiB/s]"
          }
        },
        "0c6ff0f2baf64f1c88fbec5178ab93df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db928d479c6648d8a3ff7c410860e7af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26fffc0f632c45a29a04859eaace4fc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a827664489614b5dbb39593f9bd153b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "cfeb93228e5146f9af5edc2a3c25fa6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "43a67878959348e9967989e71afba634": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "576f55c3a39647aea34e80f50b8f75d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af44fff89ad94871b2a73de0c04216e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cc61de5ccc7146cab34b9768410301a9",
              "IPY_MODEL_fc2cc68936414b9094ffd1395e37aa40",
              "IPY_MODEL_f54262dba2f54abf8f9e19107d6b290e"
            ],
            "layout": "IPY_MODEL_5357da513c4140eb8f14ce270dfd4eba"
          }
        },
        "cc61de5ccc7146cab34b9768410301a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03ebfa6d0331445e81c4ca3c85a81e9d",
            "placeholder": "​",
            "style": "IPY_MODEL_cdfef717f04e458c93002166b468a99a",
            "value": "Generating splits...: 100%"
          }
        },
        "fc2cc68936414b9094ffd1395e37aa40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba524f455c8c4c80bc737ad48b272d8e",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3cde21a50e734f42bc9c258204d1c421",
            "value": 3
          }
        },
        "f54262dba2f54abf8f9e19107d6b290e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ac23a0cbb5240699a0b8364c7cc0114",
            "placeholder": "​",
            "style": "IPY_MODEL_f614ba112e3949578d882ce3edd3a348",
            "value": " 3/3 [00:48&lt;00:00, 15.04s/ splits]"
          }
        },
        "5357da513c4140eb8f14ce270dfd4eba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "03ebfa6d0331445e81c4ca3c85a81e9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdfef717f04e458c93002166b468a99a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba524f455c8c4c80bc737ad48b272d8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cde21a50e734f42bc9c258204d1c421": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4ac23a0cbb5240699a0b8364c7cc0114": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f614ba112e3949578d882ce3edd3a348": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea65b24fcdc342b080f3d9acde5d3777": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_052e2f01b2e34436bb37f80cd22d2d20",
              "IPY_MODEL_0262eff1b9ec4b36846d884687bbf10a",
              "IPY_MODEL_7733820063a2456096de2d4411817c92"
            ],
            "layout": "IPY_MODEL_c4f32cc57fb74e10b348fae53900aaaf"
          }
        },
        "052e2f01b2e34436bb37f80cd22d2d20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_525f4be1131042659195bb4e9187614e",
            "placeholder": "​",
            "style": "IPY_MODEL_0eae2395905e49eeaee02802d8eee6ed",
            "value": "Generating train examples...: "
          }
        },
        "0262eff1b9ec4b36846d884687bbf10a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "info",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d65105c59ea45dbbc9b8bcb349687cd",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1ae041f806504b92892e3ccb23db24d6",
            "value": 1
          }
        },
        "7733820063a2456096de2d4411817c92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac262c0baaf9473d8a32216b77cf4741",
            "placeholder": "​",
            "style": "IPY_MODEL_db1028da0b1246f2b3f2d2ef044e7b45",
            "value": " 24663/? [00:17&lt;00:00, 1440.38 examples/s]"
          }
        },
        "c4f32cc57fb74e10b348fae53900aaaf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "525f4be1131042659195bb4e9187614e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0eae2395905e49eeaee02802d8eee6ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d65105c59ea45dbbc9b8bcb349687cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "1ae041f806504b92892e3ccb23db24d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ac262c0baaf9473d8a32216b77cf4741": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db1028da0b1246f2b3f2d2ef044e7b45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5605a01073f14fd4aa50e1237cd06e64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_875749aceda341aead6dc9828a16f100",
              "IPY_MODEL_b110c5a1fa0849fbaae713f836c282d1",
              "IPY_MODEL_a5d9f8a3295344a1bd0a0e7d32d2ce04"
            ],
            "layout": "IPY_MODEL_d5ae1c9637af4b11beac32cbb34538ad"
          }
        },
        "875749aceda341aead6dc9828a16f100": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04cab46f24de4660a07d3b08300f9b0d",
            "placeholder": "​",
            "style": "IPY_MODEL_13412970040243eea8f5026421f3a25b",
            "value": "Shuffling /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteGQ9H9L/imdb_reviews-train.tfrecord*...:  64%"
          }
        },
        "b110c5a1fa0849fbaae713f836c282d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c89dfdfea414366b2430272031ec4a3",
            "max": 25000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b1bbab49041e40ac9049123404bc86a8",
            "value": 25000
          }
        },
        "a5d9f8a3295344a1bd0a0e7d32d2ce04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c879489c95ad4dacbebca7a226d3e1c6",
            "placeholder": "​",
            "style": "IPY_MODEL_3afd7fdafe2140b3928ecc4678021d24",
            "value": " 15875/25000 [00:00&lt;00:00, 158731.23 examples/s]"
          }
        },
        "d5ae1c9637af4b11beac32cbb34538ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "04cab46f24de4660a07d3b08300f9b0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13412970040243eea8f5026421f3a25b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c89dfdfea414366b2430272031ec4a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1bbab49041e40ac9049123404bc86a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c879489c95ad4dacbebca7a226d3e1c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3afd7fdafe2140b3928ecc4678021d24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4740184ea380495e89e8ff51fe380e16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d381feb7836e4f22a2b0b1ab3de62b16",
              "IPY_MODEL_d59d469265314a97ab6de0a2582b1077",
              "IPY_MODEL_12c8812f93e147f692173af8f0c9d681"
            ],
            "layout": "IPY_MODEL_8ecd4025dba243139f740360c1798875"
          }
        },
        "d381feb7836e4f22a2b0b1ab3de62b16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4f9ac69ebf74e9e861485c43c900b74",
            "placeholder": "​",
            "style": "IPY_MODEL_7cdf146f3c994232a4ba694e358b508e",
            "value": "Generating test examples...: "
          }
        },
        "d59d469265314a97ab6de0a2582b1077": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "info",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9f1dc5b1a2d42f69e00128d9e8d276d",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2bb8dd1d04544ac1928f1f33aaa8a6f3",
            "value": 1
          }
        },
        "12c8812f93e147f692173af8f0c9d681": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e332b205b6294c45ad3f16fe44aab590",
            "placeholder": "​",
            "style": "IPY_MODEL_5e3c8c6508e8474990bf28f86837ee18",
            "value": " 24828/? [00:04&lt;00:00, 5522.46 examples/s]"
          }
        },
        "8ecd4025dba243139f740360c1798875": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "c4f9ac69ebf74e9e861485c43c900b74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7cdf146f3c994232a4ba694e358b508e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f9f1dc5b1a2d42f69e00128d9e8d276d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "2bb8dd1d04544ac1928f1f33aaa8a6f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e332b205b6294c45ad3f16fe44aab590": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e3c8c6508e8474990bf28f86837ee18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c1c944282954089a2ff5247767b7e0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_77635222213841a0808fb1771c86c0e9",
              "IPY_MODEL_7351c9f3f00d4644ab7c85fb8765196d",
              "IPY_MODEL_8eec3022d5814fd3a74cac8b76febaff"
            ],
            "layout": "IPY_MODEL_4153ae6ab56c4fa5b24cd52804b783d3"
          }
        },
        "77635222213841a0808fb1771c86c0e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a44eeb4b0e574e788405a1beb8d4e482",
            "placeholder": "​",
            "style": "IPY_MODEL_9455528d83444decbd7ebe8532ee0736",
            "value": "Shuffling /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteGQ9H9L/imdb_reviews-test.tfrecord*...:  38%"
          }
        },
        "7351c9f3f00d4644ab7c85fb8765196d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3fa1f2783e44665a40950bb4aeadcc6",
            "max": 25000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b6a1c13c0feb47e7acfd782748a5f29b",
            "value": 25000
          }
        },
        "8eec3022d5814fd3a74cac8b76febaff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1574cd34b3684c7db6bc06f7a4f11a5b",
            "placeholder": "​",
            "style": "IPY_MODEL_d4ceaa32ba0f4b04a80acad486738865",
            "value": " 9532/25000 [00:00&lt;00:00, 95306.23 examples/s]"
          }
        },
        "4153ae6ab56c4fa5b24cd52804b783d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "a44eeb4b0e574e788405a1beb8d4e482": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9455528d83444decbd7ebe8532ee0736": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c3fa1f2783e44665a40950bb4aeadcc6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6a1c13c0feb47e7acfd782748a5f29b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1574cd34b3684c7db6bc06f7a4f11a5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4ceaa32ba0f4b04a80acad486738865": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d992250990104c199ba804007d915301": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7ff1caf4643c4b83b09f793a0fc7a45a",
              "IPY_MODEL_f600692df3fc4098940e1db4a1cd25bb",
              "IPY_MODEL_877acaf9a7564d14936f1648b83253ae"
            ],
            "layout": "IPY_MODEL_1445917c78394084aee39192657037e1"
          }
        },
        "7ff1caf4643c4b83b09f793a0fc7a45a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41d8a6057e6d4586809002e63325adb0",
            "placeholder": "​",
            "style": "IPY_MODEL_12f982ff3bc94e3f9712c60b5129f69f",
            "value": "Generating unsupervised examples...: "
          }
        },
        "f600692df3fc4098940e1db4a1cd25bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "info",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e6bff609fd240d592658725b512a82a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6bad8a4a1df040e78997359511c99501",
            "value": 1
          }
        },
        "877acaf9a7564d14936f1648b83253ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db9981680e4543179cc529a36c467c5c",
            "placeholder": "​",
            "style": "IPY_MODEL_c58b4f45e05f4e4fb14d971c1c1fff69",
            "value": " 49635/? [00:13&lt;00:00, 5466.75 examples/s]"
          }
        },
        "1445917c78394084aee39192657037e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "41d8a6057e6d4586809002e63325adb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12f982ff3bc94e3f9712c60b5129f69f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0e6bff609fd240d592658725b512a82a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "6bad8a4a1df040e78997359511c99501": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "db9981680e4543179cc529a36c467c5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c58b4f45e05f4e4fb14d971c1c1fff69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1250dac4e74442d5b57690bf20a1eec5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cc1cdd7256774c819bbb06baf655f039",
              "IPY_MODEL_713a9ed983dd45e3bac0f827ec58facd",
              "IPY_MODEL_ef451fd1d17c4c93b348c231749c4a7b"
            ],
            "layout": "IPY_MODEL_0c955c9952d84501a4ad9b5cb7e6f16b"
          }
        },
        "cc1cdd7256774c819bbb06baf655f039": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1aeca08b30ab4a6e9fb550f6ebc9ca34",
            "placeholder": "​",
            "style": "IPY_MODEL_4d97c9f074bf4e69bd55afbfda5c751a",
            "value": "Shuffling /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteGQ9H9L/imdb_reviews-unsupervised.tfrecord*...:  76%"
          }
        },
        "713a9ed983dd45e3bac0f827ec58facd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6cbbc4d242f4605a5b90ef3c4dc466e",
            "max": 50000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_45feda96d23747b5bd137c2a17b4a138",
            "value": 50000
          }
        },
        "ef451fd1d17c4c93b348c231749c4a7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30c6419212524ec7b25208307f6bc9ae",
            "placeholder": "​",
            "style": "IPY_MODEL_6e24e134588248338e61b35b731c2294",
            "value": " 37784/50000 [00:00&lt;00:00, 201528.28 examples/s]"
          }
        },
        "0c955c9952d84501a4ad9b5cb7e6f16b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "1aeca08b30ab4a6e9fb550f6ebc9ca34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d97c9f074bf4e69bd55afbfda5c751a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a6cbbc4d242f4605a5b90ef3c4dc466e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45feda96d23747b5bd137c2a17b4a138": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "30c6419212524ec7b25208307f6bc9ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e24e134588248338e61b35b731c2294": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Milad-Khanchi/Text_Classification/blob/main/Text_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Naive Bayes model:"
      ],
      "metadata": {
        "id": "AzC9P14wHSTN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3IFG2gftFq0"
      },
      "source": [
        "#Bernoulli Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-scmhSqltFq2",
        "outputId": "535be8fa-d74b-467a-bb54-a614783dfd52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import tensorflow_datasets as tfds\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from bs4 import BeautifulSoup\n",
        "import numpy as np\n",
        "import re\n",
        "from scipy import sparse\n",
        "from nltk import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUH8dp2WtFq3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116,
          "referenced_widgets": [
            "fc835205e67a4523904777145b6e2610",
            "7dbb4483c7c348f69b75c4695a0a9d9c",
            "5ea7390bfff4454e9f771c117025208a",
            "c13f9be15b294dd7a6d0850ebffa9ed8",
            "ae480100c25949749247e40d021fa5bf",
            "cedf5d09d42c461faab355557839e8c1",
            "4f4c24f57f7249e9b920eaffd73e8e1a",
            "f29b946d257d4d5b83007ac5cc207594",
            "464ff37629ca4df190baa29b608694ef",
            "f47348a8fb7a46e492d94c8211f8f57b",
            "2e0f673141e24c128a4f8aa29cee2b57",
            "d90c1c9aafb9453dbbf6f09481051959",
            "da4001c37e6949ed91b3a602b380f3fe",
            "614a6313dca34a8e9c4498985c140611",
            "be86a3d408e04847a43f3ebc45fb66b9",
            "88880db274334e9bae6a2b575ed1cc49",
            "4b53bad9b3864261aee7daa29718b825",
            "eb73075a21644ef1b5f5670361281db1",
            "abc186421ea148bba6fc46145ce30812",
            "6e3bdede39844c48a45771a2ee199af3",
            "afcfdfe923ae4c289b35d6034f321352",
            "e6f5aee83fe249199a12aa8cb8ce3572",
            "7918a85137b14b489d4f443bf1b6636e",
            "eb20c263366a447987acf0f841cd2718",
            "616e50e4f85b486794fabd1b10ac6ee6",
            "f9edb83847414f2e945cc7d0df5e498f",
            "73b70aa1a87348ac8bfd9c6807403095",
            "4bbe93b7229e4cd1b88312e2c1db81c3",
            "3d402b0b14ca4d2794385d974e363f54",
            "6de8e092514d4a75bcfd9c858c04ec59",
            "30934340a0ac44cd993e6d7546ca5908",
            "eaae28307b524c7b86ca5b0ef0ffcc20",
            "1fabd02de55c4512b7da8d1e89313601",
            "1678597846f54cc0b7a8b45a10b9bb05",
            "569f2c3b77904395a79b6eec04775dc3",
            "0bd8f6e3247546f8974485fe5f0c7a17",
            "18b2317204c24d8d8f68f4cd23d1bb6a",
            "7ee15b6bb0e14515913fec0a4134d430",
            "1a3081a9c87b4c3899586f2e5590e761",
            "11cce374b0614c60abbd9833421843b6",
            "f7612bb4001e4234b8919174102b0e73",
            "b6ec970de9a24c9ab04e20652a9f9058",
            "7ec3bcb88ce64622b54eecaf81c492ad",
            "91c7dd4c196b473b962a2253b429ac71",
            "e9645cd826b04452b2b547b7df88bad9",
            "4cf0ac9745c84c69aeb98c87ea3298b9",
            "a0dedece9bbd4ce5bbbd6878a226de3d",
            "bfcb1ff071564b20bfcfc15c4a8c5eb9",
            "3987af8ec6814d83a010a36f54d27eec",
            "d3291ae7b5134ac7a9de142e8860f5aa",
            "64050481944948298993a12f9917a661",
            "61a32d6009294a0e86f16f5167e5c67d",
            "428055236ae24a1fb7903ffeba70b2e7",
            "0e1fffed3e5c4812a847e0d66f0ffc12",
            "f96a62807f924e6685ad3dc66ee0ffc1",
            "b5aa1c1556ea4f9d8fef9537239f9f31",
            "fbd4938bd583417daea65904f1a55860",
            "91d7dd31d90a4b9280674227ad642af0",
            "55792bb87bbf4d7abbf0134dae9a8ebe",
            "ba8e8b3084c5458fa08dd49ac28ae352",
            "efe81d67b7144ce1a314c33a5b52271b",
            "f741b539638041ebb744671f8f08c198",
            "59042db60ca54c1b89fc7dec35adaad3",
            "1ca54680509b4fb3a18675bd81a9a911",
            "ba49a117d26f4de1b6858f7d3ae99f6f",
            "17d3b4ae79a1424ab6e0c2e3c266055a",
            "338dd37d5d5a4427817026b154190344",
            "280a358f924e4f8cab61a0551e885ecb",
            "6f85493b280749388f72195951821931",
            "5f6d875af2bb497f86623611d64ef116",
            "8f8e49844e8c4e84b85bc7e4a216e2f5",
            "7c11efe7a46b476cb0df7a54428debea",
            "89be863aab3d44b788ac6b5547a41cb7",
            "037b64baa4354d588592931f7a5b13e6",
            "a49ec7a040e84b91a2849374a28c1197",
            "194841a2afd343b5ab94bf479364d440",
            "a18a2ae152a4476bac02fd9ab7ad995d",
            "cedce000b94e486fbf078edd5df59acb",
            "1e74298368aa467d8a5de1679e6a7560",
            "6ecae9ee1c1046f8beb94d0df0e5c235",
            "b97a9fa4a7ae4146b190bbe4a224fddc",
            "5dc6797fecc443ce94bece5a5627c094",
            "84b0691c3037405682f3075d3f2300e9",
            "f38256f3a1ab4f94afb5c0506d84bddd",
            "46776b3019f0482083f405b8e9d6eaa2",
            "a48fa994cb4046068a5d938793494e87",
            "ac9eb796ad6041768dd9239fa0b01061",
            "a70d917dd1084cd08360455d8411dbd0",
            "e52deafbf69749748a6ce0954025a279",
            "9ea25c82d7584da280f5fde358cefe4d",
            "73d01f38711549ccb94290dfba3e2f65",
            "b3cff57bc5c341a8977596aaf72acf1e",
            "d9ac5db336ed41f0bcd3f7ee30aca3e8",
            "4134b9e844924d53b9b9aeb890057432",
            "ed396bbcc322442782b7a180259b9936",
            "ba12e471844044949c824b071915dc8d",
            "51f93c8baf8c40faafe4c4c848f76382",
            "dcf00943bbb640a1b0744372945a64db",
            "08b0c5a539ea4602b8e82a98a37988af"
          ]
        },
        "outputId": "c5d0cede-3a42-4cdf-9d8a-de8bf686cb4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dl Completed...: 0 url [00:00, ? url/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fc835205e67a4523904777145b6e2610"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dl Size...: 0 MiB [00:00, ? MiB/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d90c1c9aafb9453dbbf6f09481051959"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7918a85137b14b489d4f443bf1b6636e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train examples...: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1678597846f54cc0b7a8b45a10b9bb05"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Shuffling /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incomplete4743OF/imdb_reviews-train.tfrecord…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e9645cd826b04452b2b547b7df88bad9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test examples...: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b5aa1c1556ea4f9d8fef9537239f9f31"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Shuffling /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incomplete4743OF/imdb_reviews-test.tfrecord*…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "338dd37d5d5a4427817026b154190344"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating unsupervised examples...: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cedce000b94e486fbf078edd5df59acb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Shuffling /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incomplete4743OF/imdb_reviews-unsupervised.t…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e52deafbf69749748a6ce0954025a279"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset imdb_reviews downloaded and prepared to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\n"
          ]
        }
      ],
      "source": [
        "train_data, test_data = tfds.load(name=\"imdb_reviews\", split=[\"train\", \"test\"],\n",
        "                                  batch_size=-1, as_supervised=True)\n",
        "\n",
        "x_train, y_train = tfds.as_numpy(train_data)\n",
        "x_test, y_test = tfds.as_numpy(test_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUwapybbtFq4",
        "outputId": "93671ff5-fcb7-42bf-f0bb-a1f1ebb1251b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input  Trainset shape:  (25000,)\n",
            "Output Trainset shape:  (25000,)\n",
            "Input  Testset  shape:  (25000,)\n",
            "Output Testset  shape:  (25000,)\n"
          ]
        }
      ],
      "source": [
        "print('Input  Trainset shape: ' ,x_train.shape)\n",
        "print('Output Trainset shape: ' ,y_train.shape)\n",
        "print('Input  Testset  shape: ' ,x_test.shape)\n",
        "print('Output Testset  shape: ' ,y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNpOraottFq5"
      },
      "outputs": [],
      "source": [
        "def logsumexp(Z):                                                # dimension C x N\n",
        "    Zmax = np.max(Z,axis=0)[None,:]                              # max over C\n",
        "    log_sum_exp = Zmax + np.log(np.sum(np.exp(Z - Zmax), axis=0))\n",
        "    return log_sum_exp\n",
        "\n",
        "class BernoulliNaiveBayes:\n",
        "    def __init__(self):\n",
        "      return\n",
        "\n",
        "    def fit(self, x, y, alpha_pi, beta_pi, alpha, beta):\n",
        "      N, D = x.shape\n",
        "      C = np.max(y) + 1\n",
        "      prob= np.zeros((C,D))\n",
        "      Nc = np.zeros(C) # number of instances in class c\n",
        "      for c in range(C):\n",
        "          x_c = x[y == c]                           #slice all the elements from class c\n",
        "          Nc[c] = x_c.shape[0]                      #get number of elements of class c\n",
        "          if c==1:\n",
        "            add= alpha\n",
        "          else:\n",
        "            add=beta\n",
        "          prob[c,:] = (np.sum(x_c, 0)+ add)/(Nc[c]+ alpha+ beta) #np.sum(x_c, 0)\n",
        "      #### NEED TO INCLUDE BAYESIAN INFERENCE!\n",
        "      #self.denom= denom\n",
        "      self.prior = (Nc+alpha_pi)/(N+alpha_pi+beta_pi)                               # C x D\n",
        "      self.prob = prob\n",
        "      return self\n",
        "\n",
        "    def predict(self, x):\n",
        "      N, D = x.shape\n",
        "      log_prior= np.log(self.prior)\n",
        "      log_likelihood= np.zeros((N, self.prob.shape[0]))\n",
        "      for c in range(self.prob.shape[0]):\n",
        "        for n in range(N):\n",
        "          prob=np.expand_dims(self.prob[c, :], axis=0)\n",
        "          #### NEED TO INCLUDE BAYESIAN INFERENCE!\n",
        "          x_test=x[n, :].toarray()\n",
        "          prob= np.where(x_test==0, 1-prob, prob);\n",
        "          log_likelihood[n, c]= np.sum(np.log(prob), axis=1)\n",
        "\n",
        "      log_posterior =np.expand_dims(log_prior, axis=0)+log_likelihood\n",
        "      posterior = np.exp(log_posterior - logsumexp(log_posterior))\n",
        "      return posterior\n",
        "\n",
        "    def evaluate_acc(self,yh, y):\n",
        "      return np.sum(yh == y)/yh.shape[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVN6KDdvtFq6"
      },
      "outputs": [],
      "source": [
        "stopword_list=nltk.corpus.stopwords.words('english')\n",
        "class LemmaTokenizer(object):\n",
        "    def __init__(self):\n",
        "        self.wnl = WordNetLemmatizer()\n",
        "    def __call__(self, articles):\n",
        "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_9f9cZBtFq6"
      },
      "outputs": [],
      "source": [
        "def strip_html(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4CwCwDHtFq7"
      },
      "source": [
        "## Fitting Bernoulli model with different kinds of preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1YzYgwwtFq7"
      },
      "source": [
        "### No clean up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBS8eM2wtFq7"
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer(binary=True)\n",
        "X_train = vectorizer.fit_transform(x_train)\n",
        "dic= vectorizer.get_feature_names_out()\n",
        "X_test=vectorizer.transform(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeuZLEEbtFq8",
        "outputId": "5ddfbed2-0865-45b8-fff1-5cc69dea43fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dictionary length: 74849\n"
          ]
        }
      ],
      "source": [
        "print(\"Dictionary length:\", len(vectorizer.get_feature_names_out()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHoXTjIptFq9"
      },
      "source": [
        "#### Without priors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsINKi_FtFq9",
        "outputId": "dd0c93f0-d307-4971-a2e4-76687bdfe1bf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-87-240c21986645>:55: RuntimeWarning: divide by zero encountered in log\n",
            "  log_likelihood[n, c]= np.sum(np.log(prob), axis=1)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.65612\n"
          ]
        }
      ],
      "source": [
        "model=BernoulliNaiveBayes()\n",
        "params=model.fit(X_train, y_train, 0,0,0,0)\n",
        "y_prob=model.predict(X_test)\n",
        "y_pred = np.argmax(y_prob, 1)\n",
        "print(\"Accuracy:\",model.evaluate_acc(y_pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0i0pSGDtFq9"
      },
      "source": [
        "#### With priors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GT-p-6kJtFq9",
        "outputId": "b6a8f94a-7faa-4f73-cc49-79e42c65f7c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.808\n"
          ]
        }
      ],
      "source": [
        "model=BernoulliNaiveBayes()\n",
        "params=model.fit(X_train, y_train, 1,1,10,10)\n",
        "y_prob=model.predict(X_test)\n",
        "y_pred = np.argmax(y_prob, 1)\n",
        "print(\"Accuracy:\",model.evaluate_acc(y_pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASWmwwUNtFq-"
      },
      "source": [
        "### No clean up but with limits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60cBc-JEtFq-"
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer(min_df=0.005, max_df=0.6, binary=True)\n",
        "X_train = vectorizer.fit_transform(x_train)\n",
        "dic= vectorizer.get_feature_names_out()\n",
        "X_test=vectorizer.transform(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "po0jUC3KtFq-",
        "outputId": "892195c8-d449-4ac9-d71d-27eb3099d0a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dictionary length: 3195\n"
          ]
        }
      ],
      "source": [
        "print(\"Dictionary length:\", len(vectorizer.get_feature_names_out()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d7gQcp-tFq-"
      },
      "source": [
        "#### Without priors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDUCICpVtFq_",
        "outputId": "d9f4c2dc-9332-4b70-f028-8019ef66b026"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8366\n"
          ]
        }
      ],
      "source": [
        "model=BernoulliNaiveBayes()\n",
        "params=model.fit(X_train, y_train, 0,0,0,0)\n",
        "y_prob=model.predict(X_test)\n",
        "y_pred = np.argmax(y_prob, 1)\n",
        "print(\"Accuracy:\",model.evaluate_acc(y_pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "durj7jQltFq_"
      },
      "source": [
        "#### With priors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQlV8-UXtFq_",
        "outputId": "3e35fab6-de20-4b0f-ffbc-8dd19b962a7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.83568\n"
          ]
        }
      ],
      "source": [
        "model=BernoulliNaiveBayes()\n",
        "params=model.fit(X_train, y_train, 1,1,10,10)\n",
        "y_prob=model.predict(X_test)\n",
        "y_pred = np.argmax(y_prob, 1)\n",
        "print(\"Accuracy:\",model.evaluate_acc(y_pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sj23vUGHtFq_"
      },
      "source": [
        "### No clean up with different limits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaQucBUZtFq_"
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer(min_df=0.05, max_df=0.6, binary=True)\n",
        "X_train = vectorizer.fit_transform(x_train)\n",
        "dic= vectorizer.get_feature_names_out()\n",
        "X_test=vectorizer.transform(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uyhLz_WtFrA",
        "outputId": "35eeb785-baab-488e-fd23-6c8032ca8f42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dictionary length: 413\n"
          ]
        }
      ],
      "source": [
        "print(\"Dictionary length:\", len(vectorizer.get_feature_names_out()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOW8fqVitFrA"
      },
      "source": [
        "#### Without priors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZ1KiYnYtFrA",
        "outputId": "db41198c-6c2c-4c28-bcfc-cfa570d4507c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.69308\n"
          ]
        }
      ],
      "source": [
        "model=BernoulliNaiveBayes()\n",
        "params=model.fit(X_train, y_train, 0,0,0,0)\n",
        "y_prob=model.predict(X_test)\n",
        "y_pred = np.argmax(y_prob, 1)\n",
        "print(\"Accuracy:\",model.evaluate_acc(y_pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Xi-ddtXtFrB"
      },
      "source": [
        "#### With priors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdB1RBfxtFrB",
        "outputId": "62fbb537-d389-4f55-849a-9c2fc559479e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.69284\n"
          ]
        }
      ],
      "source": [
        "model=BernoulliNaiveBayes()\n",
        "params=model.fit(X_train, y_train, 1,1,10,10)\n",
        "y_prob=model.predict(X_test)\n",
        "y_pred = np.argmax(y_prob, 1)\n",
        "print(\"Accuracy:\",model.evaluate_acc(y_pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05pMv5AStFrC"
      },
      "source": [
        "### Stop words and limits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cspvXfJtFrC"
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer(stop_words=stopword_list, min_df=0.005, max_df=0.6, binary=True)\n",
        "X_train = vectorizer.fit_transform(x_train)\n",
        "dic= vectorizer.get_feature_names_out()\n",
        "X_test=vectorizer.transform(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2w7_cWfotFrC",
        "outputId": "1bd768ee-f313-4435-f852-bf11783160e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dictionary length: 3075\n"
          ]
        }
      ],
      "source": [
        "print(\"Dictionary length:\", len(vectorizer.get_feature_names_out()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WBT5nTrtFrC"
      },
      "source": [
        "#### Without priors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6yEDLo8tFrC",
        "outputId": "3fd1ee86-1f03-4c3d-c2c6-c14eaa6c74ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.84756\n"
          ]
        }
      ],
      "source": [
        "model=BernoulliNaiveBayes()\n",
        "params=model.fit(X_train, y_train, 0,0,0,0)\n",
        "y_prob=model.predict(X_test)\n",
        "y_pred = np.argmax(y_prob, 1)\n",
        "print(\"Accuracy:\",model.evaluate_acc(y_pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dODEJnLdtFrD"
      },
      "source": [
        "#### With priors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-H-2RootFrD",
        "outputId": "519fbc4c-3c1e-4288-c57b-6e0596ff4a61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.84796\n"
          ]
        }
      ],
      "source": [
        "model=BernoulliNaiveBayes()\n",
        "params=model.fit(X_train, y_train, 1,1,10,10)\n",
        "y_prob=model.predict(X_test)\n",
        "y_pred = np.argmax(y_prob, 1)\n",
        "print(\"Accuracy:\",model.evaluate_acc(y_pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9EuNvRdI7ps"
      },
      "source": [
        "### Analysis with different limits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsB6KC7ZI7WZ",
        "outputId": "704e270c-46b1-4600-d8e4-58bbb342491a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for lower limit 0 and higher limit 0.35: 0.80912 ; Length of dictionary: 74698\n",
            "Accuracy for lower limit 0 and higher limit 0.4: 0.80908 ; Length of dictionary: 74699\n",
            "Accuracy for lower limit 0 and higher limit 0.45: 0.80908 ; Length of dictionary: 74699\n",
            "Accuracy for lower limit 0 and higher limit 0.5: 0.80988 ; Length of dictionary: 74700\n",
            "Accuracy for lower limit 0 and higher limit 0.55: 0.80988 ; Length of dictionary: 74700\n",
            "Accuracy for lower limit 0 and higher limit 0.6: 0.80996 ; Length of dictionary: 74703\n",
            "Accuracy for lower limit 0 and higher limit 0.65: 0.81216 ; Length of dictionary: 74704\n",
            "Accuracy for lower limit 0.005 and higher limit 0.35: 0.84764 ; Length of dictionary: 3070\n",
            "Accuracy for lower limit 0.005 and higher limit 0.4: 0.84724 ; Length of dictionary: 3071\n",
            "Accuracy for lower limit 0.005 and higher limit 0.45: 0.84724 ; Length of dictionary: 3071\n",
            "Accuracy for lower limit 0.005 and higher limit 0.5: 0.84732 ; Length of dictionary: 3072\n",
            "Accuracy for lower limit 0.005 and higher limit 0.55: 0.84732 ; Length of dictionary: 3072\n",
            "Accuracy for lower limit 0.005 and higher limit 0.6: 0.84796 ; Length of dictionary: 3075\n",
            "Accuracy for lower limit 0.005 and higher limit 0.65: 0.84568 ; Length of dictionary: 3076\n",
            "Accuracy for lower limit 0.01 and higher limit 0.35: 0.84736 ; Length of dictionary: 1667\n",
            "Accuracy for lower limit 0.01 and higher limit 0.4: 0.84768 ; Length of dictionary: 1668\n",
            "Accuracy for lower limit 0.01 and higher limit 0.45: 0.84768 ; Length of dictionary: 1668\n",
            "Accuracy for lower limit 0.01 and higher limit 0.5: 0.84668 ; Length of dictionary: 1669\n",
            "Accuracy for lower limit 0.01 and higher limit 0.55: 0.84668 ; Length of dictionary: 1669\n",
            "Accuracy for lower limit 0.01 and higher limit 0.6: 0.84668 ; Length of dictionary: 1672\n",
            "Accuracy for lower limit 0.01 and higher limit 0.65: 0.84464 ; Length of dictionary: 1673\n",
            "Accuracy for lower limit 0.015 and higher limit 0.35: 0.84236 ; Length of dictionary: 1158\n",
            "Accuracy for lower limit 0.015 and higher limit 0.4: 0.8436 ; Length of dictionary: 1159\n",
            "Accuracy for lower limit 0.015 and higher limit 0.45: 0.8436 ; Length of dictionary: 1159\n",
            "Accuracy for lower limit 0.015 and higher limit 0.5: 0.84268 ; Length of dictionary: 1160\n",
            "Accuracy for lower limit 0.015 and higher limit 0.55: 0.84268 ; Length of dictionary: 1160\n",
            "Accuracy for lower limit 0.015 and higher limit 0.6: 0.84172 ; Length of dictionary: 1163\n",
            "Accuracy for lower limit 0.015 and higher limit 0.65: 0.84108 ; Length of dictionary: 1164\n"
          ]
        }
      ],
      "source": [
        "lower_limits=[0, 0.005, 0.01, 0.015]\n",
        "upper_limits=[0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65]\n",
        "for l in lower_limits:\n",
        "  for u in upper_limits:\n",
        "    vectorizer = CountVectorizer(stop_words=stopword_list, min_df=l, max_df=u, binary=True)\n",
        "    X_train = vectorizer.fit_transform(x_train)\n",
        "    dic= vectorizer.get_feature_names_out()\n",
        "    X_test=vectorizer.transform(x_test)\n",
        "    model=BernoulliNaiveBayes()\n",
        "    params=model.fit(X_train, y_train, 1, 1, 10, 10)\n",
        "    y_prob=model.predict(X_test)\n",
        "    y_pred = np.argmax(y_prob, 1)\n",
        "    print(\"Accuracy for lower limit \"+str(l)+ \" and higher limit \"+ str(u)+\":\",model.evaluate_acc(y_pred, y_test), \"; Length of dictionary:\", len(dic))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5sELtFqtFrE"
      },
      "source": [
        "### Lemmatizer, stop words and limits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHgudV7GtFrE"
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer(stop_words=stopword_list, tokenizer=LemmaTokenizer(), min_df=0.005, max_df=0.6, binary=True)\n",
        "X_train = vectorizer.fit_transform(x_train)\n",
        "dic= vectorizer.get_feature_names_out()\n",
        "X_test=vectorizer.transform(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0piS0zTjtFrE",
        "outputId": "320b510d-e80b-4b89-c019-8229f9336cc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dictionary length: 2910\n"
          ]
        }
      ],
      "source": [
        "print(\"Dictionary length:\", len(vectorizer.get_feature_names_out()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZHosayatFrF"
      },
      "source": [
        "#### Without priors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_vV2zPitFrF",
        "outputId": "b2c30e5a-ebfc-4fc5-d092-abc548e2c88a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.83764\n"
          ]
        }
      ],
      "source": [
        "model=BernoulliNaiveBayes()\n",
        "params=model.fit(X_train, y_train, 0,0,0,0)\n",
        "y_prob=model.predict(X_test)\n",
        "y_pred = np.argmax(y_prob, 1)\n",
        "print(\"Accuracy:\",model.evaluate_acc(y_pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9RwR9ketFrF"
      },
      "source": [
        "#### With priors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ir7CwEIMtFrF",
        "outputId": "31f79aa6-876c-4c43-c251-875da4e7cdcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8358\n"
          ]
        }
      ],
      "source": [
        "model=BernoulliNaiveBayes()\n",
        "params=model.fit(X_train, y_train, 1,1,10,10)\n",
        "y_prob=model.predict(X_test)\n",
        "y_pred = np.argmax(y_prob, 1)\n",
        "print(\"Accuracy:\",model.evaluate_acc(y_pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUyET_6-tFrF"
      },
      "source": [
        "### After cleaning html tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTwVBwbNtFrG",
        "outputId": "c2d20e5a-9d0f-441b-ae48-6585f4ad9b42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-25238354f317>:2: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(text, \"html.parser\")\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(x_train)):\n",
        "  x_train[i]=strip_html(x_train[i]) ## Only removing html tags for now\n",
        "  x_test[i]=strip_html(x_test[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KaKnDTntFrG"
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer(stop_words=stopword_list, tokenizer=LemmaTokenizer(), min_df=0.005, max_df=0.6, binary=True)\n",
        "X_train = vectorizer.fit_transform(x_train)\n",
        "dic= vectorizer.get_feature_names_out()\n",
        "X_test=vectorizer.transform(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTmjql1htFrG",
        "outputId": "0ffa6d52-082e-40b4-a995-026de9356416"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dictionary length: 2880\n"
          ]
        }
      ],
      "source": [
        "print(\"Dictionary length:\", len(vectorizer.get_feature_names_out()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LhWK0ZKtFrG"
      },
      "source": [
        "#### Without priors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-UnrsvytFrH",
        "outputId": "7556bfa4-fdca-4307-f53a-be70bec4d6a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.83692\n"
          ]
        }
      ],
      "source": [
        "model=BernoulliNaiveBayes()\n",
        "params=model.fit(X_train, y_train, 0,0,0,0)\n",
        "y_prob=model.predict(X_test)\n",
        "y_pred = np.argmax(y_prob, 1)\n",
        "print(\"Accuracy:\",model.evaluate_acc(y_pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMDQFOvFtFrH"
      },
      "source": [
        "#### With priors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0waOCq-tFrH",
        "outputId": "dbd465a9-f251-4907-993c-e8ec867a48c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.83672\n"
          ]
        }
      ],
      "source": [
        "model=BernoulliNaiveBayes()\n",
        "params=model.fit(X_train, y_train, 1,1,10,10)\n",
        "y_prob=model.predict(X_test)\n",
        "y_pred = np.argmax(y_prob, 1)\n",
        "print(\"Accuracy:\",model.evaluate_acc(y_pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZKN09RUtFrH"
      },
      "source": [
        "### Also cleaning special characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAYqHIwBtFrH"
      },
      "outputs": [],
      "source": [
        "for i in range(len(x_train)):\n",
        "  x_train[i] = re.sub(r'[^a-zA-z0-9\\s]', ' ', x_train[i]).lower()\n",
        "  x_test[i] = re.sub(r'[^a-zA-z0-9\\s]', ' ', x_test[i]).lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdMDFyTStFrI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f024e09-cc78-4c8e-ceae-5a020c5af2f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "vectorizer = CountVectorizer(stop_words=stopword_list, tokenizer=LemmaTokenizer(), min_df=0.005, max_df=0.4, binary=True)\n",
        "X_train = vectorizer.fit_transform(x_train)\n",
        "dic= vectorizer.get_feature_names_out()\n",
        "X_test=vectorizer.transform(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kunqUMpltFrI",
        "outputId": "55333683-701f-448e-ae30-9952c29bf391"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dictionary length: 2958\n"
          ]
        }
      ],
      "source": [
        "print(\"Dictionary length:\", len(vectorizer.get_feature_names_out()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQwGvQPVtFrI"
      },
      "source": [
        "#### Without priors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WT9C51qLtFrJ",
        "outputId": "8db9835c-f74d-460b-a34e-1c3f2e63129f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.84736\n"
          ]
        }
      ],
      "source": [
        "model=BernoulliNaiveBayes()\n",
        "params=model.fit(X_train, y_train, 0,0,0,0)\n",
        "y_prob=model.predict(X_test)\n",
        "y_pred = np.argmax(y_prob, 1)\n",
        "print(\"Accuracy:\",model.evaluate_acc(y_pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGrLs_q3tFrJ"
      },
      "source": [
        "#### With priors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcZni-fGtFrJ",
        "outputId": "dfcd6945-827c-4340-cb9c-13d01374487a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.84772\n"
          ]
        }
      ],
      "source": [
        "model=BernoulliNaiveBayes()\n",
        "params=model.fit(X_train, y_train, 1,1,10,10)\n",
        "y_prob=model.predict(X_test)\n",
        "y_pred = np.argmax(y_prob, 1)\n",
        "print(\"Accuracy:\",model.evaluate_acc(y_pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iICiX_hMd55"
      },
      "source": [
        "### Analysis with different limits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcKpTgUzMhMf",
        "outputId": "9e5c8908-7981-4a97-8f01-905a130e32c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for lower limit 0 and higher limit 0.35: 0.81588 ; Length of dictionary:  67893\n",
            "Accuracy for lower limit 0 and higher limit 0.4: 0.81564 ; Length of dictionary:  67894\n",
            "Accuracy for lower limit 0 and higher limit 0.45: 0.81536 ; Length of dictionary:  67896\n",
            "Accuracy for lower limit 0 and higher limit 0.5: 0.81624 ; Length of dictionary:  67897\n",
            "Accuracy for lower limit 0 and higher limit 0.55: 0.81624 ; Length of dictionary:  67897\n",
            "Accuracy for lower limit 0 and higher limit 0.6: 0.81588 ; Length of dictionary:  67899\n",
            "Accuracy for lower limit 0 and higher limit 0.65: 0.81804 ; Length of dictionary:  67901\n",
            "Accuracy for lower limit 0.005 and higher limit 0.35: 0.84708 ; Length of dictionary:  2957\n",
            "Accuracy for lower limit 0.005 and higher limit 0.4: 0.84772 ; Length of dictionary:  2958\n",
            "Accuracy for lower limit 0.005 and higher limit 0.45: 0.84768 ; Length of dictionary:  2960\n",
            "Accuracy for lower limit 0.005 and higher limit 0.5: 0.84688 ; Length of dictionary:  2961\n",
            "Accuracy for lower limit 0.005 and higher limit 0.55: 0.84688 ; Length of dictionary:  2961\n",
            "Accuracy for lower limit 0.005 and higher limit 0.6: 0.8466 ; Length of dictionary:  2963\n",
            "Accuracy for lower limit 0.005 and higher limit 0.65: 0.8438 ; Length of dictionary:  2965\n",
            "Accuracy for lower limit 0.01 and higher limit 0.35: 0.846 ; Length of dictionary:  1666\n",
            "Accuracy for lower limit 0.01 and higher limit 0.4: 0.8464 ; Length of dictionary:  1667\n",
            "Accuracy for lower limit 0.01 and higher limit 0.45: 0.84672 ; Length of dictionary:  1669\n",
            "Accuracy for lower limit 0.01 and higher limit 0.5: 0.84652 ; Length of dictionary:  1670\n",
            "Accuracy for lower limit 0.01 and higher limit 0.55: 0.84652 ; Length of dictionary:  1670\n",
            "Accuracy for lower limit 0.01 and higher limit 0.6: 0.84584 ; Length of dictionary:  1672\n",
            "Accuracy for lower limit 0.01 and higher limit 0.65: 0.84052 ; Length of dictionary:  1674\n",
            "Accuracy for lower limit 0.015 and higher limit 0.35: 0.84252 ; Length of dictionary:  1165\n",
            "Accuracy for lower limit 0.015 and higher limit 0.4: 0.8436 ; Length of dictionary:  1166\n",
            "Accuracy for lower limit 0.015 and higher limit 0.45: 0.84424 ; Length of dictionary:  1168\n",
            "Accuracy for lower limit 0.015 and higher limit 0.5: 0.84388 ; Length of dictionary:  1169\n",
            "Accuracy for lower limit 0.015 and higher limit 0.55: 0.84388 ; Length of dictionary:  1169\n",
            "Accuracy for lower limit 0.015 and higher limit 0.6: 0.84228 ; Length of dictionary:  1171\n",
            "Accuracy for lower limit 0.015 and higher limit 0.65: 0.84024 ; Length of dictionary:  1173\n"
          ]
        }
      ],
      "source": [
        "lower_limits=[0, 0.005, 0.01, 0.015]\n",
        "upper_limits=[0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65]\n",
        "for l in lower_limits:\n",
        "  for u in upper_limits:\n",
        "    vectorizer = CountVectorizer(stop_words=stopword_list, tokenizer=LemmaTokenizer(), min_df=l, max_df=u, binary=True)\n",
        "    X_train = vectorizer.fit_transform(x_train)\n",
        "    dic= vectorizer.get_feature_names_out()\n",
        "    X_test=vectorizer.transform(x_test)\n",
        "    model=BernoulliNaiveBayes()\n",
        "    params=model.fit(X_train, y_train, 1, 1, 10, 10)\n",
        "    y_prob=model.predict(X_test)\n",
        "    y_pred = np.argmax(y_prob, 1)\n",
        "    print(\"Accuracy for lower limit \"+str(l)+ \" and higher limit \"+ str(u)+\":\",model.evaluate_acc(y_pred, y_test), \"; Length of dictionary: \", len(dic))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SVbrb_gtFrJ"
      },
      "source": [
        "### With other limits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoATq6RDtFrJ"
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer(stop_words=stopword_list, tokenizer=LemmaTokenizer(), min_df=0.001, max_df=0.65, binary=True)\n",
        "X_train = vectorizer.fit_transform(x_train)\n",
        "dic= vectorizer.get_feature_names_out()\n",
        "X_test=vectorizer.transform(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9krCJcytFrJ",
        "outputId": "681dccaa-c2e8-4a83-e8f1-78d1bc1a8246"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dictionary length: 9429\n"
          ]
        }
      ],
      "source": [
        "print(\"Dictionary length:\", len(vectorizer.get_feature_names_out()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cGwGJxmtFrK"
      },
      "source": [
        "#### Without priors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdE8m7iXtFrK",
        "outputId": "d9c61212-9d76-495b-9be2-0634a017e422"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-4-240c21986645>:55: RuntimeWarning: divide by zero encountered in log\n",
            "  log_likelihood[n, c]= np.sum(np.log(prob), axis=1)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8252\n"
          ]
        }
      ],
      "source": [
        "model=BernoulliNaiveBayes()\n",
        "params=model.fit(X_train, y_train, 0,0,0,0)\n",
        "y_prob=model.predict(X_test)\n",
        "y_pred = np.argmax(y_prob, 1)\n",
        "print(\"Accuracy:\",model.evaluate_acc(y_pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9BctHVBtFrK"
      },
      "source": [
        "#### With priors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93av2KL9tFrK",
        "outputId": "99d6b91f-3f8f-4971-9622-61ab254c2d07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8296\n"
          ]
        }
      ],
      "source": [
        "model=BernoulliNaiveBayes()\n",
        "params=model.fit(X_train, y_train, 1,1,10,10)\n",
        "y_prob=model.predict(X_test)\n",
        "y_pred = np.argmax(y_prob, 1)\n",
        "print(\"Accuracy:\",model.evaluate_acc(y_pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55jwjkuotFrK"
      },
      "source": [
        "# Multinomial Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpkcBm7ZtFrL",
        "outputId": "953d10d4-23f7-4fb0-cf35-d047a4464a8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import tensorflow_datasets as tfds\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from bs4 import BeautifulSoup\n",
        "import numpy as np\n",
        "import re\n",
        "from scipy import sparse\n",
        "from nltk import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5qZCMm3tFrL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "05c9264531b2438fa816dc1541003a50",
            "44ede988f74948b8b57ef76097282ba9",
            "344ae37cc82942a1b16b968d75ce4947",
            "6fc58efb927846cbb3c3b14a4b47c9bd",
            "a52e49fcff834f8ca04a26176001240e",
            "c17287c8a35c4f53b4be1e847e83e71e",
            "d93b990d4e0946a58e81f223673ff8c4",
            "6783dab12fac4ff68695ac4fa5f52e9f",
            "470cd1d981f4439795202d814f5956c2",
            "c24c6769a86448859063dae75dbf563f",
            "cd4ad1fd129c4f67a4a58698b929fbf9",
            "c4863a3478a34c208483d27aacad2c4c",
            "2c2f459916de4668b7d34c5a64c7be2a",
            "3131fae60b844dcdba69ff567ca33cbc",
            "0f67c0801d564d9aa1e08ede8934ad62",
            "0c6ff0f2baf64f1c88fbec5178ab93df",
            "db928d479c6648d8a3ff7c410860e7af",
            "26fffc0f632c45a29a04859eaace4fc0",
            "a827664489614b5dbb39593f9bd153b6",
            "cfeb93228e5146f9af5edc2a3c25fa6b",
            "43a67878959348e9967989e71afba634",
            "576f55c3a39647aea34e80f50b8f75d2",
            "af44fff89ad94871b2a73de0c04216e5",
            "cc61de5ccc7146cab34b9768410301a9",
            "fc2cc68936414b9094ffd1395e37aa40",
            "f54262dba2f54abf8f9e19107d6b290e",
            "5357da513c4140eb8f14ce270dfd4eba",
            "03ebfa6d0331445e81c4ca3c85a81e9d",
            "cdfef717f04e458c93002166b468a99a",
            "ba524f455c8c4c80bc737ad48b272d8e",
            "3cde21a50e734f42bc9c258204d1c421",
            "4ac23a0cbb5240699a0b8364c7cc0114",
            "f614ba112e3949578d882ce3edd3a348",
            "ea65b24fcdc342b080f3d9acde5d3777",
            "052e2f01b2e34436bb37f80cd22d2d20",
            "0262eff1b9ec4b36846d884687bbf10a",
            "7733820063a2456096de2d4411817c92",
            "c4f32cc57fb74e10b348fae53900aaaf",
            "525f4be1131042659195bb4e9187614e",
            "0eae2395905e49eeaee02802d8eee6ed",
            "0d65105c59ea45dbbc9b8bcb349687cd",
            "1ae041f806504b92892e3ccb23db24d6",
            "ac262c0baaf9473d8a32216b77cf4741",
            "db1028da0b1246f2b3f2d2ef044e7b45",
            "5605a01073f14fd4aa50e1237cd06e64",
            "875749aceda341aead6dc9828a16f100",
            "b110c5a1fa0849fbaae713f836c282d1",
            "a5d9f8a3295344a1bd0a0e7d32d2ce04",
            "d5ae1c9637af4b11beac32cbb34538ad",
            "04cab46f24de4660a07d3b08300f9b0d",
            "13412970040243eea8f5026421f3a25b",
            "5c89dfdfea414366b2430272031ec4a3",
            "b1bbab49041e40ac9049123404bc86a8",
            "c879489c95ad4dacbebca7a226d3e1c6",
            "3afd7fdafe2140b3928ecc4678021d24",
            "4740184ea380495e89e8ff51fe380e16",
            "d381feb7836e4f22a2b0b1ab3de62b16",
            "d59d469265314a97ab6de0a2582b1077",
            "12c8812f93e147f692173af8f0c9d681",
            "8ecd4025dba243139f740360c1798875",
            "c4f9ac69ebf74e9e861485c43c900b74",
            "7cdf146f3c994232a4ba694e358b508e",
            "f9f1dc5b1a2d42f69e00128d9e8d276d",
            "2bb8dd1d04544ac1928f1f33aaa8a6f3",
            "e332b205b6294c45ad3f16fe44aab590",
            "5e3c8c6508e8474990bf28f86837ee18",
            "5c1c944282954089a2ff5247767b7e0a",
            "77635222213841a0808fb1771c86c0e9",
            "7351c9f3f00d4644ab7c85fb8765196d",
            "8eec3022d5814fd3a74cac8b76febaff",
            "4153ae6ab56c4fa5b24cd52804b783d3",
            "a44eeb4b0e574e788405a1beb8d4e482",
            "9455528d83444decbd7ebe8532ee0736",
            "c3fa1f2783e44665a40950bb4aeadcc6",
            "b6a1c13c0feb47e7acfd782748a5f29b",
            "1574cd34b3684c7db6bc06f7a4f11a5b",
            "d4ceaa32ba0f4b04a80acad486738865",
            "d992250990104c199ba804007d915301",
            "7ff1caf4643c4b83b09f793a0fc7a45a",
            "f600692df3fc4098940e1db4a1cd25bb",
            "877acaf9a7564d14936f1648b83253ae",
            "1445917c78394084aee39192657037e1",
            "41d8a6057e6d4586809002e63325adb0",
            "12f982ff3bc94e3f9712c60b5129f69f",
            "0e6bff609fd240d592658725b512a82a",
            "6bad8a4a1df040e78997359511c99501",
            "db9981680e4543179cc529a36c467c5c",
            "c58b4f45e05f4e4fb14d971c1c1fff69",
            "1250dac4e74442d5b57690bf20a1eec5",
            "cc1cdd7256774c819bbb06baf655f039",
            "713a9ed983dd45e3bac0f827ec58facd",
            "ef451fd1d17c4c93b348c231749c4a7b",
            "0c955c9952d84501a4ad9b5cb7e6f16b",
            "1aeca08b30ab4a6e9fb550f6ebc9ca34",
            "4d97c9f074bf4e69bd55afbfda5c751a",
            "a6cbbc4d242f4605a5b90ef3c4dc466e",
            "45feda96d23747b5bd137c2a17b4a138",
            "30c6419212524ec7b25208307f6bc9ae",
            "6e24e134588248338e61b35b731c2294"
          ]
        },
        "outputId": "bb6465d8-40d2-419e-91af-b54399335bc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dl Completed...: 0 url [00:00, ? url/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "05c9264531b2438fa816dc1541003a50"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dl Size...: 0 MiB [00:00, ? MiB/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c4863a3478a34c208483d27aacad2c4c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "af44fff89ad94871b2a73de0c04216e5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train examples...: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ea65b24fcdc342b080f3d9acde5d3777"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Shuffling /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteGQ9H9L/imdb_reviews-train.tfrecord…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5605a01073f14fd4aa50e1237cd06e64"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test examples...: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4740184ea380495e89e8ff51fe380e16"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Shuffling /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteGQ9H9L/imdb_reviews-test.tfrecord*…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5c1c944282954089a2ff5247767b7e0a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating unsupervised examples...: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d992250990104c199ba804007d915301"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Shuffling /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteGQ9H9L/imdb_reviews-unsupervised.t…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1250dac4e74442d5b57690bf20a1eec5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset imdb_reviews downloaded and prepared to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\n"
          ]
        }
      ],
      "source": [
        "train_data, test_data = tfds.load(name=\"imdb_reviews\", split=[\"train\", \"test\"],\n",
        "                                  batch_size=-1, as_supervised=True)\n",
        "\n",
        "x_train, y_train = tfds.as_numpy(train_data)\n",
        "x_test, y_test = tfds.as_numpy(test_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJhxSpz1tFrL",
        "outputId": "8185d3b1-4b83-41ab-b095-5812a3643d28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input  Trainset shape:  (25000,)\n",
            "Output Trainset shape:  (25000,)\n",
            "Input  Testset  shape:  (25000,)\n",
            "Output Testset  shape:  (25000,)\n"
          ]
        }
      ],
      "source": [
        "print('Input  Trainset shape: ' ,x_train.shape)\n",
        "print('Output Trainset shape: ' ,y_train.shape)\n",
        "print('Input  Testset  shape: ' ,x_test.shape)\n",
        "print('Output Testset  shape: ' ,y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5hXSaQ7tFrL"
      },
      "outputs": [],
      "source": [
        "def logsumexp(Z):                                                # dimension C x N\n",
        "    Zmax = np.max(Z,axis=0)[None,:]                              # max over C\n",
        "    log_sum_exp = Zmax + np.log(np.sum(np.exp(Z - Zmax), axis=0))\n",
        "    return log_sum_exp\n",
        "\n",
        "class MultinomialNaiveBayes:\n",
        "    def __init__(self):\n",
        "      return\n",
        "\n",
        "    def fit(self, x, y, alpha_pi, beta_pi, alpha, beta):\n",
        "      N, D = x.shape\n",
        "      C = np.max(y) + 1\n",
        "      prob= np.zeros((C,D))\n",
        "      Nc = np.zeros(C) # number of instances in class c\n",
        "      for c in range(C):\n",
        "          x_c = x[y == c]                           #slice all the elements from class c\n",
        "          Nc[c] = x_c.shape[0]                      #get number of elements of class c\n",
        "          #prob[c,:] = (np.sum(x_c, 0)+ alpha)/(np.sum(x_c) +alpha+ beta)\n",
        "          if c==1:\n",
        "            add= alpha\n",
        "          else:\n",
        "            add=beta\n",
        "          prob[c,:] = ((np.sum(x_c, 0)+ add)*100)/(np.sum(x_c)+ alpha+ beta) #np.sum(x_c, 0)\n",
        "          #denom[c]= np.sum(x_c)\n",
        "      #### NEED TO INCLUDE BAYESIAN INFERENCE!\n",
        "      #self.denom= denom\n",
        "      self.prior = (Nc+alpha_pi)/(N+alpha_pi+beta_pi)                               # C x D\n",
        "      self.prob = prob\n",
        "      #self.alpha= alpha\n",
        "      #self.beta= beta\n",
        "      return self\n",
        "\n",
        "    def predict(self, x):\n",
        "      N, D = x.shape\n",
        "      log_prior= np.log(self.prior)\n",
        "      log_likelihood= np.zeros((N, self.prob.shape[0]))\n",
        "      \"\"\"\n",
        "      for c in range(self.prob.shape[0]):\n",
        "        prob=np.expand_dims(self.prob[c, :], axis=0)\n",
        "        #print(self.prob.shape, prob.shape)\n",
        "        log_likelihood=np.sum(np.log(sparse.csr_matrix.multiply(np.repeat(prob, N, axis=0), x), axis=1)) ##Adding index the other way doesn't work for sparse arrays\n",
        "      \"\"\"\n",
        "      for c in range(self.prob.shape[0]):\n",
        "        for n in range(N):\n",
        "          prob=np.expand_dims(self.prob[c, :], axis=0)\n",
        "          x_test=x[n, :].toarray()\n",
        "          log_likelihood[n, c]= np.sum(np.log(np.power(prob,x_test)), axis=1)\n",
        "\n",
        "      log_posterior =np.expand_dims(log_prior, axis=0)+log_likelihood\n",
        "      posterior = np.exp(log_posterior - logsumexp(log_posterior))\n",
        "      return posterior\n",
        "\n",
        "    def evaluate_acc(self,yh, y):\n",
        "      return np.sum(yh == y)/yh.shape[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aup8tdGHtFrM",
        "outputId": "045244c8-58b1-483a-874a-9142d9710265"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "b'Mann photographs the Alberta Rocky Mountains in a superb fashion, and Jimmy Stewart and Walter Brennan give enjoyable performances as they always seem to do. <br /><br />But come on Hollywood - a Mountie telling the people of Dawson City, Yukon to elect themselves a marshal (yes a marshal!) and to enforce the law themselves, then gunfighters battling it out on the streets for control of the town? <br /><br />Nothing even remotely resembling that happened on the Canadian side of the border during the Klondike gold rush. Mr. Mann and company appear to have mistaken Dawson City for Deadwood, the Canadian North for the American Wild West.<br /><br />Canadian viewers be prepared for a Reefer Madness type of enjoyable howl with this ludicrous plot, or, to shake your head in disgust.'\n",
            "b'As others have mentioned, all the women that go nude in this film are mostly absolutely gorgeous. The plot very ably shows the hypocrisy of the female libido. When men are around they want to be pursued, but when no \"men\" are around, they become the pursuers of a 14 year old boy. And the boy becomes a man really fast (we should all be so lucky at this age!). He then gets up the courage to pursue his true love.'\n",
            "b'Okay, you have:<br /><br />Penelope Keith as Miss Herringbone-Tweed, B.B.E. (Backbone of England.) She\\'s killed off in the first scene - that\\'s right, folks; this show has no backbone!<br /><br />Peter O\\'Toole as Ol\\' Colonel Cricket from The First War and now the emblazered Lord of the Manor.<br /><br />Joanna Lumley as the ensweatered Lady of the Manor, 20 years younger than the colonel and 20 years past her own prime but still glamourous (Brit spelling, not mine) enough to have a toy-boy on the side. It\\'s alright, they have Col. Cricket\\'s full knowledge and consent (they guy even comes \\'round for Christmas!) Still, she\\'s considerate of the colonel enough to have said toy-boy her own age (what a gal!)<br /><br />David McCallum as said toy-boy, equally as pointlessly glamourous as his squeeze. Pilcher couldn\\'t come up with any cover for him within the story, so she gave him a hush-hush job at the Circus.<br /><br />and finally:<br /><br />Susan Hampshire as Miss Polonia Teacups, Venerable Headmistress of the Venerable Girls\\' Boarding-School, serving tea in her office with a dash of deep, poignant advice for life in the outside world just before graduation. Her best bit of advice: \"I\\'ve only been to Nancherrow (the local Stately Home of England) once. I thought it was very beautiful but, somehow, not part of the real world.\" Well, we can\\'t say they didn\\'t warn us.<br /><br />Ah, Susan - time was, your character would have been running the whole show. They don\\'t write \\'em like that any more. Our loss, not yours.<br /><br />So - with a cast and setting like this, you have the re-makings of \"Brideshead Revisited,\" right?<br /><br />Wrong! They took these 1-dimensional supporting roles because they paid so well. After all, acting is one of the oldest temp-jobs there is (YOU name another!)<br /><br />First warning sign: lots and lots of backlighting. They get around it by shooting outdoors - \"hey, it\\'s just the sunlight!\"<br /><br />Second warning sign: Leading Lady cries a lot. When not crying, her eyes are moist. That\\'s the law of romance novels: Leading Lady is \"dewy-eyed.\"<br /><br />Henceforth, Leading Lady shall be known as L.L.<br /><br />Third warning sign: L.L. actually has stars in her eyes when she\\'s in love. Still, I\\'ll give Emily Mortimer an award just for having to act with that spotlight in her eyes (I wonder . did they use contacts?)<br /><br />And lastly, fourth warning sign: no on-screen female character is \"Mrs.\" She\\'s either \"Miss\" or \"Lady.\"<br /><br />When all was said and done, I still couldn\\'t tell you who was pursuing whom and why. I couldn\\'t even tell you what was said and done.<br /><br />To sum up: they all live through World War II without anything happening to them at all.<br /><br />OK, at the end, L.L. finds she\\'s lost her parents to the Japanese prison camps and baby sis comes home catatonic. Meanwhile (there\\'s always a \"meanwhile,\") some young guy L.L. had a crush on (when, I don\\'t know) comes home from some wartime tough spot and is found living on the street by Lady of the Manor (must be some street if SHE\\'s going to find him there.) Both war casualties are whisked away to recover at Nancherrow (SOMEBODY has to be \"whisked away\" SOMEWHERE in these romance stories!)<br /><br />Great drama.'\n"
          ]
        }
      ],
      "source": [
        "print(x_train[2])\n",
        "print(x_train[4])\n",
        "print(x_train[6])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNG5pTuQtFrM"
      },
      "outputs": [],
      "source": [
        "stopword_list=nltk.corpus.stopwords.words('english')\n",
        "class LemmaTokenizer(object):\n",
        "    def __init__(self):\n",
        "        self.wnl = WordNetLemmatizer()\n",
        "    def __call__(self, articles):\n",
        "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-2cxTCatFrM"
      },
      "outputs": [],
      "source": [
        "def strip_html(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tv332zUZtFrM"
      },
      "source": [
        "## Fitting models with different kinds of preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5s2AGNf1tFrN"
      },
      "source": [
        "### No clean up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WScA0497tFrN"
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer()\n",
        "X_train = vectorizer.fit_transform(x_train)\n",
        "dic= vectorizer.get_feature_names_out()\n",
        "X_test=vectorizer.transform(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6kHj3DftFrN",
        "outputId": "86276492-afac-44a9-d60c-b9d739ffffaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dictionary length: 74849\n"
          ]
        }
      ],
      "source": [
        "print(\"Dictionary length:\", len(vectorizer.get_feature_names_out()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLggFl6ZtFrN"
      },
      "source": [
        "#### Without priors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FP7MmzbgtFrO",
        "outputId": "73fe79e4-c656-42dc-fb84-2bcc2c00eb54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-f4b4030e8c91>:47: RuntimeWarning: divide by zero encountered in log\n",
            "  log_likelihood[n, c]= np.sum(np.log(np.power(prob,x_test)), axis=1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.63992\n"
          ]
        }
      ],
      "source": [
        "model=MultinomialNaiveBayes()\n",
        "params=model.fit(X_train, y_train, 0,0,0,0)\n",
        "y_prob=model.predict(X_test)\n",
        "y_pred = np.argmax(y_prob, 1)\n",
        "print(\"Accuracy:\",model.evaluate_acc(y_pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKGst8W2tFrO"
      },
      "source": [
        "#### With priors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMwC_nQ6tFrO",
        "outputId": "25c698ba-d59e-4314-9908-f4c8490b0e56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.75916\n"
          ]
        }
      ],
      "source": [
        "model=MultinomialNaiveBayes()\n",
        "params=model.fit(X_train, y_train, 1,1,10,10)\n",
        "y_prob=model.predict(X_test)\n",
        "y_pred = np.argmax(y_prob, 1)\n",
        "print(\"Accuracy:\",model.evaluate_acc(y_pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpQFs_-gtFrO"
      },
      "source": [
        "### No clean up but with limits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9nPNs3ftFrP"
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer(min_df=0.01, max_df=0.4)\n",
        "X_train = vectorizer.fit_transform(x_train)\n",
        "dic= vectorizer.get_feature_names_out()\n",
        "X_test=vectorizer.transform(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JU9RSM2otFrP",
        "outputId": "2b3fe963-5070-4b1b-bfc2-8f8e0edc784e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dictionary length: 1764\n"
          ]
        }
      ],
      "source": [
        "print(\"Dictionary length:\", len(vectorizer.get_feature_names_out()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoQ1v-wStFrP"
      },
      "source": [
        "#### Without priors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5uqlS1ntFrP",
        "outputId": "3e7601f1-ba95-4623-be8f-8893a63fbc16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.83308\n"
          ]
        }
      ],
      "source": [
        "model=MultinomialNaiveBayes()\n",
        "params=model.fit(X_train, y_train, 0,0,0,0)\n",
        "y_prob=model.predict(X_test)\n",
        "y_pred = np.argmax(y_prob, 1)\n",
        "print(\"Accuracy:\",model.evaluate_acc(y_pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHvTcg1ytFrP"
      },
      "source": [
        "#### With priors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thWzOZbZtFrP",
        "outputId": "3d2b2135-5793-4b7f-9e46-1ef88616612b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.83272\n"
          ]
        }
      ],
      "source": [
        "model=MultinomialNaiveBayes()\n",
        "params=model.fit(X_train, y_train, 1,1,10,10)\n",
        "y_prob=model.predict(X_test)\n",
        "y_pred = np.argmax(y_prob, 1)\n",
        "print(\"Accuracy:\",model.evaluate_acc(y_pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5yRyICjtFrQ"
      },
      "source": [
        "### No clean up with different limits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QocezbpytFrQ"
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer(min_df=0.05, max_df=0.6)\n",
        "X_train = vectorizer.fit_transform(x_train)\n",
        "dic= vectorizer.get_feature_names_out()\n",
        "X_test=vectorizer.transform(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blx5mc9itFrQ",
        "outputId": "81dfa0d5-8545-4953-af39-7543317c09a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dictionary length: 413\n"
          ]
        }
      ],
      "source": [
        "print(\"Dictionary length:\", len(vectorizer.get_feature_names_out()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYhOwL8ftFrQ"
      },
      "source": [
        "#### Without priors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0p4o3uW2tFrR",
        "outputId": "d1a107ff-08a3-4b24-a5e2-483fccd70820"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.79024\n"
          ]
        }
      ],
      "source": [
        "model=MultinomialNaiveBayes()\n",
        "params=model.fit(X_train, y_train, 0,0,0,0)\n",
        "y_prob=model.predict(X_test)\n",
        "y_pred = np.argmax(y_prob, 1)\n",
        "print(\"Accuracy:\",model.evaluate_acc(y_pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0h8Pcpd1tFrR"
      },
      "source": [
        "#### With priors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BV8f5a8JtFrR",
        "outputId": "4d45e91a-ebc7-445a-dfae-3dce522e1b5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.78984\n"
          ]
        }
      ],
      "source": [
        "model=MultinomialNaiveBayes()\n",
        "params=model.fit(X_train, y_train, 1,1,10,10)\n",
        "y_prob=model.predict(X_test)\n",
        "y_pred = np.argmax(y_prob, 1)\n",
        "print(\"Accuracy:\",model.evaluate_acc(y_pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "is-aSD8LtFrR"
      },
      "source": [
        "### Stop words and limits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8MOTI-ftFrR"
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer(stop_words=stopword_list, min_df=0.01, max_df=0.4)\n",
        "X_train = vectorizer.fit_transform(x_train)\n",
        "dic= vectorizer.get_feature_names_out()\n",
        "X_test=vectorizer.transform(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFl_ewNjtFrS",
        "outputId": "8779e539-b90c-432e-a427-c6461d71e39a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dictionary length: 1668\n"
          ]
        }
      ],
      "source": [
        "print(\"Dictionary length:\", len(vectorizer.get_feature_names_out()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4mDIqHftFrT",
        "outputId": "52133e8b-915e-4c24-9a18-d9ce4cfc08be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "100\n",
            "11\n",
            "12\n",
            "15\n",
            "20\n",
            "30\n",
            "40\n",
            "50\n",
            "60\n",
            "70\n",
            "80\n",
            "90\n",
            "ability\n",
            "able\n",
            "absolute\n",
            "absolutely\n",
            "absurd\n",
            "academy\n",
            "accent\n",
            "accept\n",
            "accident\n",
            "according\n",
            "accurate\n",
            "across\n",
            "act\n",
            "acted\n",
            "acting\n",
            "action\n",
            "actions\n",
            "actor\n",
            "actors\n",
            "actress\n",
            "actresses\n",
            "acts\n",
            "actual\n",
            "actually\n",
            "adaptation\n",
            "add\n",
            "added\n",
            "addition\n",
            "adds\n",
            "admit\n",
            "adult\n",
            "adults\n",
            "adventure\n",
            "affair\n",
            "afraid\n",
            "age\n",
            "agent\n",
            "ago\n",
            "agree\n",
            "ahead\n",
            "air\n",
            "al\n",
            "alive\n",
            "allow\n",
            "allowed\n",
            "almost\n",
            "alone\n",
            "along\n",
            "already\n",
            "also\n",
            "although\n",
            "always\n",
            "amazing\n",
            "america\n",
            "american\n",
            "americans\n",
            "among\n",
            "amount\n",
            "amusing\n",
            "angry\n",
            "animal\n",
            "animals\n",
            "animated\n",
            "animation\n",
            "annoying\n",
            "another\n",
            "answer\n",
            "anti\n",
            "anybody\n",
            "anymore\n",
            "anyone\n",
            "anything\n",
            "anyway\n",
            "anywhere\n",
            "apart\n",
            "apartment\n",
            "apparent\n",
            "apparently\n",
            "appeal\n",
            "appear\n",
            "appearance\n",
            "appeared\n",
            "appears\n",
            "appreciate\n",
            "approach\n",
            "area\n",
            "army\n",
            "around\n",
            "art\n",
            "artist\n",
            "artistic\n",
            "aside\n",
            "ask\n",
            "asked\n",
            "asks\n",
            "aspect\n",
            "aspects\n",
            "atmosphere\n",
            "attack\n",
            "attempt\n",
            "attempts\n",
            "attention\n",
            "attractive\n",
            "audience\n",
            "audiences\n",
            "available\n",
            "average\n",
            "avoid\n",
            "award\n",
            "aware\n",
            "away\n",
            "awesome\n",
            "awful\n",
            "baby\n",
            "back\n",
            "background\n",
            "bad\n",
            "badly\n",
            "band\n",
            "bar\n",
            "barely\n",
            "based\n",
            "basic\n",
            "basically\n",
            "battle\n",
            "beat\n",
            "beautiful\n",
            "beautifully\n",
            "beauty\n",
            "became\n",
            "become\n",
            "becomes\n",
            "becoming\n",
            "bed\n",
            "began\n",
            "begin\n",
            "beginning\n",
            "begins\n",
            "behind\n",
            "believable\n",
            "believe\n",
            "ben\n",
            "besides\n",
            "best\n",
            "better\n",
            "beyond\n",
            "big\n",
            "biggest\n",
            "bill\n",
            "billy\n",
            "bit\n",
            "bits\n",
            "bizarre\n",
            "black\n",
            "blame\n",
            "bland\n",
            "blood\n",
            "bloody\n",
            "blue\n",
            "body\n",
            "book\n",
            "books\n",
            "bored\n",
            "boring\n",
            "born\n",
            "boss\n",
            "bother\n",
            "bottom\n",
            "bought\n",
            "box\n",
            "boy\n",
            "boyfriend\n",
            "boys\n",
            "brain\n",
            "break\n",
            "brian\n",
            "brief\n",
            "bright\n",
            "brilliant\n",
            "bring\n",
            "brings\n",
            "british\n",
            "brother\n",
            "brothers\n",
            "brought\n",
            "brutal\n",
            "budget\n",
            "build\n",
            "building\n",
            "bunch\n",
            "business\n",
            "buy\n",
            "cable\n",
            "call\n",
            "called\n",
            "came\n",
            "camera\n",
            "camp\n",
            "cannot\n",
            "capture\n",
            "car\n",
            "care\n",
            "career\n",
            "carry\n",
            "cartoon\n",
            "case\n",
            "cast\n",
            "casting\n",
            "cat\n",
            "catch\n",
            "caught\n",
            "cause\n",
            "central\n",
            "century\n",
            "certain\n",
            "certainly\n",
            "cgi\n",
            "chance\n",
            "change\n",
            "changed\n",
            "changes\n",
            "channel\n",
            "character\n",
            "characters\n",
            "charles\n",
            "charm\n",
            "charming\n",
            "chase\n",
            "cheap\n",
            "check\n",
            "cheesy\n",
            "chemistry\n",
            "child\n",
            "childhood\n",
            "children\n",
            "choice\n",
            "chris\n",
            "christmas\n",
            "christopher\n",
            "church\n",
            "cinema\n",
            "cinematic\n",
            "cinematography\n",
            "city\n",
            "class\n",
            "classic\n",
            "clear\n",
            "clearly\n",
            "clever\n",
            "cliché\n",
            "clichés\n",
            "climax\n",
            "close\n",
            "clothes\n",
            "club\n",
            "co\n",
            "cold\n",
            "collection\n",
            "college\n",
            "color\n",
            "come\n",
            "comedic\n",
            "comedies\n",
            "comedy\n",
            "comes\n",
            "comic\n",
            "coming\n",
            "comment\n",
            "commentary\n",
            "comments\n",
            "common\n",
            "community\n",
            "company\n",
            "compare\n",
            "compared\n",
            "compelling\n",
            "complete\n",
            "completely\n",
            "complex\n",
            "computer\n",
            "concept\n",
            "concerned\n",
            "conclusion\n",
            "confused\n",
            "confusing\n",
            "consider\n",
            "considered\n",
            "considering\n",
            "constant\n",
            "constantly\n",
            "contains\n",
            "content\n",
            "continue\n",
            "continues\n",
            "control\n",
            "convincing\n",
            "cool\n",
            "cop\n",
            "copy\n",
            "costumes\n",
            "could\n",
            "count\n",
            "country\n",
            "couple\n",
            "course\n",
            "cover\n",
            "crap\n",
            "crazy\n",
            "create\n",
            "created\n",
            "creating\n",
            "creative\n",
            "creature\n",
            "credit\n",
            "credits\n",
            "creepy\n",
            "crew\n",
            "crime\n",
            "criminal\n",
            "critics\n",
            "cross\n",
            "cry\n",
            "cult\n",
            "culture\n",
            "cut\n",
            "cute\n",
            "cuts\n",
            "dad\n",
            "damn\n",
            "dance\n",
            "dancing\n",
            "dangerous\n",
            "dark\n",
            "date\n",
            "daughter\n",
            "david\n",
            "day\n",
            "days\n",
            "de\n",
            "dead\n",
            "deal\n",
            "dealing\n",
            "death\n",
            "decent\n",
            "decide\n",
            "decided\n",
            "decides\n",
            "deep\n",
            "deeply\n",
            "definitely\n",
            "delightful\n",
            "deliver\n",
            "delivers\n",
            "depth\n",
            "describe\n",
            "deserve\n",
            "deserved\n",
            "deserves\n",
            "design\n",
            "desire\n",
            "desperate\n",
            "despite\n",
            "detail\n",
            "details\n",
            "detective\n",
            "developed\n",
            "development\n",
            "dialog\n",
            "dialogue\n",
            "die\n",
            "died\n",
            "dies\n",
            "difference\n",
            "different\n",
            "difficult\n",
            "direct\n",
            "directed\n",
            "directing\n",
            "direction\n",
            "director\n",
            "directors\n",
            "dirty\n",
            "disappointed\n",
            "disappointing\n",
            "disappointment\n",
            "disaster\n",
            "discover\n",
            "discovered\n",
            "disney\n",
            "disturbing\n",
            "doctor\n",
            "documentary\n",
            "dog\n",
            "done\n",
            "door\n",
            "double\n",
            "doubt\n",
            "dr\n",
            "drama\n",
            "dramatic\n",
            "drawn\n",
            "dream\n",
            "dreams\n",
            "drive\n",
            "driving\n",
            "drug\n",
            "drugs\n",
            "drunk\n",
            "due\n",
            "dull\n",
            "dumb\n",
            "dvd\n",
            "dying\n",
            "earlier\n",
            "early\n",
            "earth\n",
            "easily\n",
            "easy\n",
            "ed\n",
            "edge\n",
            "editing\n",
            "effect\n",
            "effective\n",
            "effects\n",
            "effort\n",
            "either\n",
            "element\n",
            "elements\n",
            "else\n",
            "emotion\n",
            "emotional\n",
            "emotions\n",
            "empty\n",
            "end\n",
            "ended\n",
            "ending\n",
            "ends\n",
            "energy\n",
            "engaging\n",
            "england\n",
            "english\n",
            "enjoy\n",
            "enjoyable\n",
            "enjoyed\n",
            "enough\n",
            "entertaining\n",
            "entertainment\n",
            "entire\n",
            "entirely\n",
            "epic\n",
            "episode\n",
            "episodes\n",
            "equally\n",
            "era\n",
            "escape\n",
            "especially\n",
            "essentially\n",
            "etc\n",
            "european\n",
            "even\n",
            "event\n",
            "events\n",
            "eventually\n",
            "ever\n",
            "every\n",
            "everybody\n",
            "everyone\n",
            "everything\n",
            "evil\n",
            "ex\n",
            "exactly\n",
            "example\n",
            "excellent\n",
            "except\n",
            "exception\n",
            "exciting\n",
            "excuse\n",
            "exist\n",
            "expect\n",
            "expectations\n",
            "expected\n",
            "expecting\n",
            "experience\n",
            "explain\n",
            "explained\n",
            "explanation\n",
            "extra\n",
            "extreme\n",
            "extremely\n",
            "eye\n",
            "eyes\n",
            "face\n",
            "faces\n",
            "fact\n",
            "fail\n",
            "failed\n",
            "fails\n",
            "fair\n",
            "fairly\n",
            "fake\n",
            "fall\n",
            "falling\n",
            "falls\n",
            "familiar\n",
            "family\n",
            "famous\n",
            "fan\n",
            "fans\n",
            "fantastic\n",
            "fantasy\n",
            "far\n",
            "fascinating\n",
            "fashion\n",
            "fast\n",
            "father\n",
            "favorite\n",
            "favourite\n",
            "fear\n",
            "feature\n",
            "features\n",
            "featuring\n",
            "feel\n",
            "feeling\n",
            "feelings\n",
            "feels\n",
            "fell\n",
            "fellow\n",
            "felt\n",
            "female\n",
            "festival\n",
            "fi\n",
            "fiction\n",
            "field\n",
            "fight\n",
            "fighting\n",
            "fights\n",
            "figure\n",
            "filled\n",
            "filmed\n",
            "filming\n",
            "filmmaker\n",
            "filmmakers\n",
            "films\n",
            "final\n",
            "finally\n",
            "find\n",
            "finding\n",
            "finds\n",
            "fine\n",
            "finest\n",
            "finish\n",
            "finished\n",
            "fire\n",
            "first\n",
            "fit\n",
            "five\n",
            "flat\n",
            "flaws\n",
            "flick\n",
            "flicks\n",
            "floor\n",
            "flying\n",
            "focus\n",
            "folks\n",
            "follow\n",
            "followed\n",
            "following\n",
            "follows\n",
            "food\n",
            "footage\n",
            "force\n",
            "forced\n",
            "forever\n",
            "forget\n",
            "forgotten\n",
            "form\n",
            "former\n",
            "forward\n",
            "found\n",
            "four\n",
            "frank\n",
            "free\n",
            "french\n",
            "fresh\n",
            "friend\n",
            "friends\n",
            "front\n",
            "full\n",
            "fully\n",
            "fun\n",
            "funniest\n",
            "funny\n",
            "future\n",
            "game\n",
            "gang\n",
            "garbage\n",
            "gave\n",
            "gay\n",
            "gem\n",
            "general\n",
            "generally\n",
            "genius\n",
            "genre\n",
            "george\n",
            "german\n",
            "get\n",
            "gets\n",
            "getting\n",
            "ghost\n",
            "giant\n",
            "girl\n",
            "girlfriend\n",
            "girls\n",
            "give\n",
            "given\n",
            "gives\n",
            "giving\n",
            "glad\n",
            "go\n",
            "god\n",
            "goes\n",
            "going\n",
            "gone\n",
            "good\n",
            "gore\n",
            "gorgeous\n",
            "got\n",
            "gotten\n",
            "government\n",
            "grace\n",
            "grade\n",
            "grand\n",
            "great\n",
            "greatest\n",
            "green\n",
            "ground\n",
            "group\n",
            "growing\n",
            "guess\n",
            "gun\n",
            "guy\n",
            "guys\n",
            "hair\n",
            "half\n",
            "hand\n",
            "hands\n",
            "happen\n",
            "happened\n",
            "happening\n",
            "happens\n",
            "happy\n",
            "hard\n",
            "hardly\n",
            "hate\n",
            "hated\n",
            "head\n",
            "heads\n",
            "hear\n",
            "heard\n",
            "heart\n",
            "heavy\n",
            "held\n",
            "hell\n",
            "help\n",
            "helped\n",
            "helps\n",
            "hero\n",
            "heroes\n",
            "hey\n",
            "hidden\n",
            "high\n",
            "higher\n",
            "highly\n",
            "hilarious\n",
            "historical\n",
            "history\n",
            "hit\n",
            "hits\n",
            "hold\n",
            "holds\n",
            "holes\n",
            "hollywood\n",
            "home\n",
            "honest\n",
            "honestly\n",
            "hope\n",
            "hopes\n",
            "hoping\n",
            "horrible\n",
            "horror\n",
            "hospital\n",
            "hot\n",
            "hotel\n",
            "hour\n",
            "hours\n",
            "house\n",
            "however\n",
            "huge\n",
            "human\n",
            "humor\n",
            "humour\n",
            "hurt\n",
            "husband\n",
            "idea\n",
            "ideas\n",
            "ii\n",
            "ill\n",
            "image\n",
            "images\n",
            "imagination\n",
            "imagine\n",
            "imdb\n",
            "immediately\n",
            "impact\n",
            "important\n",
            "impossible\n",
            "impressed\n",
            "impression\n",
            "impressive\n",
            "include\n",
            "included\n",
            "includes\n",
            "including\n",
            "incredible\n",
            "incredibly\n",
            "indeed\n",
            "independent\n",
            "indian\n",
            "industry\n",
            "information\n",
            "innocent\n",
            "inside\n",
            "inspired\n",
            "instance\n",
            "instead\n",
            "intelligence\n",
            "intelligent\n",
            "intended\n",
            "intense\n",
            "interest\n",
            "interested\n",
            "interesting\n",
            "intriguing\n",
            "introduced\n",
            "involved\n",
            "involving\n",
            "island\n",
            "issue\n",
            "issues\n",
            "italian\n",
            "jack\n",
            "james\n",
            "jane\n",
            "japanese\n",
            "jason\n",
            "jean\n",
            "jim\n",
            "job\n",
            "joe\n",
            "john\n",
            "joke\n",
            "jokes\n",
            "jones\n",
            "journey\n",
            "joy\n",
            "jr\n",
            "jump\n",
            "justice\n",
            "keep\n",
            "keeping\n",
            "keeps\n",
            "kept\n",
            "key\n",
            "kid\n",
            "kids\n",
            "kill\n",
            "killed\n",
            "killer\n",
            "killing\n",
            "kills\n",
            "kind\n",
            "kinda\n",
            "king\n",
            "knew\n",
            "know\n",
            "knowing\n",
            "knowledge\n",
            "known\n",
            "knows\n",
            "la\n",
            "lack\n",
            "lacking\n",
            "lacks\n",
            "ladies\n",
            "lady\n",
            "lame\n",
            "land\n",
            "language\n",
            "large\n",
            "last\n",
            "late\n",
            "later\n",
            "latter\n",
            "laugh\n",
            "laughable\n",
            "laughed\n",
            "laughing\n",
            "laughs\n",
            "law\n",
            "lead\n",
            "leading\n",
            "leads\n",
            "learn\n",
            "least\n",
            "leave\n",
            "leaves\n",
            "leaving\n",
            "led\n",
            "lee\n",
            "left\n",
            "length\n",
            "less\n",
            "let\n",
            "lets\n",
            "level\n",
            "lies\n",
            "life\n",
            "light\n",
            "lighting\n",
            "likable\n",
            "liked\n",
            "likely\n",
            "likes\n",
            "limited\n",
            "line\n",
            "lines\n",
            "list\n",
            "listen\n",
            "literally\n",
            "little\n",
            "live\n",
            "lived\n",
            "lives\n",
            "living\n",
            "local\n",
            "location\n",
            "london\n",
            "long\n",
            "longer\n",
            "look\n",
            "looked\n",
            "looking\n",
            "looks\n",
            "loose\n",
            "lord\n",
            "lose\n",
            "lost\n",
            "lot\n",
            "lots\n",
            "loud\n",
            "love\n",
            "loved\n",
            "lovely\n",
            "lover\n",
            "lovers\n",
            "loves\n",
            "loving\n",
            "low\n",
            "lucky\n",
            "machine\n",
            "mad\n",
            "made\n",
            "magic\n",
            "main\n",
            "mainly\n",
            "major\n",
            "make\n",
            "makers\n",
            "makes\n",
            "making\n",
            "male\n",
            "man\n",
            "manage\n",
            "managed\n",
            "manages\n",
            "manner\n",
            "many\n",
            "mark\n",
            "marriage\n",
            "married\n",
            "martin\n",
            "mary\n",
            "master\n",
            "masterpiece\n",
            "match\n",
            "material\n",
            "matter\n",
            "may\n",
            "maybe\n",
            "mean\n",
            "meaning\n",
            "means\n",
            "meant\n",
            "mediocre\n",
            "meet\n",
            "meets\n",
            "member\n",
            "members\n",
            "memorable\n",
            "memories\n",
            "memory\n",
            "men\n",
            "mental\n",
            "mention\n",
            "mentioned\n",
            "merely\n",
            "mess\n",
            "message\n",
            "met\n",
            "michael\n",
            "mid\n",
            "middle\n",
            "might\n",
            "military\n",
            "million\n",
            "mind\n",
            "mine\n",
            "minor\n",
            "minute\n",
            "minutes\n",
            "miss\n",
            "missed\n",
            "missing\n",
            "mistake\n",
            "mix\n",
            "mixed\n",
            "modern\n",
            "mom\n",
            "moment\n",
            "moments\n",
            "money\n",
            "monster\n",
            "months\n",
            "mood\n",
            "moral\n",
            "mostly\n",
            "mother\n",
            "motion\n",
            "mouth\n",
            "move\n",
            "moved\n",
            "moves\n",
            "movies\n",
            "moving\n",
            "mr\n",
            "ms\n",
            "much\n",
            "murder\n",
            "murders\n",
            "music\n",
            "musical\n",
            "must\n",
            "mysterious\n",
            "mystery\n",
            "naked\n",
            "name\n",
            "named\n",
            "names\n",
            "narrative\n",
            "nasty\n",
            "natural\n",
            "naturally\n",
            "nature\n",
            "near\n",
            "nearly\n",
            "necessary\n",
            "need\n",
            "needed\n",
            "needs\n",
            "negative\n",
            "neither\n",
            "never\n",
            "new\n",
            "news\n",
            "next\n",
            "nice\n",
            "nicely\n",
            "night\n",
            "nobody\n",
            "noir\n",
            "non\n",
            "none\n",
            "nonsense\n",
            "normal\n",
            "normally\n",
            "note\n",
            "nothing\n",
            "notice\n",
            "noticed\n",
            "novel\n",
            "nowhere\n",
            "nudity\n",
            "number\n",
            "numbers\n",
            "numerous\n",
            "obvious\n",
            "obviously\n",
            "odd\n",
            "offer\n",
            "offers\n",
            "office\n",
            "often\n",
            "oh\n",
            "ok\n",
            "okay\n",
            "old\n",
            "older\n",
            "ones\n",
            "onto\n",
            "open\n",
            "opening\n",
            "opens\n",
            "opera\n",
            "opinion\n",
            "opportunity\n",
            "opposite\n",
            "order\n",
            "original\n",
            "originally\n",
            "oscar\n",
            "others\n",
            "otherwise\n",
            "outside\n",
            "outstanding\n",
            "overall\n",
            "pace\n",
            "paced\n",
            "pacing\n",
            "paid\n",
            "pain\n",
            "painful\n",
            "parents\n",
            "park\n",
            "part\n",
            "particular\n",
            "particularly\n",
            "partner\n",
            "parts\n",
            "party\n",
            "pass\n",
            "passion\n",
            "past\n",
            "pathetic\n",
            "paul\n",
            "pay\n",
            "people\n",
            "perfect\n",
            "perfectly\n",
            "performance\n",
            "performances\n",
            "perhaps\n",
            "period\n",
            "person\n",
            "personal\n",
            "personality\n",
            "personally\n",
            "peter\n",
            "phone\n",
            "photography\n",
            "physical\n",
            "pick\n",
            "picked\n",
            "picture\n",
            "pictures\n",
            "piece\n",
            "pieces\n",
            "place\n",
            "places\n",
            "plain\n",
            "plan\n",
            "planet\n",
            "play\n",
            "played\n",
            "player\n",
            "players\n",
            "playing\n",
            "plays\n",
            "please\n",
            "pleasure\n",
            "plenty\n",
            "plot\n",
            "plots\n",
            "plus\n",
            "point\n",
            "pointless\n",
            "points\n",
            "police\n",
            "political\n",
            "poor\n",
            "poorly\n",
            "pop\n",
            "popular\n",
            "porn\n",
            "portray\n",
            "portrayal\n",
            "portrayed\n",
            "positive\n",
            "possible\n",
            "possibly\n",
            "post\n",
            "potential\n",
            "power\n",
            "powerful\n",
            "powers\n",
            "pre\n",
            "predictable\n",
            "premise\n",
            "presence\n",
            "present\n",
            "presented\n",
            "pretty\n",
            "previous\n",
            "prison\n",
            "probably\n",
            "problem\n",
            "problems\n",
            "process\n",
            "produced\n",
            "producer\n",
            "producers\n",
            "production\n",
            "professional\n",
            "project\n",
            "prove\n",
            "proves\n",
            "provide\n",
            "provides\n",
            "public\n",
            "pull\n",
            "pulled\n",
            "pure\n",
            "purpose\n",
            "put\n",
            "puts\n",
            "putting\n",
            "quality\n",
            "queen\n",
            "question\n",
            "questions\n",
            "quick\n",
            "quickly\n",
            "quiet\n",
            "quite\n",
            "race\n",
            "random\n",
            "rape\n",
            "rare\n",
            "rarely\n",
            "rate\n",
            "rated\n",
            "rather\n",
            "rating\n",
            "ray\n",
            "read\n",
            "reading\n",
            "ready\n",
            "real\n",
            "realism\n",
            "realistic\n",
            "reality\n",
            "realize\n",
            "realized\n",
            "really\n",
            "reason\n",
            "reasons\n",
            "received\n",
            "recent\n",
            "recently\n",
            "recommend\n",
            "recommended\n",
            "record\n",
            "red\n",
            "redeeming\n",
            "regular\n",
            "relationship\n",
            "relationships\n",
            "release\n",
            "released\n",
            "remains\n",
            "remake\n",
            "remarkable\n",
            "remember\n",
            "reminded\n",
            "reminds\n",
            "rent\n",
            "rented\n",
            "respect\n",
            "responsible\n",
            "rest\n",
            "result\n",
            "results\n",
            "return\n",
            "returns\n",
            "revenge\n",
            "review\n",
            "reviews\n",
            "rich\n",
            "richard\n",
            "ride\n",
            "ridiculous\n",
            "right\n",
            "rip\n",
            "road\n",
            "robert\n",
            "rock\n",
            "role\n",
            "roles\n",
            "roll\n",
            "romance\n",
            "romantic\n",
            "room\n",
            "rubbish\n",
            "run\n",
            "running\n",
            "runs\n",
            "sad\n",
            "sadly\n",
            "said\n",
            "sam\n",
            "sat\n",
            "save\n",
            "saved\n",
            "saving\n",
            "saw\n",
            "say\n",
            "saying\n",
            "says\n",
            "scared\n",
            "scary\n",
            "scene\n",
            "scenery\n",
            "scenes\n",
            "school\n",
            "sci\n",
            "science\n",
            "score\n",
            "scott\n",
            "screen\n",
            "screenplay\n",
            "script\n",
            "search\n",
            "season\n",
            "second\n",
            "seconds\n",
            "secret\n",
            "see\n",
            "seeing\n",
            "seem\n",
            "seemed\n",
            "seemingly\n",
            "seems\n",
            "seen\n",
            "sees\n",
            "self\n",
            "sense\n",
            "sent\n",
            "sequel\n",
            "sequence\n",
            "sequences\n",
            "serial\n",
            "series\n",
            "serious\n",
            "seriously\n",
            "set\n",
            "sets\n",
            "setting\n",
            "seven\n",
            "several\n",
            "sex\n",
            "sexual\n",
            "sexy\n",
            "shame\n",
            "share\n",
            "shock\n",
            "shocking\n",
            "shoot\n",
            "shooting\n",
            "short\n",
            "shot\n",
            "shots\n",
            "show\n",
            "showed\n",
            "showing\n",
            "shown\n",
            "shows\n",
            "sick\n",
            "side\n",
            "sight\n",
            "sign\n",
            "silent\n",
            "silly\n",
            "similar\n",
            "simple\n",
            "simply\n",
            "since\n",
            "singing\n",
            "single\n",
            "sister\n",
            "sit\n",
            "sitting\n",
            "situation\n",
            "situations\n",
            "six\n",
            "skip\n",
            "slasher\n",
            "sleep\n",
            "slightly\n",
            "slow\n",
            "slowly\n",
            "small\n",
            "smart\n",
            "smile\n",
            "smith\n",
            "social\n",
            "society\n",
            "soft\n",
            "soldiers\n",
            "solid\n",
            "somebody\n",
            "somehow\n",
            "someone\n",
            "something\n",
            "sometimes\n",
            "somewhat\n",
            "somewhere\n",
            "son\n",
            "song\n",
            "songs\n",
            "soon\n",
            "sorry\n",
            "sort\n",
            "soul\n",
            "sound\n",
            "sounds\n",
            "soundtrack\n",
            "south\n",
            "space\n",
            "speak\n",
            "speaking\n",
            "special\n",
            "spend\n",
            "spent\n",
            "spirit\n",
            "spoiler\n",
            "spoilers\n",
            "spot\n",
            "stage\n",
            "stand\n",
            "standard\n",
            "standards\n",
            "stands\n",
            "star\n",
            "starring\n",
            "stars\n",
            "start\n",
            "started\n",
            "starting\n",
            "starts\n",
            "state\n",
            "states\n",
            "station\n",
            "stay\n",
            "step\n",
            "stephen\n",
            "steve\n",
            "stick\n",
            "still\n",
            "stone\n",
            "stop\n",
            "store\n",
            "stories\n",
            "story\n",
            "storyline\n",
            "straight\n",
            "strange\n",
            "street\n",
            "strong\n",
            "struggle\n",
            "stuck\n",
            "student\n",
            "students\n",
            "studio\n",
            "stuff\n",
            "stunning\n",
            "stupid\n",
            "style\n",
            "sub\n",
            "subject\n",
            "subtle\n",
            "success\n",
            "successful\n",
            "sucks\n",
            "suddenly\n",
            "suggest\n",
            "suicide\n",
            "suit\n",
            "summer\n",
            "super\n",
            "superb\n",
            "superior\n",
            "support\n",
            "supporting\n",
            "suppose\n",
            "supposed\n",
            "supposedly\n",
            "sure\n",
            "surely\n",
            "surprise\n",
            "surprised\n",
            "surprising\n",
            "surprisingly\n",
            "suspect\n",
            "suspense\n",
            "sweet\n",
            "system\n",
            "take\n",
            "taken\n",
            "takes\n",
            "taking\n",
            "tale\n",
            "talent\n",
            "talented\n",
            "talents\n",
            "talk\n",
            "talking\n",
            "taste\n",
            "teacher\n",
            "team\n",
            "tears\n",
            "technical\n",
            "teen\n",
            "teenage\n",
            "television\n",
            "tell\n",
            "telling\n",
            "tells\n",
            "ten\n",
            "tension\n",
            "terms\n",
            "terrible\n",
            "terribly\n",
            "terrific\n",
            "thank\n",
            "thanks\n",
            "thats\n",
            "theater\n",
            "theatre\n",
            "theme\n",
            "themes\n",
            "therefore\n",
            "thin\n",
            "thing\n",
            "things\n",
            "think\n",
            "thinking\n",
            "thinks\n",
            "third\n",
            "thoroughly\n",
            "though\n",
            "thought\n",
            "three\n",
            "thriller\n",
            "throughout\n",
            "throw\n",
            "thrown\n",
            "thus\n",
            "time\n",
            "times\n",
            "tired\n",
            "title\n",
            "today\n",
            "together\n",
            "told\n",
            "tom\n",
            "tone\n",
            "tony\n",
            "took\n",
            "top\n",
            "torture\n",
            "total\n",
            "totally\n",
            "touch\n",
            "touching\n",
            "tough\n",
            "toward\n",
            "towards\n",
            "town\n",
            "track\n",
            "tragedy\n",
            "tragic\n",
            "trailer\n",
            "train\n",
            "trash\n",
            "treat\n",
            "treated\n",
            "tried\n",
            "tries\n",
            "trip\n",
            "trouble\n",
            "true\n",
            "truly\n",
            "trust\n",
            "truth\n",
            "try\n",
            "trying\n",
            "turn\n",
            "turned\n",
            "turning\n",
            "turns\n",
            "tv\n",
            "twenty\n",
            "twice\n",
            "twist\n",
            "twists\n",
            "two\n",
            "type\n",
            "typical\n",
            "ugly\n",
            "ultimately\n",
            "unbelievable\n",
            "understand\n",
            "understanding\n",
            "unfortunately\n",
            "unique\n",
            "unknown\n",
            "unless\n",
            "unlike\n",
            "unnecessary\n",
            "unusual\n",
            "upon\n",
            "ups\n",
            "us\n",
            "use\n",
            "used\n",
            "uses\n",
            "using\n",
            "usual\n",
            "usually\n",
            "utterly\n",
            "value\n",
            "values\n",
            "van\n",
            "various\n",
            "version\n",
            "vhs\n",
            "victim\n",
            "victims\n",
            "video\n",
            "view\n",
            "viewer\n",
            "viewers\n",
            "viewing\n",
            "villain\n",
            "violence\n",
            "violent\n",
            "vision\n",
            "visit\n",
            "visual\n",
            "voice\n",
            "wait\n",
            "waiting\n",
            "walk\n",
            "walking\n",
            "wall\n",
            "want\n",
            "wanted\n",
            "wanting\n",
            "wants\n",
            "war\n",
            "warning\n",
            "waste\n",
            "wasted\n",
            "watch\n",
            "watchable\n",
            "watched\n",
            "watching\n",
            "water\n",
            "way\n",
            "ways\n",
            "weak\n",
            "wearing\n",
            "week\n",
            "weird\n",
            "well\n",
            "went\n",
            "west\n",
            "western\n",
            "whatever\n",
            "whatsoever\n",
            "whenever\n",
            "whether\n",
            "white\n",
            "whole\n",
            "whose\n",
            "wide\n",
            "wife\n",
            "wild\n",
            "william\n",
            "willing\n",
            "win\n",
            "winning\n",
            "wise\n",
            "wish\n",
            "within\n",
            "without\n",
            "witty\n",
            "woman\n",
            "women\n",
            "wonder\n",
            "wonderful\n",
            "wonderfully\n",
            "wondering\n",
            "wooden\n",
            "woods\n",
            "word\n",
            "words\n",
            "work\n",
            "worked\n",
            "working\n",
            "works\n",
            "world\n",
            "worse\n",
            "worst\n",
            "worth\n",
            "worthy\n",
            "would\n",
            "wow\n",
            "write\n",
            "writer\n",
            "writers\n",
            "writing\n",
            "written\n",
            "wrong\n",
            "wrote\n",
            "yeah\n",
            "year\n",
            "years\n",
            "yes\n",
            "yet\n",
            "york\n",
            "young\n",
            "younger\n",
            "zero\n",
            "zombie\n",
            "zombies\n"
          ]
        }
      ],
      "source": [
        "for d in vectorizer.get_feature_names_out():\n",
        "  print(d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ON9LsmGwtFrU"
      },
      "source": [
        "#### Without priors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijnYwo1MtFrU",
        "outputId": "97308d52-7220-4012-8eb8-00ca1909f716"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.84604\n"
          ]
        }
      ],
      "source": [
        "model=MultinomialNaiveBayes()\n",
        "params=model.fit(X_train, y_train, 0,0,0,0)\n",
        "y_prob=model.predict(X_test)\n",
        "y_pred = np.argmax(y_prob, 1)\n",
        "print(\"Accuracy:\",model.evaluate_acc(y_pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB6RdoTvtFrU"
      },
      "source": [
        "#### With priors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMvN4mivtFrU",
        "outputId": "f3109671-f2f6-4c7b-a773-c30cb158b2a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.846\n"
          ]
        }
      ],
      "source": [
        "model=MultinomialNaiveBayes()\n",
        "params=model.fit(X_train, y_train, 1,1,10, 10)\n",
        "y_prob=model.predict(X_test)\n",
        "y_pred = np.argmax(y_prob, 1)\n",
        "print(\"Accuracy:\",model.evaluate_acc(y_pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qilAfb3hUmsr"
      },
      "source": [
        "### Analysis with different limits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swjyXVcoUp7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "300c0f2d-4024-4126-ce12-9056b1062d26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for lower limit 0 and higher limit 0.35: 0.79912 ; Length of dictionary:  74698\n",
            "Accuracy for lower limit 0 and higher limit 0.4: 0.79932 ; Length of dictionary:  74699\n",
            "Accuracy for lower limit 0 and higher limit 0.45: 0.79932 ; Length of dictionary:  74699\n",
            "Accuracy for lower limit 0 and higher limit 0.5: 0.79864 ; Length of dictionary:  74700\n",
            "Accuracy for lower limit 0 and higher limit 0.55: 0.79864 ; Length of dictionary:  74700\n",
            "Accuracy for lower limit 0 and higher limit 0.6: 0.79876 ; Length of dictionary:  74703\n",
            "Accuracy for lower limit 0 and higher limit 0.65: 0.79616 ; Length of dictionary:  74704\n",
            "Accuracy for lower limit 0.005 and higher limit 0.35: 0.84032 ; Length of dictionary:  3070\n",
            "Accuracy for lower limit 0.005 and higher limit 0.4: 0.84028 ; Length of dictionary:  3071\n",
            "Accuracy for lower limit 0.005 and higher limit 0.45: 0.84028 ; Length of dictionary:  3071\n",
            "Accuracy for lower limit 0.005 and higher limit 0.5: 0.83912 ; Length of dictionary:  3072\n",
            "Accuracy for lower limit 0.005 and higher limit 0.55: 0.83912 ; Length of dictionary:  3072\n",
            "Accuracy for lower limit 0.005 and higher limit 0.6: 0.83672 ; Length of dictionary:  3075\n",
            "Accuracy for lower limit 0.005 and higher limit 0.65: 0.8336 ; Length of dictionary:  3076\n",
            "Accuracy for lower limit 0.01 and higher limit 0.35: 0.84508 ; Length of dictionary:  1667\n",
            "Accuracy for lower limit 0.01 and higher limit 0.4: 0.846 ; Length of dictionary:  1668\n",
            "Accuracy for lower limit 0.01 and higher limit 0.45: 0.846 ; Length of dictionary:  1668\n",
            "Accuracy for lower limit 0.01 and higher limit 0.5: 0.84428 ; Length of dictionary:  1669\n",
            "Accuracy for lower limit 0.01 and higher limit 0.55: 0.84428 ; Length of dictionary:  1669\n",
            "Accuracy for lower limit 0.01 and higher limit 0.6: 0.84488 ; Length of dictionary:  1672\n",
            "Accuracy for lower limit 0.01 and higher limit 0.65: 0.84124 ; Length of dictionary:  1673\n",
            "Accuracy for lower limit 0.015 and higher limit 0.35: 0.842 ; Length of dictionary:  1158\n",
            "Accuracy for lower limit 0.015 and higher limit 0.4: 0.8416 ; Length of dictionary:  1159\n",
            "Accuracy for lower limit 0.015 and higher limit 0.45: 0.8416 ; Length of dictionary:  1159\n",
            "Accuracy for lower limit 0.015 and higher limit 0.5: 0.84116 ; Length of dictionary:  1160\n",
            "Accuracy for lower limit 0.015 and higher limit 0.55: 0.84116 ; Length of dictionary:  1160\n",
            "Accuracy for lower limit 0.015 and higher limit 0.6: 0.81532 ; Length of dictionary:  1163\n",
            "Accuracy for lower limit 0.015 and higher limit 0.65: 0.81648 ; Length of dictionary:  1164\n"
          ]
        }
      ],
      "source": [
        "lower_limits=[0, 0.005, 0.01, 0.015]\n",
        "upper_limits=[0.35 , 0.4, 0.45, 0.5, 0.55, 0.6, 0.65]\n",
        "for l in lower_limits:\n",
        "  for u in upper_limits:\n",
        "    vectorizer = CountVectorizer(stop_words=stopword_list, min_df=l, max_df=u)\n",
        "    X_train = vectorizer.fit_transform(x_train)\n",
        "    dic= vectorizer.get_feature_names_out()\n",
        "    X_test=vectorizer.transform(x_test)\n",
        "    model=MultinomialNaiveBayes()\n",
        "    params=model.fit(X_train, y_train, 1,1,10, 10)\n",
        "    y_prob=model.predict(X_test)\n",
        "    y_pred = np.argmax(y_prob, 1)\n",
        "    print(\"Accuracy for lower limit \"+str(l)+ \" and higher limit \"+ str(u)+\":\",model.evaluate_acc(y_pred, y_test), \"; Length of dictionary: \", len(dic))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc2uVXWRtFrU"
      },
      "source": [
        "### Lemmatizer, stop words and limits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBX7fZRytFrU"
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer(stop_words=stopword_list, tokenizer=LemmaTokenizer(), min_df=0.01, max_df=0.4)\n",
        "X_train = vectorizer.fit_transform(x_train)\n",
        "dic= vectorizer.get_feature_names_out()\n",
        "X_test=vectorizer.transform(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuLstE0jtFrV",
        "outputId": "bbc9a66c-a717-4c11-be83-bbaa02a05469"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dictionary length: 1621\n"
          ]
        }
      ],
      "source": [
        "print(\"Dictionary length:\", len(vectorizer.get_feature_names_out()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1D6ss9dtFrV",
        "outputId": "1ed7d815-f13b-47cd-f8d3-143f9bb54a22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!\n",
            "$\n",
            "%\n",
            "&\n",
            "'\n",
            "'d\n",
            "'ll\n",
            "'m\n",
            "'re\n",
            "'the\n",
            "'ve\n",
            "*\n",
            "-\n",
            "--\n",
            "..\n",
            "...\n",
            "....\n",
            "1\n",
            "10\n",
            "10/10\n",
            "100\n",
            "15\n",
            "2\n",
            "20\n",
            "3\n",
            "30\n",
            "4\n",
            "40\n",
            "5\n",
            "50\n",
            "6\n",
            "60\n",
            "7\n",
            "70\n",
            "8\n",
            "80\n",
            "9\n",
            "90\n",
            ":\n",
            ";\n",
            "?\n",
            "ability\n",
            "able\n",
            "absolute\n",
            "absolutely\n",
            "absurd\n",
            "academy\n",
            "accent\n",
            "accept\n",
            "accident\n",
            "according\n",
            "accurate\n",
            "across\n",
            "act\n",
            "acted\n",
            "acting\n",
            "action\n",
            "actor\n",
            "actress\n",
            "actual\n",
            "actually\n",
            "adam\n",
            "adaptation\n",
            "add\n",
            "added\n",
            "addition\n",
            "admit\n",
            "adult\n",
            "adventure\n",
            "affair\n",
            "afraid\n",
            "age\n",
            "agent\n",
            "ago\n",
            "agree\n",
            "ahead\n",
            "air\n",
            "alien\n",
            "alive\n",
            "allow\n",
            "allowed\n",
            "almost\n",
            "alone\n",
            "along\n",
            "already\n",
            "also\n",
            "although\n",
            "always\n",
            "amazing\n",
            "america\n",
            "american\n",
            "among\n",
            "amount\n",
            "amusing\n",
            "angel\n",
            "angle\n",
            "angry\n",
            "animal\n",
            "animated\n",
            "animation\n",
            "annoying\n",
            "another\n",
            "answer\n",
            "anybody\n",
            "anymore\n",
            "anyone\n",
            "anything\n",
            "anyway\n",
            "anywhere\n",
            "apart\n",
            "apartment\n",
            "apparent\n",
            "apparently\n",
            "appeal\n",
            "appear\n",
            "appearance\n",
            "appeared\n",
            "appears\n",
            "appreciate\n",
            "approach\n",
            "area\n",
            "arm\n",
            "army\n",
            "around\n",
            "art\n",
            "artist\n",
            "artistic\n",
            "aside\n",
            "ask\n",
            "asked\n",
            "asks\n",
            "aspect\n",
            "atmosphere\n",
            "attack\n",
            "attempt\n",
            "attention\n",
            "attitude\n",
            "attractive\n",
            "audience\n",
            "author\n",
            "available\n",
            "average\n",
            "avoid\n",
            "award\n",
            "aware\n",
            "away\n",
            "awesome\n",
            "awful\n",
            "b\n",
            "baby\n",
            "back\n",
            "background\n",
            "bad\n",
            "badly\n",
            "ball\n",
            "band\n",
            "bar\n",
            "barely\n",
            "based\n",
            "basic\n",
            "basically\n",
            "battle\n",
            "bear\n",
            "beat\n",
            "beautiful\n",
            "beautifully\n",
            "beauty\n",
            "became\n",
            "become\n",
            "becomes\n",
            "becoming\n",
            "bed\n",
            "began\n",
            "begin\n",
            "beginning\n",
            "behavior\n",
            "behind\n",
            "belief\n",
            "believable\n",
            "believe\n",
            "ben\n",
            "besides\n",
            "best\n",
            "better\n",
            "beyond\n",
            "big\n",
            "biggest\n",
            "bill\n",
            "billy\n",
            "bit\n",
            "bizarre\n",
            "black\n",
            "blame\n",
            "bland\n",
            "blood\n",
            "bloody\n",
            "blow\n",
            "blue\n",
            "board\n",
            "body\n",
            "book\n",
            "bored\n",
            "boring\n",
            "born\n",
            "bos\n",
            "bother\n",
            "bottom\n",
            "bought\n",
            "box\n",
            "boy\n",
            "boyfriend\n",
            "brain\n",
            "break\n",
            "brian\n",
            "brief\n",
            "brilliant\n",
            "bring\n",
            "brings\n",
            "british\n",
            "brother\n",
            "brought\n",
            "brutal\n",
            "buddy\n",
            "budget\n",
            "build\n",
            "building\n",
            "bunch\n",
            "business\n",
            "buy\n",
            "ca\n",
            "call\n",
            "called\n",
            "came\n",
            "cameo\n",
            "camera\n",
            "camp\n",
            "capture\n",
            "car\n",
            "care\n",
            "career\n",
            "carry\n",
            "cartoon\n",
            "case\n",
            "cast\n",
            "casting\n",
            "cat\n",
            "catch\n",
            "caught\n",
            "cause\n",
            "center\n",
            "central\n",
            "century\n",
            "certain\n",
            "certainly\n",
            "chance\n",
            "change\n",
            "changed\n",
            "channel\n",
            "character\n",
            "charles\n",
            "charm\n",
            "charming\n",
            "chase\n",
            "cheap\n",
            "check\n",
            "cheesy\n",
            "chemistry\n",
            "chick\n",
            "child\n",
            "childhood\n",
            "choice\n",
            "chris\n",
            "christian\n",
            "christmas\n",
            "christopher\n",
            "church\n",
            "cinema\n",
            "cinematic\n",
            "cinematography\n",
            "city\n",
            "claim\n",
            "class\n",
            "classic\n",
            "clear\n",
            "clearly\n",
            "clever\n",
            "cliché\n",
            "clichés\n",
            "climax\n",
            "close\n",
            "clothes\n",
            "club\n",
            "clue\n",
            "cold\n",
            "collection\n",
            "college\n",
            "color\n",
            "come\n",
            "comedic\n",
            "comedy\n",
            "comic\n",
            "coming\n",
            "comment\n",
            "commentary\n",
            "commercial\n",
            "common\n",
            "community\n",
            "company\n",
            "compare\n",
            "compared\n",
            "comparison\n",
            "compelling\n",
            "complete\n",
            "completely\n",
            "complex\n",
            "computer\n",
            "concept\n",
            "conclusion\n",
            "conflict\n",
            "confused\n",
            "confusing\n",
            "connection\n",
            "consider\n",
            "considered\n",
            "considering\n",
            "constant\n",
            "constantly\n",
            "contains\n",
            "content\n",
            "continue\n",
            "control\n",
            "conversation\n",
            "convincing\n",
            "cool\n",
            "cop\n",
            "copy\n",
            "cost\n",
            "costume\n",
            "could\n",
            "count\n",
            "country\n",
            "couple\n",
            "course\n",
            "cover\n",
            "crap\n",
            "crazy\n",
            "create\n",
            "created\n",
            "creating\n",
            "creative\n",
            "creature\n",
            "credit\n",
            "creepy\n",
            "crew\n",
            "crime\n",
            "criminal\n",
            "critic\n",
            "cross\n",
            "cry\n",
            "cult\n",
            "culture\n",
            "cut\n",
            "cute\n",
            "dad\n",
            "damn\n",
            "dance\n",
            "dancing\n",
            "dangerous\n",
            "dark\n",
            "date\n",
            "daughter\n",
            "david\n",
            "day\n",
            "de\n",
            "dead\n",
            "deal\n",
            "dealing\n",
            "death\n",
            "decade\n",
            "decent\n",
            "decide\n",
            "decided\n",
            "decides\n",
            "decision\n",
            "deep\n",
            "deeply\n",
            "definitely\n",
            "deliver\n",
            "delivers\n",
            "depth\n",
            "describe\n",
            "deserve\n",
            "deserved\n",
            "deserves\n",
            "design\n",
            "desire\n",
            "desperate\n",
            "despite\n",
            "detail\n",
            "detective\n",
            "developed\n",
            "development\n",
            "dialog\n",
            "dialogue\n",
            "die\n",
            "died\n",
            "difference\n",
            "different\n",
            "difficult\n",
            "direct\n",
            "directed\n",
            "directing\n",
            "direction\n",
            "director\n",
            "dirty\n",
            "disappointed\n",
            "disappointing\n",
            "disappointment\n",
            "disaster\n",
            "discover\n",
            "disney\n",
            "display\n",
            "disturbing\n",
            "doctor\n",
            "documentary\n",
            "doe\n",
            "dog\n",
            "dollar\n",
            "done\n",
            "door\n",
            "double\n",
            "doubt\n",
            "dozen\n",
            "dr.\n",
            "drag\n",
            "drama\n",
            "dramatic\n",
            "draw\n",
            "drawn\n",
            "dream\n",
            "drive\n",
            "drop\n",
            "drug\n",
            "drunk\n",
            "due\n",
            "dull\n",
            "dumb\n",
            "dvd\n",
            "dy\n",
            "dying\n",
            "earlier\n",
            "early\n",
            "earth\n",
            "easily\n",
            "easy\n",
            "edge\n",
            "editing\n",
            "effect\n",
            "effective\n",
            "effort\n",
            "either\n",
            "element\n",
            "else\n",
            "emotion\n",
            "emotional\n",
            "encounter\n",
            "end\n",
            "ended\n",
            "ending\n",
            "energy\n",
            "engaging\n",
            "england\n",
            "english\n",
            "enjoy\n",
            "enjoyable\n",
            "enjoyed\n",
            "enough\n",
            "entertaining\n",
            "entertainment\n",
            "entire\n",
            "entirely\n",
            "epic\n",
            "episode\n",
            "equally\n",
            "era\n",
            "escape\n",
            "especially\n",
            "essentially\n",
            "etc\n",
            "european\n",
            "even\n",
            "event\n",
            "eventually\n",
            "ever\n",
            "every\n",
            "everybody\n",
            "everyone\n",
            "everything\n",
            "evil\n",
            "exactly\n",
            "example\n",
            "excellent\n",
            "except\n",
            "exception\n",
            "exciting\n",
            "excuse\n",
            "exist\n",
            "expect\n",
            "expectation\n",
            "expected\n",
            "expecting\n",
            "experience\n",
            "explain\n",
            "explained\n",
            "explanation\n",
            "expression\n",
            "extra\n",
            "extreme\n",
            "extremely\n",
            "eye\n",
            "face\n",
            "fact\n",
            "fail\n",
            "failed\n",
            "fails\n",
            "failure\n",
            "fair\n",
            "fairly\n",
            "fake\n",
            "fall\n",
            "falling\n",
            "familiar\n",
            "family\n",
            "famous\n",
            "fan\n",
            "fantastic\n",
            "fantasy\n",
            "far\n",
            "fascinating\n",
            "fashion\n",
            "fast\n",
            "fate\n",
            "father\n",
            "fault\n",
            "favor\n",
            "favorite\n",
            "favourite\n",
            "fear\n",
            "feature\n",
            "featuring\n",
            "feel\n",
            "feeling\n",
            "fell\n",
            "fellow\n",
            "felt\n",
            "female\n",
            "festival\n",
            "fiction\n",
            "field\n",
            "fight\n",
            "fighting\n",
            "figure\n",
            "fill\n",
            "filled\n",
            "film.\n",
            "filmed\n",
            "filming\n",
            "filmmaker\n",
            "final\n",
            "finally\n",
            "find\n",
            "finding\n",
            "fine\n",
            "finest\n",
            "finish\n",
            "finished\n",
            "fire\n",
            "first\n",
            "fit\n",
            "five\n",
            "flashback\n",
            "flat\n",
            "flaw\n",
            "flick\n",
            "floor\n",
            "fly\n",
            "flying\n",
            "focus\n",
            "folk\n",
            "follow\n",
            "followed\n",
            "following\n",
            "follows\n",
            "food\n",
            "foot\n",
            "footage\n",
            "force\n",
            "forced\n",
            "forever\n",
            "forget\n",
            "forgotten\n",
            "form\n",
            "former\n",
            "forward\n",
            "found\n",
            "four\n",
            "frame\n",
            "frank\n",
            "free\n",
            "french\n",
            "fresh\n",
            "friend\n",
            "friendship\n",
            "front\n",
            "full\n",
            "fully\n",
            "fun\n",
            "funniest\n",
            "funny\n",
            "future\n",
            "gag\n",
            "game\n",
            "gang\n",
            "garbage\n",
            "gave\n",
            "gay\n",
            "gem\n",
            "general\n",
            "generally\n",
            "generation\n",
            "genius\n",
            "genre\n",
            "george\n",
            "german\n",
            "get\n",
            "getting\n",
            "ghost\n",
            "giant\n",
            "girl\n",
            "girlfriend\n",
            "give\n",
            "given\n",
            "giving\n",
            "glad\n",
            "go\n",
            "god\n",
            "going\n",
            "gone\n",
            "good\n",
            "gore\n",
            "gorgeous\n",
            "got\n",
            "gotten\n",
            "government\n",
            "grace\n",
            "grade\n",
            "graphic\n",
            "great\n",
            "greatest\n",
            "green\n",
            "ground\n",
            "group\n",
            "growing\n",
            "guess\n",
            "gun\n",
            "guy\n",
            "hair\n",
            "half\n",
            "hand\n",
            "happen\n",
            "happened\n",
            "happening\n",
            "happens\n",
            "happy\n",
            "hard\n",
            "hardly\n",
            "hate\n",
            "hated\n",
            "head\n",
            "hear\n",
            "heard\n",
            "heart\n",
            "heaven\n",
            "heavy\n",
            "held\n",
            "hell\n",
            "help\n",
            "helped\n",
            "hero\n",
            "hey\n",
            "hidden\n",
            "hide\n",
            "high\n",
            "highlight\n",
            "highly\n",
            "hilarious\n",
            "hill\n",
            "historical\n",
            "history\n",
            "hit\n",
            "hold\n",
            "hole\n",
            "hollywood\n",
            "home\n",
            "honest\n",
            "honestly\n",
            "hope\n",
            "hoping\n",
            "horrible\n",
            "horror\n",
            "horse\n",
            "hospital\n",
            "hot\n",
            "hotel\n",
            "hour\n",
            "house\n",
            "however\n",
            "huge\n",
            "human\n",
            "humor\n",
            "humour\n",
            "hundred\n",
            "hurt\n",
            "husband\n",
            "idea\n",
            "identity\n",
            "idiot\n",
            "ii\n",
            "image\n",
            "imagination\n",
            "imagine\n",
            "imdb\n",
            "immediately\n",
            "impact\n",
            "important\n",
            "impossible\n",
            "impressed\n",
            "impression\n",
            "impressive\n",
            "include\n",
            "included\n",
            "includes\n",
            "including\n",
            "incredible\n",
            "incredibly\n",
            "indeed\n",
            "independent\n",
            "indian\n",
            "individual\n",
            "industry\n",
            "information\n",
            "innocent\n",
            "inside\n",
            "inspired\n",
            "instance\n",
            "instead\n",
            "insult\n",
            "intelligence\n",
            "intelligent\n",
            "intended\n",
            "intense\n",
            "intention\n",
            "interest\n",
            "interested\n",
            "interesting\n",
            "interview\n",
            "intriguing\n",
            "introduced\n",
            "involved\n",
            "involving\n",
            "island\n",
            "issue\n",
            "it.\n",
            "italian\n",
            "jack\n",
            "james\n",
            "jane\n",
            "japanese\n",
            "jason\n",
            "jim\n",
            "job\n",
            "joe\n",
            "john\n",
            "joke\n",
            "jones\n",
            "journey\n",
            "joy\n",
            "judge\n",
            "jump\n",
            "justice\n",
            "keep\n",
            "keeping\n",
            "kept\n",
            "key\n",
            "kick\n",
            "kid\n",
            "kill\n",
            "killed\n",
            "killer\n",
            "killing\n",
            "kind\n",
            "kinda\n",
            "king\n",
            "knew\n",
            "know\n",
            "knowing\n",
            "known\n",
            "la\n",
            "lack\n",
            "lacking\n",
            "lady\n",
            "lame\n",
            "land\n",
            "language\n",
            "large\n",
            "last\n",
            "late\n",
            "later\n",
            "latter\n",
            "laugh\n",
            "laughable\n",
            "laughed\n",
            "laughing\n",
            "law\n",
            "le\n",
            "lead\n",
            "leader\n",
            "leading\n",
            "leaf\n",
            "learn\n",
            "least\n",
            "leave\n",
            "leaving\n",
            "led\n",
            "lee\n",
            "left\n",
            "legend\n",
            "length\n",
            "lesson\n",
            "let\n",
            "level\n",
            "lie\n",
            "life\n",
            "light\n",
            "lighting\n",
            "likable\n",
            "liked\n",
            "likely\n",
            "limited\n",
            "line\n",
            "list\n",
            "listen\n",
            "literally\n",
            "little\n",
            "live\n",
            "lived\n",
            "living\n",
            "local\n",
            "location\n",
            "london\n",
            "long\n",
            "longer\n",
            "look\n",
            "looked\n",
            "looking\n",
            "loose\n",
            "lord\n",
            "lose\n",
            "loss\n",
            "lost\n",
            "lot\n",
            "loud\n",
            "love\n",
            "loved\n",
            "lovely\n",
            "lover\n",
            "loving\n",
            "low\n",
            "machine\n",
            "mad\n",
            "made\n",
            "magic\n",
            "main\n",
            "mainly\n",
            "major\n",
            "make\n",
            "maker\n",
            "making\n",
            "male\n",
            "man\n",
            "manage\n",
            "managed\n",
            "manages\n",
            "manner\n",
            "many\n",
            "mark\n",
            "marriage\n",
            "married\n",
            "martin\n",
            "mary\n",
            "master\n",
            "masterpiece\n",
            "match\n",
            "material\n",
            "matter\n",
            "may\n",
            "maybe\n",
            "mean\n",
            "meaning\n",
            "meant\n",
            "mediocre\n",
            "medium\n",
            "meet\n",
            "member\n",
            "memorable\n",
            "memory\n",
            "men\n",
            "mental\n",
            "mention\n",
            "mentioned\n",
            "merely\n",
            "mess\n",
            "message\n",
            "met\n",
            "michael\n",
            "middle\n",
            "might\n",
            "mile\n",
            "military\n",
            "million\n",
            "mind\n",
            "mine\n",
            "minor\n",
            "minute\n",
            "miss\n",
            "missed\n",
            "missing\n",
            "mistake\n",
            "mix\n",
            "model\n",
            "modern\n",
            "mom\n",
            "moment\n",
            "money\n",
            "monster\n",
            "month\n",
            "mood\n",
            "moral\n",
            "mostly\n",
            "mother\n",
            "motion\n",
            "mouth\n",
            "move\n",
            "moved\n",
            "movement\n",
            "movie.\n",
            "moving\n",
            "mr\n",
            "mr.\n",
            "much\n",
            "murder\n",
            "music\n",
            "musical\n",
            "must\n",
            "mysterious\n",
            "mystery\n",
            "na\n",
            "naked\n",
            "name\n",
            "named\n",
            "narrative\n",
            "nasty\n",
            "natural\n",
            "naturally\n",
            "nature\n",
            "near\n",
            "nearly\n",
            "necessary\n",
            "need\n",
            "needed\n",
            "negative\n",
            "neither\n",
            "never\n",
            "new\n",
            "news\n",
            "next\n",
            "nice\n",
            "nicely\n",
            "night\n",
            "nightmare\n",
            "nobody\n",
            "none\n",
            "normal\n",
            "normally\n",
            "note\n",
            "nothing\n",
            "notice\n",
            "noticed\n",
            "novel\n",
            "nowhere\n",
            "nudity\n",
            "number\n",
            "numerous\n",
            "obvious\n",
            "obviously\n",
            "odd\n",
            "offer\n",
            "office\n",
            "officer\n",
            "often\n",
            "oh\n",
            "ok\n",
            "okay\n",
            "old\n",
            "older\n",
            "onto\n",
            "open\n",
            "opening\n",
            "opera\n",
            "opinion\n",
            "opportunity\n",
            "opposite\n",
            "order\n",
            "original\n",
            "originally\n",
            "oscar\n",
            "others\n",
            "otherwise\n",
            "outside\n",
            "outstanding\n",
            "overall\n",
            "owner\n",
            "pace\n",
            "pacing\n",
            "page\n",
            "paid\n",
            "pain\n",
            "painful\n",
            "parent\n",
            "park\n",
            "part\n",
            "particular\n",
            "particularly\n",
            "partner\n",
            "party\n",
            "pas\n",
            "passion\n",
            "past\n",
            "pathetic\n",
            "paul\n",
            "pay\n",
            "people\n",
            "perfect\n",
            "perfectly\n",
            "performance\n",
            "perhaps\n",
            "period\n",
            "person\n",
            "personal\n",
            "personality\n",
            "personally\n",
            "perspective\n",
            "peter\n",
            "phone\n",
            "photography\n",
            "physical\n",
            "pick\n",
            "picked\n",
            "picture\n",
            "piece\n",
            "place\n",
            "plain\n",
            "plan\n",
            "plane\n",
            "planet\n",
            "play\n",
            "played\n",
            "player\n",
            "playing\n",
            "please\n",
            "pleasure\n",
            "plenty\n",
            "plot\n",
            "plus\n",
            "point\n",
            "pointless\n",
            "police\n",
            "political\n",
            "poor\n",
            "poorly\n",
            "pop\n",
            "popular\n",
            "porn\n",
            "portray\n",
            "portrayal\n",
            "portrayed\n",
            "positive\n",
            "possible\n",
            "possibly\n",
            "potential\n",
            "power\n",
            "powerful\n",
            "predictable\n",
            "premise\n",
            "presence\n",
            "present\n",
            "presented\n",
            "pretty\n",
            "previous\n",
            "price\n",
            "prison\n",
            "probably\n",
            "problem\n",
            "process\n",
            "produce\n",
            "produced\n",
            "producer\n",
            "product\n",
            "production\n",
            "prof\n",
            "professional\n",
            "project\n",
            "promise\n",
            "protagonist\n",
            "prove\n",
            "provide\n",
            "provides\n",
            "public\n",
            "pull\n",
            "pulled\n",
            "pure\n",
            "purpose\n",
            "put\n",
            "putting\n",
            "quality\n",
            "queen\n",
            "question\n",
            "quick\n",
            "quickly\n",
            "quiet\n",
            "quite\n",
            "race\n",
            "random\n",
            "rape\n",
            "rare\n",
            "rarely\n",
            "rate\n",
            "rated\n",
            "rather\n",
            "rating\n",
            "ray\n",
            "reach\n",
            "reaction\n",
            "read\n",
            "reading\n",
            "ready\n",
            "real\n",
            "realistic\n",
            "reality\n",
            "realize\n",
            "realized\n",
            "really\n",
            "reason\n",
            "recent\n",
            "recently\n",
            "recommend\n",
            "recommended\n",
            "record\n",
            "red\n",
            "redeeming\n",
            "reference\n",
            "regular\n",
            "relationship\n",
            "release\n",
            "released\n",
            "remains\n",
            "remake\n",
            "remarkable\n",
            "remember\n",
            "reminded\n",
            "reminds\n",
            "rent\n",
            "rented\n",
            "respect\n",
            "responsible\n",
            "rest\n",
            "result\n",
            "return\n",
            "revenge\n",
            "review\n",
            "reviewer\n",
            "rich\n",
            "richard\n",
            "ride\n",
            "ridiculous\n",
            "right\n",
            "ring\n",
            "rise\n",
            "road\n",
            "robert\n",
            "rock\n",
            "role\n",
            "roll\n",
            "romance\n",
            "romantic\n",
            "room\n",
            "ruin\n",
            "rule\n",
            "run\n",
            "running\n",
            "sad\n",
            "sadly\n",
            "said\n",
            "sam\n",
            "sat\n",
            "save\n",
            "saved\n",
            "saving\n",
            "saw\n",
            "say\n",
            "saying\n",
            "scare\n",
            "scared\n",
            "scary\n",
            "scene\n",
            "scenery\n",
            "school\n",
            "sci-fi\n",
            "science\n",
            "scientist\n",
            "score\n",
            "scott\n",
            "scream\n",
            "screen\n",
            "screenplay\n",
            "script\n",
            "search\n",
            "season\n",
            "second\n",
            "secret\n",
            "see\n",
            "seeing\n",
            "seem\n",
            "seemed\n",
            "seemingly\n",
            "seems\n",
            "seen\n",
            "self\n",
            "sell\n",
            "sense\n",
            "sent\n",
            "sequel\n",
            "sequence\n",
            "serial\n",
            "series\n",
            "serious\n",
            "seriously\n",
            "set\n",
            "setting\n",
            "seven\n",
            "several\n",
            "sex\n",
            "sexual\n",
            "sexy\n",
            "shame\n",
            "share\n",
            "ship\n",
            "shock\n",
            "shocking\n",
            "shoot\n",
            "shooting\n",
            "short\n",
            "shot\n",
            "show\n",
            "showed\n",
            "showing\n",
            "shown\n",
            "sick\n",
            "side\n",
            "sight\n",
            "sign\n",
            "silent\n",
            "silly\n",
            "similar\n",
            "simple\n",
            "simply\n",
            "since\n",
            "singer\n",
            "singing\n",
            "single\n",
            "sister\n",
            "sit\n",
            "sitting\n",
            "situation\n",
            "six\n",
            "skill\n",
            "skip\n",
            "slasher\n",
            "sleep\n",
            "slightly\n",
            "slow\n",
            "slowly\n",
            "small\n",
            "smart\n",
            "smile\n",
            "smith\n",
            "social\n",
            "society\n",
            "soldier\n",
            "solid\n",
            "somebody\n",
            "somehow\n",
            "someone\n",
            "something\n",
            "sometimes\n",
            "somewhat\n",
            "somewhere\n",
            "son\n",
            "song\n",
            "soon\n",
            "sorry\n",
            "sort\n",
            "soul\n",
            "sound\n",
            "soundtrack\n",
            "south\n",
            "space\n",
            "speak\n",
            "speaking\n",
            "special\n",
            "spend\n",
            "spent\n",
            "spirit\n",
            "spoiler\n",
            "spot\n",
            "stage\n",
            "stand\n",
            "standard\n",
            "star\n",
            "starring\n",
            "start\n",
            "started\n",
            "starting\n",
            "state\n",
            "station\n",
            "stay\n",
            "steal\n",
            "step\n",
            "stephen\n",
            "stereotype\n",
            "steve\n",
            "stick\n",
            "still\n",
            "stone\n",
            "stop\n",
            "store\n",
            "story\n",
            "storyline\n",
            "straight\n",
            "strange\n",
            "street\n",
            "strength\n",
            "strong\n",
            "struggle\n",
            "stuck\n",
            "student\n",
            "studio\n",
            "study\n",
            "stuff\n",
            "stunning\n",
            "stupid\n",
            "style\n",
            "subject\n",
            "subtle\n",
            "success\n",
            "successful\n",
            "suck\n",
            "suddenly\n",
            "suggest\n",
            "suicide\n",
            "suit\n",
            "summer\n",
            "super\n",
            "superb\n",
            "superior\n",
            "support\n",
            "supporting\n",
            "suppose\n",
            "supposed\n",
            "supposedly\n",
            "sure\n",
            "surely\n",
            "surprise\n",
            "surprised\n",
            "surprising\n",
            "surprisingly\n",
            "suspect\n",
            "suspense\n",
            "sweet\n",
            "system\n",
            "take\n",
            "taken\n",
            "taking\n",
            "tale\n",
            "talent\n",
            "talented\n",
            "talk\n",
            "talking\n",
            "taste\n",
            "teacher\n",
            "team\n",
            "tear\n",
            "technical\n",
            "teen\n",
            "teenage\n",
            "teenager\n",
            "television\n",
            "tell\n",
            "telling\n",
            "ten\n",
            "tension\n",
            "term\n",
            "terrible\n",
            "terribly\n",
            "terrific\n",
            "thank\n",
            "thanks\n",
            "thats\n",
            "theater\n",
            "theatre\n",
            "theme\n",
            "therefore\n",
            "thin\n",
            "thing\n",
            "think\n",
            "thinking\n",
            "third\n",
            "thoroughly\n",
            "though\n",
            "thought\n",
            "thousand\n",
            "three\n",
            "thriller\n",
            "throughout\n",
            "throw\n",
            "thrown\n",
            "thus\n",
            "time\n",
            "time.\n",
            "tired\n",
            "title\n",
            "today\n",
            "together\n",
            "told\n",
            "tom\n",
            "tone\n",
            "tony\n",
            "took\n",
            "top\n",
            "torture\n",
            "total\n",
            "totally\n",
            "touch\n",
            "touching\n",
            "tough\n",
            "toward\n",
            "towards\n",
            "town\n",
            "track\n",
            "tragedy\n",
            "tragic\n",
            "trailer\n",
            "train\n",
            "trash\n",
            "travel\n",
            "treat\n",
            "treated\n",
            "trick\n",
            "tried\n",
            "trip\n",
            "trouble\n",
            "true\n",
            "truly\n",
            "trust\n",
            "truth\n",
            "try\n",
            "trying\n",
            "turn\n",
            "turned\n",
            "turning\n",
            "tv\n",
            "twice\n",
            "twist\n",
            "two\n",
            "type\n",
            "typical\n",
            "u\n",
            "ugly\n",
            "ultimately\n",
            "unbelievable\n",
            "understand\n",
            "understanding\n",
            "unfortunately\n",
            "unique\n",
            "unknown\n",
            "unless\n",
            "unlike\n",
            "unnecessary\n",
            "unusual\n",
            "upon\n",
            "us\n",
            "use\n",
            "used\n",
            "using\n",
            "usual\n",
            "usually\n",
            "utterly\n",
            "value\n",
            "vampire\n",
            "van\n",
            "various\n",
            "vehicle\n",
            "version\n",
            "victim\n",
            "video\n",
            "view\n",
            "viewer\n",
            "viewing\n",
            "villain\n",
            "violence\n",
            "violent\n",
            "vision\n",
            "visit\n",
            "visual\n",
            "voice\n",
            "vote\n",
            "wait\n",
            "waiting\n",
            "walk\n",
            "walking\n",
            "wall\n",
            "want\n",
            "wanted\n",
            "wanting\n",
            "war\n",
            "warning\n",
            "waste\n",
            "wasted\n",
            "watch\n",
            "watchable\n",
            "watched\n",
            "watching\n",
            "water\n",
            "way\n",
            "weak\n",
            "weapon\n",
            "wear\n",
            "wearing\n",
            "week\n",
            "weird\n",
            "well\n",
            "went\n",
            "west\n",
            "western\n",
            "whatever\n",
            "whatsoever\n",
            "whenever\n",
            "whether\n",
            "white\n",
            "whole\n",
            "whose\n",
            "wife\n",
            "wild\n",
            "william\n",
            "willing\n",
            "win\n",
            "wind\n",
            "window\n",
            "winning\n",
            "wish\n",
            "witch\n",
            "within\n",
            "without\n",
            "witness\n",
            "witty\n",
            "wo\n",
            "woman\n",
            "wonder\n",
            "wonderful\n",
            "wonderfully\n",
            "wondering\n",
            "wood\n",
            "wooden\n",
            "word\n",
            "work\n",
            "worked\n",
            "working\n",
            "world\n",
            "worse\n",
            "worst\n",
            "worth\n",
            "worthy\n",
            "would\n",
            "wow\n",
            "write\n",
            "writer\n",
            "writing\n",
            "written\n",
            "wrong\n",
            "wrote\n",
            "yeah\n",
            "year\n",
            "yes\n",
            "yet\n",
            "york\n",
            "young\n",
            "younger\n",
            "youth\n",
            "zero\n",
            "zombie\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for d in vectorizer.get_feature_names_out():\n",
        "  print(d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNmC-F1qtFrV"
      },
      "source": [
        "#### Without priors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqTIOyoUtFrV",
        "outputId": "d30e9328-14be-4ee5-b188-9214d265a59c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.83784\n"
          ]
        }
      ],
      "source": [
        "model=MultinomialNaiveBayes()\n",
        "params=model.fit(X_train, y_train, 0,0,0,0)\n",
        "y_prob=model.predict(X_test)\n",
        "y_pred = np.argmax(y_prob, 1)\n",
        "print(\"Accuracy:\",model.evaluate_acc(y_pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KkSpflmtFrV"
      },
      "source": [
        "#### With priors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCjzvlG6tFrV",
        "outputId": "c331e678-e181-46a5-8448-390cd0280809"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.83764\n"
          ]
        }
      ],
      "source": [
        "model=MultinomialNaiveBayes()\n",
        "params=model.fit(X_train, y_train, 1,1,10,10)\n",
        "y_prob=model.predict(X_test)\n",
        "y_pred = np.argmax(y_prob, 1)\n",
        "print(\"Accuracy:\",model.evaluate_acc(y_pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-LHj7kLtFrW"
      },
      "source": [
        "### After cleaning html tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2SFVLi4tFrW",
        "outputId": "dd48d77d-f0b4-4387-8761-fb4b5af92e09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-25238354f317>:2: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(text, \"html.parser\")\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(x_train)):\n",
        "  x_train[i]=strip_html(x_train[i]) ## Only removing html tags for now\n",
        "  x_test[i]=strip_html(x_test[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZO3vi3Y4tFrW"
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer(stop_words=stopword_list, tokenizer=LemmaTokenizer(), min_df=0.01, max_df=0.4)\n",
        "X_train = vectorizer.fit_transform(x_train)\n",
        "dic= vectorizer.get_feature_names_out()\n",
        "X_test=vectorizer.transform(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4h5CqfJ3tFrW",
        "outputId": "e8cb7acc-605f-4682-c2db-f25788c44e47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dictionary length: 1610\n"
          ]
        }
      ],
      "source": [
        "print(\"Dictionary length:\", len(vectorizer.get_feature_names_out()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3fZ8KsQtFrW",
        "outputId": "0bbf1cbb-04a5-4afd-a318-5308610becd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!\n",
            "$\n",
            "%\n",
            "&\n",
            "'\n",
            "'d\n",
            "'ll\n",
            "'m\n",
            "'re\n",
            "'the\n",
            "'ve\n",
            "*\n",
            "-\n",
            "--\n",
            "..\n",
            "...\n",
            "....\n",
            ".....\n",
            ".the\n",
            "1\n",
            "10\n",
            "100\n",
            "15\n",
            "2\n",
            "20\n",
            "3\n",
            "30\n",
            "4\n",
            "40\n",
            "5\n",
            "50\n",
            "6\n",
            "60\n",
            "7\n",
            "70\n",
            "8\n",
            "80\n",
            "9\n",
            "90\n",
            ":\n",
            ";\n",
            "?\n",
            "ability\n",
            "able\n",
            "absolute\n",
            "absolutely\n",
            "absurd\n",
            "academy\n",
            "accent\n",
            "accept\n",
            "accident\n",
            "according\n",
            "accurate\n",
            "across\n",
            "act\n",
            "acted\n",
            "acting\n",
            "action\n",
            "actor\n",
            "actress\n",
            "actual\n",
            "actually\n",
            "adam\n",
            "adaptation\n",
            "add\n",
            "added\n",
            "addition\n",
            "admit\n",
            "adult\n",
            "adventure\n",
            "affair\n",
            "afraid\n",
            "age\n",
            "agent\n",
            "ago\n",
            "agree\n",
            "ahead\n",
            "air\n",
            "alien\n",
            "alive\n",
            "allow\n",
            "allowed\n",
            "almost\n",
            "alone\n",
            "along\n",
            "already\n",
            "also\n",
            "although\n",
            "always\n",
            "amazing\n",
            "america\n",
            "american\n",
            "among\n",
            "amount\n",
            "amusing\n",
            "angel\n",
            "angle\n",
            "angry\n",
            "animal\n",
            "animated\n",
            "animation\n",
            "annoying\n",
            "another\n",
            "answer\n",
            "anybody\n",
            "anymore\n",
            "anyone\n",
            "anything\n",
            "anyway\n",
            "anywhere\n",
            "apart\n",
            "apartment\n",
            "apparent\n",
            "apparently\n",
            "appeal\n",
            "appear\n",
            "appearance\n",
            "appeared\n",
            "appears\n",
            "appreciate\n",
            "approach\n",
            "area\n",
            "arm\n",
            "army\n",
            "around\n",
            "art\n",
            "artist\n",
            "artistic\n",
            "aside\n",
            "ask\n",
            "asked\n",
            "asks\n",
            "aspect\n",
            "atmosphere\n",
            "attack\n",
            "attempt\n",
            "attention\n",
            "attitude\n",
            "attractive\n",
            "audience\n",
            "author\n",
            "available\n",
            "average\n",
            "avoid\n",
            "award\n",
            "aware\n",
            "away\n",
            "awesome\n",
            "awful\n",
            "b\n",
            "baby\n",
            "back\n",
            "background\n",
            "bad\n",
            "badly\n",
            "ball\n",
            "band\n",
            "bar\n",
            "barely\n",
            "based\n",
            "basic\n",
            "basically\n",
            "battle\n",
            "bear\n",
            "beat\n",
            "beautiful\n",
            "beautifully\n",
            "beauty\n",
            "became\n",
            "become\n",
            "becomes\n",
            "becoming\n",
            "bed\n",
            "began\n",
            "begin\n",
            "beginning\n",
            "behavior\n",
            "behind\n",
            "belief\n",
            "believable\n",
            "believe\n",
            "ben\n",
            "besides\n",
            "best\n",
            "better\n",
            "beyond\n",
            "big\n",
            "biggest\n",
            "bill\n",
            "billy\n",
            "bit\n",
            "bizarre\n",
            "black\n",
            "blame\n",
            "blood\n",
            "bloody\n",
            "blow\n",
            "blue\n",
            "board\n",
            "body\n",
            "book\n",
            "bored\n",
            "boring\n",
            "born\n",
            "bos\n",
            "bother\n",
            "bottom\n",
            "bought\n",
            "box\n",
            "boy\n",
            "boyfriend\n",
            "brain\n",
            "break\n",
            "brian\n",
            "brief\n",
            "brilliant\n",
            "bring\n",
            "brings\n",
            "british\n",
            "brother\n",
            "brought\n",
            "brutal\n",
            "buddy\n",
            "budget\n",
            "build\n",
            "building\n",
            "bunch\n",
            "business\n",
            "buy\n",
            "ca\n",
            "call\n",
            "called\n",
            "came\n",
            "cameo\n",
            "camera\n",
            "camp\n",
            "capture\n",
            "car\n",
            "care\n",
            "career\n",
            "carry\n",
            "cartoon\n",
            "case\n",
            "cast\n",
            "casting\n",
            "cat\n",
            "catch\n",
            "caught\n",
            "cause\n",
            "center\n",
            "central\n",
            "century\n",
            "certain\n",
            "certainly\n",
            "chance\n",
            "change\n",
            "changed\n",
            "channel\n",
            "character\n",
            "charles\n",
            "charm\n",
            "charming\n",
            "chase\n",
            "cheap\n",
            "check\n",
            "cheesy\n",
            "chemistry\n",
            "chick\n",
            "child\n",
            "childhood\n",
            "choice\n",
            "chris\n",
            "christian\n",
            "christmas\n",
            "christopher\n",
            "church\n",
            "cinema\n",
            "cinematic\n",
            "cinematography\n",
            "city\n",
            "claim\n",
            "class\n",
            "classic\n",
            "clear\n",
            "clearly\n",
            "clever\n",
            "climax\n",
            "close\n",
            "clothes\n",
            "club\n",
            "clue\n",
            "cold\n",
            "collection\n",
            "college\n",
            "color\n",
            "come\n",
            "comedic\n",
            "comedy\n",
            "comic\n",
            "coming\n",
            "comment\n",
            "commentary\n",
            "commercial\n",
            "common\n",
            "community\n",
            "company\n",
            "compare\n",
            "compared\n",
            "comparison\n",
            "compelling\n",
            "complete\n",
            "completely\n",
            "complex\n",
            "computer\n",
            "concept\n",
            "conclusion\n",
            "conflict\n",
            "confused\n",
            "confusing\n",
            "connection\n",
            "consider\n",
            "considered\n",
            "considering\n",
            "constant\n",
            "constantly\n",
            "contains\n",
            "content\n",
            "continue\n",
            "control\n",
            "conversation\n",
            "convincing\n",
            "cool\n",
            "cop\n",
            "copy\n",
            "cost\n",
            "costume\n",
            "could\n",
            "count\n",
            "country\n",
            "couple\n",
            "course\n",
            "cover\n",
            "crap\n",
            "crazy\n",
            "create\n",
            "created\n",
            "creating\n",
            "creative\n",
            "creature\n",
            "credit\n",
            "creepy\n",
            "crew\n",
            "crime\n",
            "criminal\n",
            "critic\n",
            "cross\n",
            "cry\n",
            "cult\n",
            "culture\n",
            "cut\n",
            "cute\n",
            "dad\n",
            "damn\n",
            "dance\n",
            "dancing\n",
            "dangerous\n",
            "dark\n",
            "date\n",
            "daughter\n",
            "david\n",
            "day\n",
            "de\n",
            "dead\n",
            "deal\n",
            "dealing\n",
            "death\n",
            "decade\n",
            "decent\n",
            "decide\n",
            "decided\n",
            "decides\n",
            "decision\n",
            "deep\n",
            "deeply\n",
            "definitely\n",
            "deliver\n",
            "delivers\n",
            "depth\n",
            "describe\n",
            "deserve\n",
            "deserved\n",
            "deserves\n",
            "design\n",
            "desire\n",
            "desperate\n",
            "despite\n",
            "detail\n",
            "detective\n",
            "developed\n",
            "development\n",
            "dialog\n",
            "dialogue\n",
            "die\n",
            "died\n",
            "difference\n",
            "different\n",
            "difficult\n",
            "direct\n",
            "directed\n",
            "directing\n",
            "direction\n",
            "director\n",
            "dirty\n",
            "disappointed\n",
            "disappointing\n",
            "disappointment\n",
            "disaster\n",
            "discover\n",
            "disney\n",
            "display\n",
            "disturbing\n",
            "doctor\n",
            "documentary\n",
            "doe\n",
            "dog\n",
            "dollar\n",
            "done\n",
            "door\n",
            "double\n",
            "doubt\n",
            "dozen\n",
            "dr.\n",
            "drag\n",
            "drama\n",
            "dramatic\n",
            "draw\n",
            "drawn\n",
            "dream\n",
            "drive\n",
            "drop\n",
            "drug\n",
            "drunk\n",
            "due\n",
            "dull\n",
            "dumb\n",
            "dvd\n",
            "dy\n",
            "dying\n",
            "earlier\n",
            "early\n",
            "earth\n",
            "easily\n",
            "easy\n",
            "edge\n",
            "editing\n",
            "effect\n",
            "effective\n",
            "effort\n",
            "either\n",
            "element\n",
            "else\n",
            "emotion\n",
            "emotional\n",
            "encounter\n",
            "end\n",
            "ended\n",
            "ending\n",
            "energy\n",
            "engaging\n",
            "england\n",
            "english\n",
            "enjoy\n",
            "enjoyable\n",
            "enjoyed\n",
            "enough\n",
            "entertaining\n",
            "entertainment\n",
            "entire\n",
            "entirely\n",
            "epic\n",
            "episode\n",
            "equally\n",
            "era\n",
            "escape\n",
            "especially\n",
            "etc\n",
            "european\n",
            "even\n",
            "event\n",
            "eventually\n",
            "ever\n",
            "every\n",
            "everybody\n",
            "everyone\n",
            "everything\n",
            "evil\n",
            "exactly\n",
            "example\n",
            "excellent\n",
            "except\n",
            "exception\n",
            "exciting\n",
            "excuse\n",
            "exist\n",
            "expect\n",
            "expectation\n",
            "expected\n",
            "expecting\n",
            "experience\n",
            "explain\n",
            "explained\n",
            "explanation\n",
            "expression\n",
            "extra\n",
            "extreme\n",
            "extremely\n",
            "eye\n",
            "face\n",
            "fact\n",
            "fail\n",
            "failed\n",
            "fails\n",
            "failure\n",
            "fair\n",
            "fairly\n",
            "fake\n",
            "fall\n",
            "falling\n",
            "familiar\n",
            "family\n",
            "famous\n",
            "fan\n",
            "fantastic\n",
            "fantasy\n",
            "far\n",
            "fascinating\n",
            "fashion\n",
            "fast\n",
            "fate\n",
            "father\n",
            "fault\n",
            "favor\n",
            "favorite\n",
            "favourite\n",
            "fear\n",
            "feature\n",
            "featuring\n",
            "feel\n",
            "feeling\n",
            "fell\n",
            "fellow\n",
            "felt\n",
            "female\n",
            "festival\n",
            "fiction\n",
            "field\n",
            "fight\n",
            "fighting\n",
            "figure\n",
            "fill\n",
            "filled\n",
            "filmed\n",
            "filming\n",
            "filmmaker\n",
            "final\n",
            "finally\n",
            "find\n",
            "finding\n",
            "fine\n",
            "finest\n",
            "finish\n",
            "finished\n",
            "fire\n",
            "first\n",
            "fit\n",
            "five\n",
            "flashback\n",
            "flat\n",
            "flaw\n",
            "flick\n",
            "floor\n",
            "fly\n",
            "flying\n",
            "focus\n",
            "folk\n",
            "follow\n",
            "followed\n",
            "following\n",
            "follows\n",
            "food\n",
            "foot\n",
            "footage\n",
            "force\n",
            "forced\n",
            "forever\n",
            "forget\n",
            "forgotten\n",
            "form\n",
            "former\n",
            "forward\n",
            "found\n",
            "four\n",
            "frame\n",
            "frank\n",
            "free\n",
            "french\n",
            "fresh\n",
            "friend\n",
            "friendship\n",
            "front\n",
            "full\n",
            "fully\n",
            "fun\n",
            "funniest\n",
            "funny\n",
            "future\n",
            "gag\n",
            "game\n",
            "gang\n",
            "garbage\n",
            "gave\n",
            "gay\n",
            "gem\n",
            "general\n",
            "generally\n",
            "generation\n",
            "genius\n",
            "genre\n",
            "george\n",
            "german\n",
            "get\n",
            "getting\n",
            "ghost\n",
            "giant\n",
            "girl\n",
            "girlfriend\n",
            "give\n",
            "given\n",
            "giving\n",
            "glad\n",
            "go\n",
            "god\n",
            "going\n",
            "gone\n",
            "good\n",
            "gore\n",
            "gorgeous\n",
            "got\n",
            "gotten\n",
            "government\n",
            "grace\n",
            "grade\n",
            "graphic\n",
            "great\n",
            "greatest\n",
            "green\n",
            "ground\n",
            "group\n",
            "growing\n",
            "guess\n",
            "gun\n",
            "guy\n",
            "hair\n",
            "half\n",
            "hand\n",
            "happen\n",
            "happened\n",
            "happening\n",
            "happens\n",
            "happy\n",
            "hard\n",
            "hardly\n",
            "hate\n",
            "hated\n",
            "head\n",
            "hear\n",
            "heard\n",
            "heart\n",
            "heaven\n",
            "heavy\n",
            "held\n",
            "hell\n",
            "help\n",
            "helped\n",
            "hero\n",
            "hey\n",
            "hidden\n",
            "hide\n",
            "high\n",
            "highlight\n",
            "highly\n",
            "hilarious\n",
            "hill\n",
            "historical\n",
            "history\n",
            "hit\n",
            "hold\n",
            "hole\n",
            "hollywood\n",
            "home\n",
            "honest\n",
            "honestly\n",
            "hope\n",
            "hoping\n",
            "horrible\n",
            "horror\n",
            "horse\n",
            "hospital\n",
            "hot\n",
            "hotel\n",
            "hour\n",
            "house\n",
            "however\n",
            "huge\n",
            "human\n",
            "humor\n",
            "humour\n",
            "hundred\n",
            "hurt\n",
            "husband\n",
            "idea\n",
            "identity\n",
            "idiot\n",
            "ii\n",
            "image\n",
            "imagination\n",
            "imagine\n",
            "imdb\n",
            "immediately\n",
            "impact\n",
            "important\n",
            "impossible\n",
            "impressed\n",
            "impression\n",
            "impressive\n",
            "include\n",
            "included\n",
            "includes\n",
            "including\n",
            "incredible\n",
            "incredibly\n",
            "indeed\n",
            "independent\n",
            "indian\n",
            "individual\n",
            "industry\n",
            "information\n",
            "innocent\n",
            "inside\n",
            "inspired\n",
            "instance\n",
            "instead\n",
            "insult\n",
            "intelligence\n",
            "intelligent\n",
            "intended\n",
            "intense\n",
            "intention\n",
            "interest\n",
            "interested\n",
            "interesting\n",
            "interview\n",
            "intriguing\n",
            "introduced\n",
            "involved\n",
            "involving\n",
            "island\n",
            "issue\n",
            "italian\n",
            "jack\n",
            "james\n",
            "jane\n",
            "japanese\n",
            "jason\n",
            "jim\n",
            "job\n",
            "joe\n",
            "john\n",
            "joke\n",
            "jones\n",
            "journey\n",
            "joy\n",
            "judge\n",
            "jump\n",
            "justice\n",
            "keep\n",
            "keeping\n",
            "kept\n",
            "key\n",
            "kick\n",
            "kid\n",
            "kill\n",
            "killed\n",
            "killer\n",
            "killing\n",
            "kind\n",
            "kinda\n",
            "king\n",
            "knew\n",
            "know\n",
            "knowing\n",
            "known\n",
            "la\n",
            "lack\n",
            "lacking\n",
            "lady\n",
            "lame\n",
            "land\n",
            "language\n",
            "large\n",
            "last\n",
            "late\n",
            "later\n",
            "latter\n",
            "laugh\n",
            "laughable\n",
            "laughed\n",
            "laughing\n",
            "law\n",
            "le\n",
            "lead\n",
            "leader\n",
            "leading\n",
            "leaf\n",
            "learn\n",
            "least\n",
            "leave\n",
            "leaving\n",
            "led\n",
            "lee\n",
            "left\n",
            "legend\n",
            "length\n",
            "lesson\n",
            "let\n",
            "level\n",
            "lie\n",
            "life\n",
            "light\n",
            "lighting\n",
            "likable\n",
            "liked\n",
            "likely\n",
            "limited\n",
            "line\n",
            "list\n",
            "listen\n",
            "literally\n",
            "little\n",
            "live\n",
            "lived\n",
            "living\n",
            "local\n",
            "location\n",
            "london\n",
            "long\n",
            "longer\n",
            "look\n",
            "looked\n",
            "looking\n",
            "loose\n",
            "lord\n",
            "lose\n",
            "loss\n",
            "lost\n",
            "lot\n",
            "loud\n",
            "love\n",
            "loved\n",
            "lovely\n",
            "lover\n",
            "loving\n",
            "low\n",
            "machine\n",
            "mad\n",
            "made\n",
            "magic\n",
            "main\n",
            "mainly\n",
            "major\n",
            "make\n",
            "maker\n",
            "making\n",
            "male\n",
            "man\n",
            "manage\n",
            "managed\n",
            "manages\n",
            "manner\n",
            "many\n",
            "mark\n",
            "marriage\n",
            "married\n",
            "martin\n",
            "mary\n",
            "master\n",
            "masterpiece\n",
            "match\n",
            "material\n",
            "matter\n",
            "may\n",
            "maybe\n",
            "mean\n",
            "meaning\n",
            "meant\n",
            "mediocre\n",
            "medium\n",
            "meet\n",
            "member\n",
            "memorable\n",
            "memory\n",
            "men\n",
            "mental\n",
            "mention\n",
            "mentioned\n",
            "merely\n",
            "mess\n",
            "message\n",
            "met\n",
            "michael\n",
            "middle\n",
            "might\n",
            "mile\n",
            "military\n",
            "million\n",
            "mind\n",
            "mine\n",
            "minor\n",
            "minute\n",
            "miss\n",
            "missed\n",
            "missing\n",
            "mistake\n",
            "mix\n",
            "model\n",
            "modern\n",
            "mom\n",
            "moment\n",
            "money\n",
            "monster\n",
            "month\n",
            "mood\n",
            "moral\n",
            "mostly\n",
            "mother\n",
            "motion\n",
            "mouth\n",
            "move\n",
            "moved\n",
            "movement\n",
            "moving\n",
            "mr.\n",
            "much\n",
            "murder\n",
            "music\n",
            "musical\n",
            "must\n",
            "mysterious\n",
            "mystery\n",
            "na\n",
            "naked\n",
            "name\n",
            "named\n",
            "narrative\n",
            "nasty\n",
            "natural\n",
            "nature\n",
            "near\n",
            "nearly\n",
            "necessary\n",
            "need\n",
            "needed\n",
            "negative\n",
            "neither\n",
            "never\n",
            "new\n",
            "news\n",
            "next\n",
            "nice\n",
            "nicely\n",
            "night\n",
            "nightmare\n",
            "nobody\n",
            "none\n",
            "normal\n",
            "normally\n",
            "note\n",
            "nothing\n",
            "notice\n",
            "noticed\n",
            "novel\n",
            "nowhere\n",
            "nudity\n",
            "number\n",
            "numerous\n",
            "obvious\n",
            "obviously\n",
            "odd\n",
            "offer\n",
            "office\n",
            "officer\n",
            "often\n",
            "oh\n",
            "ok\n",
            "okay\n",
            "old\n",
            "older\n",
            "onto\n",
            "open\n",
            "opening\n",
            "opera\n",
            "opinion\n",
            "opportunity\n",
            "opposite\n",
            "order\n",
            "original\n",
            "originally\n",
            "oscar\n",
            "others\n",
            "otherwise\n",
            "outside\n",
            "outstanding\n",
            "overall\n",
            "owner\n",
            "pace\n",
            "pacing\n",
            "page\n",
            "paid\n",
            "pain\n",
            "painful\n",
            "parent\n",
            "park\n",
            "part\n",
            "particular\n",
            "particularly\n",
            "partner\n",
            "party\n",
            "pas\n",
            "passion\n",
            "past\n",
            "pathetic\n",
            "paul\n",
            "pay\n",
            "people\n",
            "perfect\n",
            "perfectly\n",
            "performance\n",
            "perhaps\n",
            "period\n",
            "person\n",
            "personal\n",
            "personality\n",
            "personally\n",
            "perspective\n",
            "peter\n",
            "phone\n",
            "photography\n",
            "physical\n",
            "pick\n",
            "picked\n",
            "picture\n",
            "piece\n",
            "place\n",
            "plain\n",
            "plan\n",
            "plane\n",
            "planet\n",
            "play\n",
            "played\n",
            "player\n",
            "playing\n",
            "please\n",
            "pleasure\n",
            "plenty\n",
            "plot\n",
            "plus\n",
            "point\n",
            "pointless\n",
            "police\n",
            "political\n",
            "poor\n",
            "poorly\n",
            "pop\n",
            "popular\n",
            "porn\n",
            "portray\n",
            "portrayal\n",
            "portrayed\n",
            "positive\n",
            "possible\n",
            "possibly\n",
            "potential\n",
            "power\n",
            "powerful\n",
            "predictable\n",
            "premise\n",
            "presence\n",
            "present\n",
            "presented\n",
            "pretty\n",
            "previous\n",
            "price\n",
            "prison\n",
            "probably\n",
            "problem\n",
            "process\n",
            "produce\n",
            "produced\n",
            "producer\n",
            "product\n",
            "production\n",
            "prof\n",
            "professional\n",
            "project\n",
            "promise\n",
            "protagonist\n",
            "prove\n",
            "provide\n",
            "provides\n",
            "public\n",
            "pull\n",
            "pulled\n",
            "pure\n",
            "purpose\n",
            "put\n",
            "putting\n",
            "quality\n",
            "queen\n",
            "question\n",
            "quick\n",
            "quickly\n",
            "quiet\n",
            "quite\n",
            "race\n",
            "random\n",
            "rape\n",
            "rare\n",
            "rarely\n",
            "rate\n",
            "rated\n",
            "rather\n",
            "rating\n",
            "ray\n",
            "reach\n",
            "reaction\n",
            "read\n",
            "reading\n",
            "ready\n",
            "real\n",
            "realistic\n",
            "reality\n",
            "realize\n",
            "realized\n",
            "really\n",
            "reason\n",
            "recent\n",
            "recently\n",
            "recommend\n",
            "recommended\n",
            "record\n",
            "red\n",
            "redeeming\n",
            "reference\n",
            "regular\n",
            "relationship\n",
            "release\n",
            "released\n",
            "remains\n",
            "remake\n",
            "remarkable\n",
            "remember\n",
            "reminded\n",
            "reminds\n",
            "rent\n",
            "rented\n",
            "respect\n",
            "responsible\n",
            "rest\n",
            "result\n",
            "return\n",
            "revenge\n",
            "review\n",
            "reviewer\n",
            "rich\n",
            "richard\n",
            "ride\n",
            "ridiculous\n",
            "right\n",
            "ring\n",
            "rise\n",
            "road\n",
            "robert\n",
            "rock\n",
            "role\n",
            "roll\n",
            "romance\n",
            "romantic\n",
            "room\n",
            "ruin\n",
            "rule\n",
            "run\n",
            "running\n",
            "sad\n",
            "sadly\n",
            "said\n",
            "sam\n",
            "sat\n",
            "save\n",
            "saved\n",
            "saving\n",
            "saw\n",
            "say\n",
            "saying\n",
            "scare\n",
            "scared\n",
            "scary\n",
            "scene\n",
            "scenery\n",
            "school\n",
            "sci-fi\n",
            "science\n",
            "scientist\n",
            "score\n",
            "scott\n",
            "scream\n",
            "screen\n",
            "screenplay\n",
            "script\n",
            "search\n",
            "season\n",
            "second\n",
            "secret\n",
            "see\n",
            "seeing\n",
            "seem\n",
            "seemed\n",
            "seemingly\n",
            "seems\n",
            "seen\n",
            "self\n",
            "sell\n",
            "sense\n",
            "sent\n",
            "sequel\n",
            "sequence\n",
            "serial\n",
            "series\n",
            "serious\n",
            "seriously\n",
            "set\n",
            "setting\n",
            "several\n",
            "sex\n",
            "sexual\n",
            "sexy\n",
            "shame\n",
            "share\n",
            "ship\n",
            "shock\n",
            "shocking\n",
            "shoot\n",
            "shooting\n",
            "short\n",
            "shot\n",
            "show\n",
            "showed\n",
            "showing\n",
            "shown\n",
            "sick\n",
            "side\n",
            "sight\n",
            "sign\n",
            "silent\n",
            "silly\n",
            "similar\n",
            "simple\n",
            "simply\n",
            "since\n",
            "singer\n",
            "singing\n",
            "single\n",
            "sister\n",
            "sit\n",
            "sitting\n",
            "situation\n",
            "six\n",
            "skill\n",
            "skip\n",
            "slasher\n",
            "sleep\n",
            "slightly\n",
            "slow\n",
            "slowly\n",
            "small\n",
            "smart\n",
            "smile\n",
            "smith\n",
            "social\n",
            "society\n",
            "soldier\n",
            "solid\n",
            "somebody\n",
            "somehow\n",
            "someone\n",
            "something\n",
            "sometimes\n",
            "somewhat\n",
            "somewhere\n",
            "son\n",
            "song\n",
            "soon\n",
            "sorry\n",
            "sort\n",
            "soul\n",
            "sound\n",
            "soundtrack\n",
            "south\n",
            "space\n",
            "speak\n",
            "speaking\n",
            "special\n",
            "spend\n",
            "spent\n",
            "spirit\n",
            "spoiler\n",
            "spot\n",
            "stage\n",
            "stand\n",
            "standard\n",
            "star\n",
            "starring\n",
            "start\n",
            "started\n",
            "starting\n",
            "state\n",
            "station\n",
            "stay\n",
            "steal\n",
            "step\n",
            "stephen\n",
            "stereotype\n",
            "steve\n",
            "stick\n",
            "still\n",
            "stone\n",
            "stop\n",
            "store\n",
            "story\n",
            "storyline\n",
            "straight\n",
            "strange\n",
            "street\n",
            "strength\n",
            "strong\n",
            "struggle\n",
            "stuck\n",
            "student\n",
            "studio\n",
            "study\n",
            "stuff\n",
            "stunning\n",
            "stupid\n",
            "style\n",
            "subject\n",
            "subtle\n",
            "success\n",
            "successful\n",
            "suck\n",
            "suddenly\n",
            "suggest\n",
            "suicide\n",
            "suit\n",
            "summer\n",
            "super\n",
            "superb\n",
            "superior\n",
            "support\n",
            "supporting\n",
            "suppose\n",
            "supposed\n",
            "supposedly\n",
            "sure\n",
            "surely\n",
            "surprise\n",
            "surprised\n",
            "surprising\n",
            "surprisingly\n",
            "suspect\n",
            "suspense\n",
            "sweet\n",
            "system\n",
            "take\n",
            "taken\n",
            "taking\n",
            "tale\n",
            "talent\n",
            "talented\n",
            "talk\n",
            "talking\n",
            "taste\n",
            "teacher\n",
            "team\n",
            "tear\n",
            "technical\n",
            "teen\n",
            "teenage\n",
            "teenager\n",
            "television\n",
            "tell\n",
            "telling\n",
            "ten\n",
            "tension\n",
            "term\n",
            "terrible\n",
            "terribly\n",
            "terrific\n",
            "thank\n",
            "thanks\n",
            "thats\n",
            "theater\n",
            "theatre\n",
            "theme\n",
            "therefore\n",
            "thin\n",
            "thing\n",
            "think\n",
            "thinking\n",
            "third\n",
            "thoroughly\n",
            "though\n",
            "thought\n",
            "thousand\n",
            "three\n",
            "thriller\n",
            "throughout\n",
            "throw\n",
            "thrown\n",
            "thus\n",
            "time\n",
            "tired\n",
            "title\n",
            "today\n",
            "together\n",
            "told\n",
            "tom\n",
            "tone\n",
            "tony\n",
            "took\n",
            "top\n",
            "torture\n",
            "total\n",
            "totally\n",
            "touch\n",
            "touching\n",
            "tough\n",
            "toward\n",
            "towards\n",
            "town\n",
            "track\n",
            "tragedy\n",
            "tragic\n",
            "trailer\n",
            "train\n",
            "trash\n",
            "travel\n",
            "treat\n",
            "treated\n",
            "trick\n",
            "tried\n",
            "trip\n",
            "trouble\n",
            "true\n",
            "truly\n",
            "trust\n",
            "truth\n",
            "try\n",
            "trying\n",
            "turn\n",
            "turned\n",
            "turning\n",
            "tv\n",
            "twice\n",
            "twist\n",
            "two\n",
            "type\n",
            "typical\n",
            "u\n",
            "ugly\n",
            "ultimately\n",
            "unbelievable\n",
            "understand\n",
            "understanding\n",
            "unfortunately\n",
            "unique\n",
            "unknown\n",
            "unless\n",
            "unlike\n",
            "unnecessary\n",
            "unusual\n",
            "upon\n",
            "us\n",
            "use\n",
            "used\n",
            "using\n",
            "usual\n",
            "usually\n",
            "utterly\n",
            "value\n",
            "vampire\n",
            "van\n",
            "various\n",
            "vehicle\n",
            "version\n",
            "victim\n",
            "video\n",
            "view\n",
            "viewer\n",
            "viewing\n",
            "villain\n",
            "violence\n",
            "violent\n",
            "vision\n",
            "visit\n",
            "visual\n",
            "voice\n",
            "vote\n",
            "wait\n",
            "waiting\n",
            "walk\n",
            "walking\n",
            "wall\n",
            "want\n",
            "wanted\n",
            "wanting\n",
            "war\n",
            "warning\n",
            "waste\n",
            "wasted\n",
            "watch\n",
            "watchable\n",
            "watched\n",
            "watching\n",
            "water\n",
            "way\n",
            "weak\n",
            "weapon\n",
            "wear\n",
            "wearing\n",
            "week\n",
            "weird\n",
            "well\n",
            "went\n",
            "west\n",
            "western\n",
            "whatever\n",
            "whatsoever\n",
            "whenever\n",
            "whether\n",
            "white\n",
            "whole\n",
            "whose\n",
            "wife\n",
            "wild\n",
            "william\n",
            "willing\n",
            "win\n",
            "wind\n",
            "window\n",
            "winning\n",
            "wish\n",
            "witch\n",
            "within\n",
            "without\n",
            "witness\n",
            "witty\n",
            "wo\n",
            "woman\n",
            "wonder\n",
            "wonderful\n",
            "wonderfully\n",
            "wondering\n",
            "wood\n",
            "wooden\n",
            "word\n",
            "work\n",
            "worked\n",
            "working\n",
            "world\n",
            "worse\n",
            "worst\n",
            "worth\n",
            "worthy\n",
            "would\n",
            "wow\n",
            "write\n",
            "writer\n",
            "writing\n",
            "written\n",
            "wrong\n",
            "wrote\n",
            "yeah\n",
            "year\n",
            "yes\n",
            "yet\n",
            "york\n",
            "young\n",
            "younger\n",
            "zero\n",
            "zombie\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for d in vectorizer.get_feature_names_out():\n",
        "  print(d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TylIC8GwtFrX"
      },
      "source": [
        "#### Without priors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPPp_riktFrX",
        "outputId": "fa6898fd-e8f1-4d9f-b4d8-6c776e027d88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.83548\n"
          ]
        }
      ],
      "source": [
        "model=MultinomialNaiveBayes()\n",
        "params=model.fit(X_train, y_train, 0,0,0,0)\n",
        "y_prob=model.predict(X_test)\n",
        "y_pred = np.argmax(y_prob, 1)\n",
        "print(\"Accuracy:\",model.evaluate_acc(y_pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPDFDvUstFrX"
      },
      "source": [
        "#### With priors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjksySRVtFrX",
        "outputId": "ff47d7e8-55b5-4c33-fbd8-bd047d3d38f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.835\n"
          ]
        }
      ],
      "source": [
        "model=MultinomialNaiveBayes()\n",
        "params=model.fit(X_train, y_train, 1,1,10,10)\n",
        "y_prob=model.predict(X_test)\n",
        "y_pred = np.argmax(y_prob, 1)\n",
        "print(\"Accuracy:\",model.evaluate_acc(y_pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFauj00FtFrY"
      },
      "source": [
        "### Also cleaning special characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tYW463vtFrY"
      },
      "outputs": [],
      "source": [
        "for i in range(len(x_train)):\n",
        "  x_train[i] = re.sub(r'[^a-zA-z0-9\\s]', ' ', x_train[i]).lower()\n",
        "  x_test[i] = re.sub(r'[^a-zA-z0-9\\s]', ' ', x_test[i]).lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "St30VXqwtFrY"
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer(stop_words=stopword_list, tokenizer=LemmaTokenizer(), min_df=0.01, max_df=0.35)\n",
        "X_train = vectorizer.fit_transform(x_train)\n",
        "dic= vectorizer.get_feature_names_out()\n",
        "X_test=vectorizer.transform(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7m2Z8AKytFrY",
        "outputId": "b79c2c70-12e9-4039-815a-252d22ec2af6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dictionary length: 1666\n"
          ]
        }
      ],
      "source": [
        "print(\"Dictionary length:\", len(vectorizer.get_feature_names_out()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rqe6nmWptFrZ",
        "outputId": "b8aebcb0-7a37-4624-ae57-9d8f9448cf32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "10\n",
            "100\n",
            "11\n",
            "12\n",
            "15\n",
            "2\n",
            "20\n",
            "3\n",
            "30\n",
            "4\n",
            "40\n",
            "5\n",
            "50\n",
            "6\n",
            "60\n",
            "7\n",
            "70\n",
            "8\n",
            "80\n",
            "9\n",
            "90\n",
            "ability\n",
            "able\n",
            "absolute\n",
            "absolutely\n",
            "absurd\n",
            "academy\n",
            "accent\n",
            "accept\n",
            "accident\n",
            "according\n",
            "accurate\n",
            "across\n",
            "act\n",
            "acted\n",
            "acting\n",
            "action\n",
            "actor\n",
            "actress\n",
            "actual\n",
            "actually\n",
            "adam\n",
            "adaptation\n",
            "add\n",
            "added\n",
            "addition\n",
            "admit\n",
            "adult\n",
            "adventure\n",
            "affair\n",
            "afraid\n",
            "age\n",
            "agent\n",
            "ago\n",
            "agree\n",
            "ahead\n",
            "air\n",
            "al\n",
            "alien\n",
            "alive\n",
            "allow\n",
            "allowed\n",
            "almost\n",
            "alone\n",
            "along\n",
            "already\n",
            "also\n",
            "although\n",
            "always\n",
            "amazing\n",
            "america\n",
            "american\n",
            "among\n",
            "amount\n",
            "amusing\n",
            "angel\n",
            "angle\n",
            "angry\n",
            "animal\n",
            "animated\n",
            "animation\n",
            "annoying\n",
            "another\n",
            "answer\n",
            "anti\n",
            "anybody\n",
            "anymore\n",
            "anyone\n",
            "anything\n",
            "anyway\n",
            "anywhere\n",
            "apart\n",
            "apartment\n",
            "apparent\n",
            "apparently\n",
            "appeal\n",
            "appear\n",
            "appearance\n",
            "appeared\n",
            "appears\n",
            "appreciate\n",
            "approach\n",
            "area\n",
            "arm\n",
            "army\n",
            "around\n",
            "art\n",
            "artist\n",
            "artistic\n",
            "aside\n",
            "ask\n",
            "asked\n",
            "asks\n",
            "aspect\n",
            "atmosphere\n",
            "attack\n",
            "attempt\n",
            "attention\n",
            "attitude\n",
            "attractive\n",
            "audience\n",
            "author\n",
            "available\n",
            "average\n",
            "avoid\n",
            "award\n",
            "aware\n",
            "away\n",
            "awesome\n",
            "awful\n",
            "b\n",
            "baby\n",
            "back\n",
            "background\n",
            "bad\n",
            "badly\n",
            "ball\n",
            "band\n",
            "bar\n",
            "barely\n",
            "based\n",
            "basic\n",
            "basically\n",
            "battle\n",
            "bear\n",
            "beat\n",
            "beautiful\n",
            "beautifully\n",
            "beauty\n",
            "became\n",
            "become\n",
            "becomes\n",
            "becoming\n",
            "bed\n",
            "began\n",
            "begin\n",
            "beginning\n",
            "behavior\n",
            "behind\n",
            "belief\n",
            "believable\n",
            "believe\n",
            "ben\n",
            "besides\n",
            "best\n",
            "better\n",
            "beyond\n",
            "big\n",
            "biggest\n",
            "bill\n",
            "billy\n",
            "bit\n",
            "bizarre\n",
            "black\n",
            "blame\n",
            "bland\n",
            "blood\n",
            "bloody\n",
            "blow\n",
            "blue\n",
            "board\n",
            "body\n",
            "bond\n",
            "book\n",
            "bored\n",
            "boring\n",
            "born\n",
            "bos\n",
            "bother\n",
            "bottom\n",
            "bought\n",
            "box\n",
            "boy\n",
            "boyfriend\n",
            "brain\n",
            "break\n",
            "brian\n",
            "brief\n",
            "bright\n",
            "brilliant\n",
            "bring\n",
            "brings\n",
            "british\n",
            "brother\n",
            "brought\n",
            "brutal\n",
            "buck\n",
            "buddy\n",
            "budget\n",
            "build\n",
            "building\n",
            "bunch\n",
            "business\n",
            "buy\n",
            "c\n",
            "cable\n",
            "call\n",
            "called\n",
            "came\n",
            "cameo\n",
            "camera\n",
            "camp\n",
            "capture\n",
            "car\n",
            "care\n",
            "career\n",
            "carry\n",
            "cartoon\n",
            "case\n",
            "cast\n",
            "casting\n",
            "cat\n",
            "catch\n",
            "caught\n",
            "cause\n",
            "center\n",
            "central\n",
            "century\n",
            "certain\n",
            "certainly\n",
            "cgi\n",
            "chance\n",
            "change\n",
            "changed\n",
            "channel\n",
            "character\n",
            "charles\n",
            "charm\n",
            "charming\n",
            "chase\n",
            "cheap\n",
            "check\n",
            "cheesy\n",
            "chemistry\n",
            "chick\n",
            "child\n",
            "childhood\n",
            "choice\n",
            "chris\n",
            "christian\n",
            "christmas\n",
            "christopher\n",
            "church\n",
            "cinema\n",
            "cinematic\n",
            "cinematography\n",
            "city\n",
            "claim\n",
            "class\n",
            "classic\n",
            "clear\n",
            "clearly\n",
            "clever\n",
            "clich\n",
            "climax\n",
            "close\n",
            "clothes\n",
            "club\n",
            "clue\n",
            "co\n",
            "cold\n",
            "collection\n",
            "college\n",
            "color\n",
            "come\n",
            "comedic\n",
            "comedy\n",
            "comic\n",
            "coming\n",
            "comment\n",
            "commentary\n",
            "commercial\n",
            "common\n",
            "community\n",
            "company\n",
            "compare\n",
            "compared\n",
            "comparison\n",
            "compelling\n",
            "complete\n",
            "completely\n",
            "complex\n",
            "computer\n",
            "concept\n",
            "concerned\n",
            "conclusion\n",
            "conflict\n",
            "confused\n",
            "confusing\n",
            "connection\n",
            "consider\n",
            "considered\n",
            "considering\n",
            "constant\n",
            "constantly\n",
            "contains\n",
            "content\n",
            "continue\n",
            "continues\n",
            "control\n",
            "conversation\n",
            "convincing\n",
            "cool\n",
            "cop\n",
            "copy\n",
            "cost\n",
            "costume\n",
            "could\n",
            "count\n",
            "country\n",
            "couple\n",
            "course\n",
            "cover\n",
            "crap\n",
            "crazy\n",
            "create\n",
            "created\n",
            "creating\n",
            "creative\n",
            "creature\n",
            "credit\n",
            "creepy\n",
            "crew\n",
            "crime\n",
            "criminal\n",
            "critic\n",
            "cross\n",
            "cry\n",
            "cult\n",
            "culture\n",
            "current\n",
            "cut\n",
            "cute\n",
            "dad\n",
            "damn\n",
            "dance\n",
            "dancing\n",
            "dangerous\n",
            "dark\n",
            "date\n",
            "daughter\n",
            "david\n",
            "day\n",
            "de\n",
            "dead\n",
            "deal\n",
            "dealing\n",
            "death\n",
            "decade\n",
            "decent\n",
            "decide\n",
            "decided\n",
            "decides\n",
            "decision\n",
            "deep\n",
            "deeply\n",
            "definitely\n",
            "delightful\n",
            "deliver\n",
            "delivers\n",
            "depth\n",
            "describe\n",
            "deserve\n",
            "deserved\n",
            "deserves\n",
            "design\n",
            "desire\n",
            "desperate\n",
            "despite\n",
            "detail\n",
            "detective\n",
            "developed\n",
            "development\n",
            "devil\n",
            "dialog\n",
            "dialogue\n",
            "die\n",
            "died\n",
            "difference\n",
            "different\n",
            "difficult\n",
            "direct\n",
            "directed\n",
            "directing\n",
            "direction\n",
            "director\n",
            "dirty\n",
            "disappointed\n",
            "disappointing\n",
            "disappointment\n",
            "disaster\n",
            "discover\n",
            "discovered\n",
            "disney\n",
            "display\n",
            "disturbing\n",
            "doctor\n",
            "documentary\n",
            "doe\n",
            "dog\n",
            "dollar\n",
            "done\n",
            "door\n",
            "double\n",
            "doubt\n",
            "dozen\n",
            "dr\n",
            "drag\n",
            "drama\n",
            "dramatic\n",
            "draw\n",
            "drawn\n",
            "dream\n",
            "drive\n",
            "drop\n",
            "drug\n",
            "drunk\n",
            "due\n",
            "dull\n",
            "dumb\n",
            "dvd\n",
            "dy\n",
            "dying\n",
            "e\n",
            "earlier\n",
            "early\n",
            "earth\n",
            "easily\n",
            "easy\n",
            "ed\n",
            "edge\n",
            "editing\n",
            "effect\n",
            "effective\n",
            "effort\n",
            "either\n",
            "element\n",
            "else\n",
            "emotion\n",
            "emotional\n",
            "empty\n",
            "encounter\n",
            "end\n",
            "ended\n",
            "ending\n",
            "enemy\n",
            "energy\n",
            "engaging\n",
            "england\n",
            "english\n",
            "enjoy\n",
            "enjoyable\n",
            "enjoyed\n",
            "enough\n",
            "entertaining\n",
            "entertainment\n",
            "entire\n",
            "entirely\n",
            "epic\n",
            "episode\n",
            "equally\n",
            "era\n",
            "escape\n",
            "especially\n",
            "essentially\n",
            "etc\n",
            "european\n",
            "even\n",
            "event\n",
            "eventually\n",
            "ever\n",
            "every\n",
            "everybody\n",
            "everyone\n",
            "everything\n",
            "evil\n",
            "ex\n",
            "exactly\n",
            "example\n",
            "excellent\n",
            "except\n",
            "exception\n",
            "exciting\n",
            "excuse\n",
            "exist\n",
            "existence\n",
            "expect\n",
            "expectation\n",
            "expected\n",
            "expecting\n",
            "experience\n",
            "explain\n",
            "explained\n",
            "explanation\n",
            "expression\n",
            "extra\n",
            "extreme\n",
            "extremely\n",
            "eye\n",
            "f\n",
            "face\n",
            "fact\n",
            "factor\n",
            "fail\n",
            "failed\n",
            "fails\n",
            "failure\n",
            "fair\n",
            "fairly\n",
            "fake\n",
            "fall\n",
            "falling\n",
            "familiar\n",
            "family\n",
            "famous\n",
            "fan\n",
            "fantastic\n",
            "fantasy\n",
            "far\n",
            "fascinating\n",
            "fashion\n",
            "fast\n",
            "fate\n",
            "father\n",
            "fault\n",
            "favor\n",
            "favorite\n",
            "favourite\n",
            "fear\n",
            "feature\n",
            "featuring\n",
            "feel\n",
            "feeling\n",
            "fell\n",
            "fellow\n",
            "felt\n",
            "female\n",
            "festival\n",
            "fi\n",
            "fiction\n",
            "field\n",
            "fight\n",
            "fighting\n",
            "figure\n",
            "fill\n",
            "filled\n",
            "filmed\n",
            "filming\n",
            "filmmaker\n",
            "final\n",
            "finale\n",
            "finally\n",
            "find\n",
            "finding\n",
            "fine\n",
            "finest\n",
            "finish\n",
            "finished\n",
            "fire\n",
            "first\n",
            "fit\n",
            "five\n",
            "flashback\n",
            "flat\n",
            "flaw\n",
            "flick\n",
            "floor\n",
            "fly\n",
            "flying\n",
            "focus\n",
            "folk\n",
            "follow\n",
            "followed\n",
            "following\n",
            "follows\n",
            "food\n",
            "foot\n",
            "footage\n",
            "force\n",
            "forced\n",
            "forever\n",
            "forget\n",
            "forgotten\n",
            "form\n",
            "former\n",
            "forward\n",
            "found\n",
            "four\n",
            "frame\n",
            "frank\n",
            "free\n",
            "french\n",
            "fresh\n",
            "friend\n",
            "friendship\n",
            "front\n",
            "full\n",
            "fully\n",
            "fun\n",
            "funniest\n",
            "funny\n",
            "future\n",
            "g\n",
            "gag\n",
            "game\n",
            "gang\n",
            "gangster\n",
            "garbage\n",
            "gave\n",
            "gay\n",
            "gem\n",
            "general\n",
            "generally\n",
            "generation\n",
            "genius\n",
            "genre\n",
            "george\n",
            "german\n",
            "get\n",
            "getting\n",
            "ghost\n",
            "giant\n",
            "girl\n",
            "girlfriend\n",
            "give\n",
            "given\n",
            "giving\n",
            "glad\n",
            "go\n",
            "god\n",
            "going\n",
            "gone\n",
            "gore\n",
            "gorgeous\n",
            "got\n",
            "gotten\n",
            "government\n",
            "grace\n",
            "grade\n",
            "grand\n",
            "graphic\n",
            "great\n",
            "greatest\n",
            "green\n",
            "ground\n",
            "group\n",
            "growing\n",
            "guess\n",
            "gun\n",
            "guy\n",
            "hair\n",
            "half\n",
            "hand\n",
            "happen\n",
            "happened\n",
            "happening\n",
            "happens\n",
            "happy\n",
            "hard\n",
            "hardly\n",
            "hate\n",
            "hated\n",
            "head\n",
            "hear\n",
            "heard\n",
            "heart\n",
            "heaven\n",
            "heavy\n",
            "held\n",
            "hell\n",
            "help\n",
            "helped\n",
            "hero\n",
            "heroine\n",
            "hey\n",
            "hidden\n",
            "hide\n",
            "high\n",
            "higher\n",
            "highlight\n",
            "highly\n",
            "hilarious\n",
            "hill\n",
            "historical\n",
            "history\n",
            "hit\n",
            "hold\n",
            "hole\n",
            "hollywood\n",
            "home\n",
            "honest\n",
            "honestly\n",
            "hope\n",
            "hoping\n",
            "horrible\n",
            "horror\n",
            "horse\n",
            "hospital\n",
            "hot\n",
            "hotel\n",
            "hour\n",
            "house\n",
            "however\n",
            "huge\n",
            "human\n",
            "humanity\n",
            "humor\n",
            "humour\n",
            "hundred\n",
            "hurt\n",
            "husband\n",
            "idea\n",
            "identity\n",
            "idiot\n",
            "ii\n",
            "ill\n",
            "image\n",
            "imagination\n",
            "imagine\n",
            "imdb\n",
            "immediately\n",
            "impact\n",
            "important\n",
            "impossible\n",
            "impressed\n",
            "impression\n",
            "impressive\n",
            "include\n",
            "included\n",
            "includes\n",
            "including\n",
            "incredible\n",
            "incredibly\n",
            "indeed\n",
            "independent\n",
            "indian\n",
            "individual\n",
            "industry\n",
            "information\n",
            "innocent\n",
            "inside\n",
            "inspired\n",
            "instance\n",
            "instead\n",
            "insult\n",
            "intelligence\n",
            "intelligent\n",
            "intended\n",
            "intense\n",
            "intention\n",
            "interest\n",
            "interested\n",
            "interesting\n",
            "interview\n",
            "intriguing\n",
            "introduced\n",
            "involved\n",
            "involving\n",
            "island\n",
            "issue\n",
            "italian\n",
            "j\n",
            "jack\n",
            "james\n",
            "jane\n",
            "japanese\n",
            "jason\n",
            "jean\n",
            "jim\n",
            "job\n",
            "joe\n",
            "john\n",
            "joke\n",
            "jones\n",
            "journey\n",
            "joy\n",
            "jr\n",
            "judge\n",
            "jump\n",
            "justice\n",
            "keep\n",
            "keeping\n",
            "kept\n",
            "key\n",
            "kick\n",
            "kid\n",
            "kill\n",
            "killed\n",
            "killer\n",
            "killing\n",
            "kind\n",
            "kinda\n",
            "king\n",
            "knew\n",
            "know\n",
            "knowing\n",
            "knowledge\n",
            "known\n",
            "l\n",
            "la\n",
            "lack\n",
            "lacking\n",
            "lady\n",
            "lame\n",
            "land\n",
            "language\n",
            "large\n",
            "last\n",
            "late\n",
            "later\n",
            "latter\n",
            "laugh\n",
            "laughable\n",
            "laughed\n",
            "laughing\n",
            "law\n",
            "le\n",
            "lead\n",
            "leader\n",
            "leading\n",
            "leaf\n",
            "learn\n",
            "least\n",
            "leave\n",
            "leaving\n",
            "led\n",
            "lee\n",
            "left\n",
            "legend\n",
            "length\n",
            "lesson\n",
            "let\n",
            "level\n",
            "lie\n",
            "life\n",
            "light\n",
            "lighting\n",
            "likable\n",
            "liked\n",
            "likely\n",
            "limited\n",
            "line\n",
            "list\n",
            "listen\n",
            "literally\n",
            "little\n",
            "live\n",
            "lived\n",
            "living\n",
            "local\n",
            "location\n",
            "london\n",
            "long\n",
            "longer\n",
            "look\n",
            "looked\n",
            "looking\n",
            "loose\n",
            "lord\n",
            "lose\n",
            "loss\n",
            "lost\n",
            "lot\n",
            "loud\n",
            "love\n",
            "loved\n",
            "lovely\n",
            "lover\n",
            "loving\n",
            "low\n",
            "lucky\n",
            "machine\n",
            "mad\n",
            "made\n",
            "magic\n",
            "main\n",
            "mainly\n",
            "major\n",
            "make\n",
            "maker\n",
            "making\n",
            "male\n",
            "man\n",
            "manage\n",
            "managed\n",
            "manages\n",
            "manner\n",
            "many\n",
            "mark\n",
            "marriage\n",
            "married\n",
            "martin\n",
            "mary\n",
            "master\n",
            "masterpiece\n",
            "match\n",
            "material\n",
            "matter\n",
            "may\n",
            "maybe\n",
            "mean\n",
            "meaning\n",
            "meant\n",
            "mediocre\n",
            "medium\n",
            "meet\n",
            "member\n",
            "memorable\n",
            "memory\n",
            "men\n",
            "mental\n",
            "mention\n",
            "mentioned\n",
            "merely\n",
            "mess\n",
            "message\n",
            "met\n",
            "michael\n",
            "mid\n",
            "middle\n",
            "might\n",
            "mile\n",
            "military\n",
            "million\n",
            "mind\n",
            "mine\n",
            "minor\n",
            "minute\n",
            "miss\n",
            "missed\n",
            "missing\n",
            "mission\n",
            "mistake\n",
            "mix\n",
            "mixed\n",
            "model\n",
            "modern\n",
            "mom\n",
            "moment\n",
            "money\n",
            "monster\n",
            "month\n",
            "mood\n",
            "moral\n",
            "mostly\n",
            "mother\n",
            "motion\n",
            "mouth\n",
            "move\n",
            "moved\n",
            "movement\n",
            "moving\n",
            "mr\n",
            "much\n",
            "murder\n",
            "music\n",
            "musical\n",
            "must\n",
            "mysterious\n",
            "mystery\n",
            "n\n",
            "na\n",
            "naked\n",
            "name\n",
            "named\n",
            "narrative\n",
            "nasty\n",
            "natural\n",
            "naturally\n",
            "nature\n",
            "near\n",
            "nearly\n",
            "necessary\n",
            "need\n",
            "needed\n",
            "negative\n",
            "neither\n",
            "never\n",
            "new\n",
            "news\n",
            "next\n",
            "nice\n",
            "nicely\n",
            "night\n",
            "nightmare\n",
            "nobody\n",
            "noir\n",
            "non\n",
            "none\n",
            "nonsense\n",
            "normal\n",
            "normally\n",
            "note\n",
            "nothing\n",
            "notice\n",
            "noticed\n",
            "novel\n",
            "nowhere\n",
            "nudity\n",
            "number\n",
            "numerous\n",
            "obvious\n",
            "obviously\n",
            "odd\n",
            "offer\n",
            "office\n",
            "officer\n",
            "often\n",
            "oh\n",
            "ok\n",
            "okay\n",
            "old\n",
            "older\n",
            "onto\n",
            "open\n",
            "opening\n",
            "opera\n",
            "opinion\n",
            "opportunity\n",
            "opposite\n",
            "order\n",
            "original\n",
            "originally\n",
            "oscar\n",
            "others\n",
            "otherwise\n",
            "outside\n",
            "outstanding\n",
            "overall\n",
            "owner\n",
            "p\n",
            "pace\n",
            "paced\n",
            "pacing\n",
            "page\n",
            "paid\n",
            "pain\n",
            "painful\n",
            "paper\n",
            "parent\n",
            "park\n",
            "part\n",
            "particular\n",
            "particularly\n",
            "partner\n",
            "party\n",
            "pas\n",
            "passion\n",
            "past\n",
            "pathetic\n",
            "paul\n",
            "pay\n",
            "people\n",
            "perfect\n",
            "perfectly\n",
            "performance\n",
            "perhaps\n",
            "period\n",
            "person\n",
            "personal\n",
            "personality\n",
            "personally\n",
            "perspective\n",
            "peter\n",
            "phone\n",
            "photography\n",
            "physical\n",
            "pick\n",
            "picked\n",
            "picture\n",
            "piece\n",
            "place\n",
            "plain\n",
            "plan\n",
            "plane\n",
            "planet\n",
            "play\n",
            "played\n",
            "player\n",
            "playing\n",
            "please\n",
            "pleasure\n",
            "plenty\n",
            "plot\n",
            "plus\n",
            "point\n",
            "pointless\n",
            "police\n",
            "political\n",
            "poor\n",
            "poorly\n",
            "pop\n",
            "popular\n",
            "porn\n",
            "portray\n",
            "portrayal\n",
            "portrayed\n",
            "positive\n",
            "possible\n",
            "possibly\n",
            "post\n",
            "potential\n",
            "power\n",
            "powerful\n",
            "pre\n",
            "predictable\n",
            "premise\n",
            "presence\n",
            "present\n",
            "presented\n",
            "pretty\n",
            "previous\n",
            "price\n",
            "prison\n",
            "probably\n",
            "problem\n",
            "process\n",
            "produce\n",
            "produced\n",
            "producer\n",
            "product\n",
            "production\n",
            "prof\n",
            "professional\n",
            "project\n",
            "promise\n",
            "protagonist\n",
            "prove\n",
            "provide\n",
            "provides\n",
            "public\n",
            "pull\n",
            "pulled\n",
            "pure\n",
            "purpose\n",
            "put\n",
            "putting\n",
            "quality\n",
            "queen\n",
            "question\n",
            "quick\n",
            "quickly\n",
            "quiet\n",
            "quite\n",
            "r\n",
            "race\n",
            "radio\n",
            "random\n",
            "range\n",
            "rape\n",
            "rare\n",
            "rarely\n",
            "rate\n",
            "rated\n",
            "rather\n",
            "rating\n",
            "ray\n",
            "reach\n",
            "reaction\n",
            "read\n",
            "reading\n",
            "ready\n",
            "real\n",
            "realism\n",
            "realistic\n",
            "reality\n",
            "realize\n",
            "realized\n",
            "really\n",
            "reason\n",
            "received\n",
            "recent\n",
            "recently\n",
            "recommend\n",
            "recommended\n",
            "record\n",
            "red\n",
            "redeeming\n",
            "reference\n",
            "regular\n",
            "relationship\n",
            "release\n",
            "released\n",
            "remains\n",
            "remake\n",
            "remarkable\n",
            "remember\n",
            "reminded\n",
            "reminds\n",
            "rent\n",
            "rented\n",
            "respect\n",
            "responsible\n",
            "rest\n",
            "result\n",
            "return\n",
            "revenge\n",
            "review\n",
            "reviewer\n",
            "rich\n",
            "richard\n",
            "ride\n",
            "ridiculous\n",
            "right\n",
            "ring\n",
            "rip\n",
            "rise\n",
            "river\n",
            "road\n",
            "robert\n",
            "rock\n",
            "role\n",
            "roll\n",
            "romance\n",
            "romantic\n",
            "room\n",
            "round\n",
            "rubbish\n",
            "ruin\n",
            "rule\n",
            "run\n",
            "running\n",
            "sad\n",
            "sadly\n",
            "said\n",
            "sam\n",
            "sat\n",
            "save\n",
            "saved\n",
            "saving\n",
            "saw\n",
            "say\n",
            "saying\n",
            "scare\n",
            "scared\n",
            "scary\n",
            "scene\n",
            "scenery\n",
            "school\n",
            "sci\n",
            "science\n",
            "scientist\n",
            "score\n",
            "scott\n",
            "scream\n",
            "screen\n",
            "screenplay\n",
            "script\n",
            "search\n",
            "season\n",
            "seat\n",
            "second\n",
            "secret\n",
            "see\n",
            "seeing\n",
            "seek\n",
            "seem\n",
            "seemed\n",
            "seemingly\n",
            "seems\n",
            "seen\n",
            "self\n",
            "sell\n",
            "sense\n",
            "sent\n",
            "sequel\n",
            "sequence\n",
            "serial\n",
            "series\n",
            "serious\n",
            "seriously\n",
            "set\n",
            "setting\n",
            "seven\n",
            "several\n",
            "sex\n",
            "sexual\n",
            "sexy\n",
            "shadow\n",
            "shame\n",
            "share\n",
            "ship\n",
            "shock\n",
            "shocking\n",
            "shoot\n",
            "shooting\n",
            "short\n",
            "shot\n",
            "show\n",
            "showed\n",
            "showing\n",
            "shown\n",
            "sick\n",
            "side\n",
            "sight\n",
            "sign\n",
            "silent\n",
            "silly\n",
            "similar\n",
            "simple\n",
            "simply\n",
            "since\n",
            "singer\n",
            "singing\n",
            "single\n",
            "sister\n",
            "sit\n",
            "site\n",
            "sitting\n",
            "situation\n",
            "six\n",
            "skill\n",
            "skip\n",
            "slasher\n",
            "sleep\n",
            "slightly\n",
            "slow\n",
            "slowly\n",
            "small\n",
            "smart\n",
            "smile\n",
            "smith\n",
            "social\n",
            "society\n",
            "soft\n",
            "soldier\n",
            "solid\n",
            "somebody\n",
            "somehow\n",
            "someone\n",
            "something\n",
            "sometimes\n",
            "somewhat\n",
            "somewhere\n",
            "son\n",
            "song\n",
            "soon\n",
            "sorry\n",
            "sort\n",
            "soul\n",
            "sound\n",
            "soundtrack\n",
            "south\n",
            "space\n",
            "speak\n",
            "speaking\n",
            "special\n",
            "spend\n",
            "spent\n",
            "spirit\n",
            "spoiler\n",
            "spot\n",
            "stage\n",
            "stand\n",
            "standard\n",
            "star\n",
            "starring\n",
            "start\n",
            "started\n",
            "starting\n",
            "state\n",
            "station\n",
            "stay\n",
            "steal\n",
            "step\n",
            "stephen\n",
            "stereotype\n",
            "steve\n",
            "stick\n",
            "still\n",
            "stone\n",
            "stop\n",
            "store\n",
            "story\n",
            "storyline\n",
            "straight\n",
            "strange\n",
            "street\n",
            "strength\n",
            "strong\n",
            "struggle\n",
            "stuck\n",
            "student\n",
            "studio\n",
            "study\n",
            "stuff\n",
            "stunning\n",
            "stupid\n",
            "style\n",
            "sub\n",
            "subject\n",
            "subtle\n",
            "success\n",
            "successful\n",
            "suck\n",
            "suddenly\n",
            "suggest\n",
            "suicide\n",
            "suit\n",
            "summer\n",
            "super\n",
            "superb\n",
            "superior\n",
            "support\n",
            "supporting\n",
            "suppose\n",
            "supposed\n",
            "supposedly\n",
            "sure\n",
            "surely\n",
            "surprise\n",
            "surprised\n",
            "surprising\n",
            "surprisingly\n",
            "suspect\n",
            "suspense\n",
            "sweet\n",
            "system\n",
            "take\n",
            "taken\n",
            "taking\n",
            "tale\n",
            "talent\n",
            "talented\n",
            "talk\n",
            "talking\n",
            "tape\n",
            "taste\n",
            "teacher\n",
            "team\n",
            "tear\n",
            "technical\n",
            "technique\n",
            "teen\n",
            "teenage\n",
            "teenager\n",
            "television\n",
            "tell\n",
            "telling\n",
            "ten\n",
            "tension\n",
            "term\n",
            "terrible\n",
            "terribly\n",
            "terrific\n",
            "thank\n",
            "thanks\n",
            "thats\n",
            "theater\n",
            "theatre\n",
            "theme\n",
            "therefore\n",
            "thin\n",
            "thing\n",
            "think\n",
            "thinking\n",
            "third\n",
            "thoroughly\n",
            "though\n",
            "thought\n",
            "thousand\n",
            "three\n",
            "thriller\n",
            "throughout\n",
            "throw\n",
            "thrown\n",
            "thus\n",
            "tired\n",
            "title\n",
            "today\n",
            "together\n",
            "told\n",
            "tom\n",
            "tone\n",
            "tony\n",
            "took\n",
            "top\n",
            "torture\n",
            "total\n",
            "totally\n",
            "touch\n",
            "touching\n",
            "tough\n",
            "toward\n",
            "towards\n",
            "town\n",
            "track\n",
            "tragedy\n",
            "tragic\n",
            "trailer\n",
            "train\n",
            "trash\n",
            "travel\n",
            "treat\n",
            "treated\n",
            "trick\n",
            "tried\n",
            "trip\n",
            "trouble\n",
            "true\n",
            "truly\n",
            "trust\n",
            "truth\n",
            "try\n",
            "trying\n",
            "turn\n",
            "turned\n",
            "turning\n",
            "tv\n",
            "twenty\n",
            "twice\n",
            "twist\n",
            "two\n",
            "type\n",
            "typical\n",
            "u\n",
            "ugly\n",
            "ultimately\n",
            "unbelievable\n",
            "understand\n",
            "understanding\n",
            "unfortunately\n",
            "unique\n",
            "unknown\n",
            "unless\n",
            "unlike\n",
            "unnecessary\n",
            "unusual\n",
            "upon\n",
            "ups\n",
            "us\n",
            "use\n",
            "used\n",
            "using\n",
            "usual\n",
            "usually\n",
            "utterly\n",
            "v\n",
            "value\n",
            "vampire\n",
            "van\n",
            "various\n",
            "vehicle\n",
            "version\n",
            "vhs\n",
            "victim\n",
            "video\n",
            "view\n",
            "viewer\n",
            "viewing\n",
            "villain\n",
            "violence\n",
            "violent\n",
            "vision\n",
            "visit\n",
            "visual\n",
            "voice\n",
            "vote\n",
            "wait\n",
            "waiting\n",
            "walk\n",
            "walking\n",
            "wall\n",
            "want\n",
            "wanted\n",
            "wanting\n",
            "war\n",
            "warning\n",
            "waste\n",
            "wasted\n",
            "watch\n",
            "watchable\n",
            "watched\n",
            "watching\n",
            "water\n",
            "way\n",
            "weak\n",
            "weapon\n",
            "wear\n",
            "wearing\n",
            "week\n",
            "weird\n",
            "well\n",
            "went\n",
            "west\n",
            "western\n",
            "whatever\n",
            "whatsoever\n",
            "whenever\n",
            "whether\n",
            "white\n",
            "whole\n",
            "whose\n",
            "wide\n",
            "wife\n",
            "wild\n",
            "william\n",
            "willing\n",
            "win\n",
            "wind\n",
            "window\n",
            "winning\n",
            "wise\n",
            "wish\n",
            "wit\n",
            "witch\n",
            "within\n",
            "without\n",
            "witness\n",
            "witty\n",
            "woman\n",
            "wonder\n",
            "wonderful\n",
            "wonderfully\n",
            "wondering\n",
            "wood\n",
            "wooden\n",
            "word\n",
            "work\n",
            "worked\n",
            "working\n",
            "world\n",
            "worse\n",
            "worst\n",
            "worth\n",
            "worthy\n",
            "would\n",
            "wow\n",
            "write\n",
            "writer\n",
            "writing\n",
            "written\n",
            "wrong\n",
            "wrote\n",
            "x\n",
            "yeah\n",
            "year\n",
            "yes\n",
            "yet\n",
            "york\n",
            "young\n",
            "younger\n",
            "youth\n",
            "zero\n",
            "zombie\n"
          ]
        }
      ],
      "source": [
        "for d in vectorizer.get_feature_names_out():\n",
        "  print(d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDaFEJ2jtFrZ"
      },
      "source": [
        "#### Without priors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6l5rqEsftFrZ",
        "outputId": "e561cb33-3d11-4eda-f708-9c411bb73e8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.844\n"
          ]
        }
      ],
      "source": [
        "model=MultinomialNaiveBayes()\n",
        "params=model.fit(X_train, y_train, 0,0,0,0)\n",
        "y_prob=model.predict(X_test)\n",
        "y_pred = np.argmax(y_prob, 1)\n",
        "print(\"Accuracy:\",model.evaluate_acc(y_pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QthJshSrtFra"
      },
      "source": [
        "#### With priors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXlLYB7FtFra",
        "outputId": "852f944e-4d75-4ea2-e9f1-4444886f521c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.844\n"
          ]
        }
      ],
      "source": [
        "model=MultinomialNaiveBayes()\n",
        "params=model.fit(X_train, y_train, 1,1,10,10)\n",
        "y_prob=model.predict(X_test)\n",
        "y_pred = np.argmax(y_prob, 1)\n",
        "print(\"Accuracy:\",model.evaluate_acc(y_pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis with different limits"
      ],
      "metadata": {
        "id": "Pgu2aisxvoDb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lower_limits=[0, 0.005, 0.01, 0.015]\n",
        "upper_limits=[0.35 , 0.4, 0.45, 0.5, 0.55, 0.6, 0.65]\n",
        "for l in lower_limits:\n",
        "  for u in upper_limits:\n",
        "    vectorizer = CountVectorizer(stop_words=stopword_list, tokenizer=LemmaTokenizer(), min_df=l, max_df=u)\n",
        "    X_train = vectorizer.fit_transform(x_train)\n",
        "    dic= vectorizer.get_feature_names_out()\n",
        "    X_test=vectorizer.transform(x_test)\n",
        "    model=MultinomialNaiveBayes()\n",
        "    params=model.fit(X_train, y_train, 1,1,10, 10)\n",
        "    y_prob=model.predict(X_test)\n",
        "    y_pred = np.argmax(y_prob, 1)\n",
        "    print(\"Accuracy for lower limit \"+str(l)+ \" and higher limit \"+ str(u)+\":\",model.evaluate_acc(y_pred, y_test),\"; Length of dictionary: \", len(dic))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVkp17eIvrWD",
        "outputId": "c60d71ee-c614-4480-f97b-e04e0de05a5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for lower limit 0 and higher limit 0.35: 0.80136 ; Length of dictionary:  67893\n",
            "Accuracy for lower limit 0 and higher limit 0.4: 0.8014 ; Length of dictionary:  67894\n",
            "Accuracy for lower limit 0 and higher limit 0.45: 0.80136 ; Length of dictionary:  67896\n",
            "Accuracy for lower limit 0 and higher limit 0.5: 0.80032 ; Length of dictionary:  67897\n",
            "Accuracy for lower limit 0 and higher limit 0.55: 0.80032 ; Length of dictionary:  67897\n",
            "Accuracy for lower limit 0 and higher limit 0.6: 0.80044 ; Length of dictionary:  67899\n",
            "Accuracy for lower limit 0 and higher limit 0.65: 0.79732 ; Length of dictionary:  67901\n",
            "Accuracy for lower limit 0.005 and higher limit 0.35: 0.839 ; Length of dictionary:  2957\n",
            "Accuracy for lower limit 0.005 and higher limit 0.4: 0.83872 ; Length of dictionary:  2958\n",
            "Accuracy for lower limit 0.005 and higher limit 0.45: 0.83912 ; Length of dictionary:  2960\n",
            "Accuracy for lower limit 0.005 and higher limit 0.5: 0.83856 ; Length of dictionary:  2961\n",
            "Accuracy for lower limit 0.005 and higher limit 0.55: 0.83856 ; Length of dictionary:  2961\n",
            "Accuracy for lower limit 0.005 and higher limit 0.6: 0.83468 ; Length of dictionary:  2963\n",
            "Accuracy for lower limit 0.005 and higher limit 0.65: 0.83132 ; Length of dictionary:  2965\n",
            "Accuracy for lower limit 0.01 and higher limit 0.35: 0.844 ; Length of dictionary:  1666\n",
            "Accuracy for lower limit 0.01 and higher limit 0.4: 0.8436 ; Length of dictionary:  1667\n",
            "Accuracy for lower limit 0.01 and higher limit 0.45: 0.84328 ; Length of dictionary:  1669\n",
            "Accuracy for lower limit 0.01 and higher limit 0.5: 0.84296 ; Length of dictionary:  1670\n",
            "Accuracy for lower limit 0.01 and higher limit 0.55: 0.84296 ; Length of dictionary:  1670\n",
            "Accuracy for lower limit 0.01 and higher limit 0.6: 0.84248 ; Length of dictionary:  1672\n",
            "Accuracy for lower limit 0.01 and higher limit 0.65: 0.83652 ; Length of dictionary:  1674\n",
            "Accuracy for lower limit 0.015 and higher limit 0.35: 0.84156 ; Length of dictionary:  1165\n",
            "Accuracy for lower limit 0.015 and higher limit 0.4: 0.84176 ; Length of dictionary:  1166\n",
            "Accuracy for lower limit 0.015 and higher limit 0.45: 0.84192 ; Length of dictionary:  1168\n",
            "Accuracy for lower limit 0.015 and higher limit 0.5: 0.84164 ; Length of dictionary:  1169\n",
            "Accuracy for lower limit 0.015 and higher limit 0.55: 0.84164 ; Length of dictionary:  1169\n",
            "Accuracy for lower limit 0.015 and higher limit 0.6: 0.84056 ; Length of dictionary:  1171\n",
            "Accuracy for lower limit 0.015 and higher limit 0.65: 0.8376 ; Length of dictionary:  1173\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking for limits beyond the optimal obtained above (lower=0.01; upper=0.35)"
      ],
      "metadata": {
        "id": "8379N8O8gn5n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7L9GQVvOgk8I"
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer(stop_words=stopword_list, tokenizer=LemmaTokenizer(), min_df=0.01, max_df=0.3)\n",
        "X_train = vectorizer.fit_transform(x_train)\n",
        "dic= vectorizer.get_feature_names_out()\n",
        "X_test=vectorizer.transform(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2651ab7f-0a01-412e-c018-95ce6a91ee15",
        "id": "cZaVdpAygk8J"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dictionary length: 1657\n"
          ]
        }
      ],
      "source": [
        "print(\"Dictionary length:\", len(vectorizer.get_feature_names_out()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66e21004-03f7-402e-c1ae-6e1f4b44e1e0",
        "id": "S8h5Xb4Hgk8L"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "10\n",
            "100\n",
            "11\n",
            "12\n",
            "15\n",
            "2\n",
            "20\n",
            "3\n",
            "30\n",
            "4\n",
            "40\n",
            "5\n",
            "50\n",
            "6\n",
            "60\n",
            "7\n",
            "70\n",
            "8\n",
            "80\n",
            "9\n",
            "90\n",
            "ability\n",
            "able\n",
            "absolute\n",
            "absolutely\n",
            "absurd\n",
            "academy\n",
            "accent\n",
            "accept\n",
            "accident\n",
            "according\n",
            "accurate\n",
            "across\n",
            "act\n",
            "acted\n",
            "acting\n",
            "action\n",
            "actor\n",
            "actress\n",
            "actual\n",
            "actually\n",
            "adam\n",
            "adaptation\n",
            "add\n",
            "added\n",
            "addition\n",
            "admit\n",
            "adult\n",
            "adventure\n",
            "affair\n",
            "afraid\n",
            "age\n",
            "agent\n",
            "ago\n",
            "agree\n",
            "ahead\n",
            "air\n",
            "al\n",
            "alien\n",
            "alive\n",
            "allow\n",
            "allowed\n",
            "almost\n",
            "alone\n",
            "along\n",
            "already\n",
            "also\n",
            "although\n",
            "always\n",
            "amazing\n",
            "america\n",
            "american\n",
            "among\n",
            "amount\n",
            "amusing\n",
            "angel\n",
            "angle\n",
            "angry\n",
            "animal\n",
            "animated\n",
            "animation\n",
            "annoying\n",
            "another\n",
            "answer\n",
            "anti\n",
            "anybody\n",
            "anymore\n",
            "anyone\n",
            "anything\n",
            "anyway\n",
            "anywhere\n",
            "apart\n",
            "apartment\n",
            "apparent\n",
            "apparently\n",
            "appeal\n",
            "appear\n",
            "appearance\n",
            "appeared\n",
            "appears\n",
            "appreciate\n",
            "approach\n",
            "area\n",
            "arm\n",
            "army\n",
            "around\n",
            "art\n",
            "artist\n",
            "artistic\n",
            "aside\n",
            "ask\n",
            "asked\n",
            "asks\n",
            "aspect\n",
            "atmosphere\n",
            "attack\n",
            "attempt\n",
            "attention\n",
            "attitude\n",
            "attractive\n",
            "audience\n",
            "author\n",
            "available\n",
            "average\n",
            "avoid\n",
            "award\n",
            "aware\n",
            "away\n",
            "awesome\n",
            "awful\n",
            "b\n",
            "baby\n",
            "back\n",
            "background\n",
            "bad\n",
            "badly\n",
            "ball\n",
            "band\n",
            "bar\n",
            "barely\n",
            "based\n",
            "basic\n",
            "basically\n",
            "battle\n",
            "bear\n",
            "beat\n",
            "beautiful\n",
            "beautifully\n",
            "beauty\n",
            "became\n",
            "become\n",
            "becomes\n",
            "becoming\n",
            "bed\n",
            "began\n",
            "begin\n",
            "beginning\n",
            "behavior\n",
            "behind\n",
            "belief\n",
            "believable\n",
            "believe\n",
            "ben\n",
            "besides\n",
            "best\n",
            "better\n",
            "beyond\n",
            "big\n",
            "biggest\n",
            "bill\n",
            "billy\n",
            "bit\n",
            "bizarre\n",
            "black\n",
            "blame\n",
            "bland\n",
            "blood\n",
            "bloody\n",
            "blow\n",
            "blue\n",
            "board\n",
            "body\n",
            "bond\n",
            "book\n",
            "bored\n",
            "boring\n",
            "born\n",
            "bos\n",
            "bother\n",
            "bottom\n",
            "bought\n",
            "box\n",
            "boy\n",
            "boyfriend\n",
            "brain\n",
            "break\n",
            "brian\n",
            "brief\n",
            "bright\n",
            "brilliant\n",
            "bring\n",
            "brings\n",
            "british\n",
            "brother\n",
            "brought\n",
            "brutal\n",
            "buck\n",
            "buddy\n",
            "budget\n",
            "build\n",
            "building\n",
            "bunch\n",
            "business\n",
            "buy\n",
            "c\n",
            "cable\n",
            "call\n",
            "called\n",
            "came\n",
            "cameo\n",
            "camera\n",
            "camp\n",
            "capture\n",
            "car\n",
            "care\n",
            "career\n",
            "carry\n",
            "cartoon\n",
            "case\n",
            "cast\n",
            "casting\n",
            "cat\n",
            "catch\n",
            "caught\n",
            "cause\n",
            "center\n",
            "central\n",
            "century\n",
            "certain\n",
            "certainly\n",
            "cgi\n",
            "chance\n",
            "change\n",
            "changed\n",
            "channel\n",
            "charles\n",
            "charm\n",
            "charming\n",
            "chase\n",
            "cheap\n",
            "check\n",
            "cheesy\n",
            "chemistry\n",
            "chick\n",
            "child\n",
            "childhood\n",
            "choice\n",
            "chris\n",
            "christian\n",
            "christmas\n",
            "christopher\n",
            "church\n",
            "cinema\n",
            "cinematic\n",
            "cinematography\n",
            "city\n",
            "claim\n",
            "class\n",
            "classic\n",
            "clear\n",
            "clearly\n",
            "clever\n",
            "clich\n",
            "climax\n",
            "close\n",
            "clothes\n",
            "club\n",
            "clue\n",
            "co\n",
            "cold\n",
            "collection\n",
            "college\n",
            "color\n",
            "come\n",
            "comedic\n",
            "comedy\n",
            "comic\n",
            "coming\n",
            "comment\n",
            "commentary\n",
            "commercial\n",
            "common\n",
            "community\n",
            "company\n",
            "compare\n",
            "compared\n",
            "comparison\n",
            "compelling\n",
            "complete\n",
            "completely\n",
            "complex\n",
            "computer\n",
            "concept\n",
            "concerned\n",
            "conclusion\n",
            "conflict\n",
            "confused\n",
            "confusing\n",
            "connection\n",
            "consider\n",
            "considered\n",
            "considering\n",
            "constant\n",
            "constantly\n",
            "contains\n",
            "content\n",
            "continue\n",
            "continues\n",
            "control\n",
            "conversation\n",
            "convincing\n",
            "cool\n",
            "cop\n",
            "copy\n",
            "cost\n",
            "costume\n",
            "could\n",
            "count\n",
            "country\n",
            "couple\n",
            "course\n",
            "cover\n",
            "crap\n",
            "crazy\n",
            "create\n",
            "created\n",
            "creating\n",
            "creative\n",
            "creature\n",
            "credit\n",
            "creepy\n",
            "crew\n",
            "crime\n",
            "criminal\n",
            "critic\n",
            "cross\n",
            "cry\n",
            "cult\n",
            "culture\n",
            "current\n",
            "cut\n",
            "cute\n",
            "dad\n",
            "damn\n",
            "dance\n",
            "dancing\n",
            "dangerous\n",
            "dark\n",
            "date\n",
            "daughter\n",
            "david\n",
            "day\n",
            "de\n",
            "dead\n",
            "deal\n",
            "dealing\n",
            "death\n",
            "decade\n",
            "decent\n",
            "decide\n",
            "decided\n",
            "decides\n",
            "decision\n",
            "deep\n",
            "deeply\n",
            "definitely\n",
            "delightful\n",
            "deliver\n",
            "delivers\n",
            "depth\n",
            "describe\n",
            "deserve\n",
            "deserved\n",
            "deserves\n",
            "design\n",
            "desire\n",
            "desperate\n",
            "despite\n",
            "detail\n",
            "detective\n",
            "developed\n",
            "development\n",
            "devil\n",
            "dialog\n",
            "dialogue\n",
            "die\n",
            "died\n",
            "difference\n",
            "different\n",
            "difficult\n",
            "direct\n",
            "directed\n",
            "directing\n",
            "direction\n",
            "director\n",
            "dirty\n",
            "disappointed\n",
            "disappointing\n",
            "disappointment\n",
            "disaster\n",
            "discover\n",
            "discovered\n",
            "disney\n",
            "display\n",
            "disturbing\n",
            "doctor\n",
            "documentary\n",
            "doe\n",
            "dog\n",
            "dollar\n",
            "done\n",
            "door\n",
            "double\n",
            "doubt\n",
            "dozen\n",
            "dr\n",
            "drag\n",
            "drama\n",
            "dramatic\n",
            "draw\n",
            "drawn\n",
            "dream\n",
            "drive\n",
            "drop\n",
            "drug\n",
            "drunk\n",
            "due\n",
            "dull\n",
            "dumb\n",
            "dvd\n",
            "dy\n",
            "dying\n",
            "e\n",
            "earlier\n",
            "early\n",
            "earth\n",
            "easily\n",
            "easy\n",
            "ed\n",
            "edge\n",
            "editing\n",
            "effect\n",
            "effective\n",
            "effort\n",
            "either\n",
            "element\n",
            "else\n",
            "emotion\n",
            "emotional\n",
            "empty\n",
            "encounter\n",
            "end\n",
            "ended\n",
            "ending\n",
            "enemy\n",
            "energy\n",
            "engaging\n",
            "england\n",
            "english\n",
            "enjoy\n",
            "enjoyable\n",
            "enjoyed\n",
            "enough\n",
            "entertaining\n",
            "entertainment\n",
            "entire\n",
            "entirely\n",
            "epic\n",
            "episode\n",
            "equally\n",
            "era\n",
            "escape\n",
            "especially\n",
            "essentially\n",
            "etc\n",
            "european\n",
            "event\n",
            "eventually\n",
            "ever\n",
            "every\n",
            "everybody\n",
            "everyone\n",
            "everything\n",
            "evil\n",
            "ex\n",
            "exactly\n",
            "example\n",
            "excellent\n",
            "except\n",
            "exception\n",
            "exciting\n",
            "excuse\n",
            "exist\n",
            "existence\n",
            "expect\n",
            "expectation\n",
            "expected\n",
            "expecting\n",
            "experience\n",
            "explain\n",
            "explained\n",
            "explanation\n",
            "expression\n",
            "extra\n",
            "extreme\n",
            "extremely\n",
            "eye\n",
            "f\n",
            "face\n",
            "fact\n",
            "factor\n",
            "fail\n",
            "failed\n",
            "fails\n",
            "failure\n",
            "fair\n",
            "fairly\n",
            "fake\n",
            "fall\n",
            "falling\n",
            "familiar\n",
            "family\n",
            "famous\n",
            "fan\n",
            "fantastic\n",
            "fantasy\n",
            "far\n",
            "fascinating\n",
            "fashion\n",
            "fast\n",
            "fate\n",
            "father\n",
            "fault\n",
            "favor\n",
            "favorite\n",
            "favourite\n",
            "fear\n",
            "feature\n",
            "featuring\n",
            "feel\n",
            "feeling\n",
            "fell\n",
            "fellow\n",
            "felt\n",
            "female\n",
            "festival\n",
            "fi\n",
            "fiction\n",
            "field\n",
            "fight\n",
            "fighting\n",
            "figure\n",
            "fill\n",
            "filled\n",
            "filmed\n",
            "filming\n",
            "filmmaker\n",
            "final\n",
            "finale\n",
            "finally\n",
            "find\n",
            "finding\n",
            "fine\n",
            "finest\n",
            "finish\n",
            "finished\n",
            "fire\n",
            "first\n",
            "fit\n",
            "five\n",
            "flashback\n",
            "flat\n",
            "flaw\n",
            "flick\n",
            "floor\n",
            "fly\n",
            "flying\n",
            "focus\n",
            "folk\n",
            "follow\n",
            "followed\n",
            "following\n",
            "follows\n",
            "food\n",
            "foot\n",
            "footage\n",
            "force\n",
            "forced\n",
            "forever\n",
            "forget\n",
            "forgotten\n",
            "form\n",
            "former\n",
            "forward\n",
            "found\n",
            "four\n",
            "frame\n",
            "frank\n",
            "free\n",
            "french\n",
            "fresh\n",
            "friend\n",
            "friendship\n",
            "front\n",
            "full\n",
            "fully\n",
            "fun\n",
            "funniest\n",
            "funny\n",
            "future\n",
            "g\n",
            "gag\n",
            "game\n",
            "gang\n",
            "gangster\n",
            "garbage\n",
            "gave\n",
            "gay\n",
            "gem\n",
            "general\n",
            "generally\n",
            "generation\n",
            "genius\n",
            "genre\n",
            "george\n",
            "german\n",
            "getting\n",
            "ghost\n",
            "giant\n",
            "girl\n",
            "girlfriend\n",
            "give\n",
            "given\n",
            "giving\n",
            "glad\n",
            "go\n",
            "god\n",
            "going\n",
            "gone\n",
            "gore\n",
            "gorgeous\n",
            "got\n",
            "gotten\n",
            "government\n",
            "grace\n",
            "grade\n",
            "grand\n",
            "graphic\n",
            "great\n",
            "greatest\n",
            "green\n",
            "ground\n",
            "group\n",
            "growing\n",
            "guess\n",
            "gun\n",
            "guy\n",
            "hair\n",
            "half\n",
            "hand\n",
            "happen\n",
            "happened\n",
            "happening\n",
            "happens\n",
            "happy\n",
            "hard\n",
            "hardly\n",
            "hate\n",
            "hated\n",
            "head\n",
            "hear\n",
            "heard\n",
            "heart\n",
            "heaven\n",
            "heavy\n",
            "held\n",
            "hell\n",
            "help\n",
            "helped\n",
            "hero\n",
            "heroine\n",
            "hey\n",
            "hidden\n",
            "hide\n",
            "high\n",
            "higher\n",
            "highlight\n",
            "highly\n",
            "hilarious\n",
            "hill\n",
            "historical\n",
            "history\n",
            "hit\n",
            "hold\n",
            "hole\n",
            "hollywood\n",
            "home\n",
            "honest\n",
            "honestly\n",
            "hope\n",
            "hoping\n",
            "horrible\n",
            "horror\n",
            "horse\n",
            "hospital\n",
            "hot\n",
            "hotel\n",
            "hour\n",
            "house\n",
            "however\n",
            "huge\n",
            "human\n",
            "humanity\n",
            "humor\n",
            "humour\n",
            "hundred\n",
            "hurt\n",
            "husband\n",
            "idea\n",
            "identity\n",
            "idiot\n",
            "ii\n",
            "ill\n",
            "image\n",
            "imagination\n",
            "imagine\n",
            "imdb\n",
            "immediately\n",
            "impact\n",
            "important\n",
            "impossible\n",
            "impressed\n",
            "impression\n",
            "impressive\n",
            "include\n",
            "included\n",
            "includes\n",
            "including\n",
            "incredible\n",
            "incredibly\n",
            "indeed\n",
            "independent\n",
            "indian\n",
            "individual\n",
            "industry\n",
            "information\n",
            "innocent\n",
            "inside\n",
            "inspired\n",
            "instance\n",
            "instead\n",
            "insult\n",
            "intelligence\n",
            "intelligent\n",
            "intended\n",
            "intense\n",
            "intention\n",
            "interest\n",
            "interested\n",
            "interesting\n",
            "interview\n",
            "intriguing\n",
            "introduced\n",
            "involved\n",
            "involving\n",
            "island\n",
            "issue\n",
            "italian\n",
            "j\n",
            "jack\n",
            "james\n",
            "jane\n",
            "japanese\n",
            "jason\n",
            "jean\n",
            "jim\n",
            "job\n",
            "joe\n",
            "john\n",
            "joke\n",
            "jones\n",
            "journey\n",
            "joy\n",
            "jr\n",
            "judge\n",
            "jump\n",
            "justice\n",
            "keep\n",
            "keeping\n",
            "kept\n",
            "key\n",
            "kick\n",
            "kid\n",
            "kill\n",
            "killed\n",
            "killer\n",
            "killing\n",
            "kind\n",
            "kinda\n",
            "king\n",
            "knew\n",
            "know\n",
            "knowing\n",
            "knowledge\n",
            "known\n",
            "l\n",
            "la\n",
            "lack\n",
            "lacking\n",
            "lady\n",
            "lame\n",
            "land\n",
            "language\n",
            "large\n",
            "last\n",
            "late\n",
            "later\n",
            "latter\n",
            "laugh\n",
            "laughable\n",
            "laughed\n",
            "laughing\n",
            "law\n",
            "le\n",
            "lead\n",
            "leader\n",
            "leading\n",
            "leaf\n",
            "learn\n",
            "least\n",
            "leave\n",
            "leaving\n",
            "led\n",
            "lee\n",
            "left\n",
            "legend\n",
            "length\n",
            "lesson\n",
            "let\n",
            "level\n",
            "lie\n",
            "life\n",
            "light\n",
            "lighting\n",
            "likable\n",
            "liked\n",
            "likely\n",
            "limited\n",
            "line\n",
            "list\n",
            "listen\n",
            "literally\n",
            "little\n",
            "live\n",
            "lived\n",
            "living\n",
            "local\n",
            "location\n",
            "london\n",
            "long\n",
            "longer\n",
            "look\n",
            "looked\n",
            "looking\n",
            "loose\n",
            "lord\n",
            "lose\n",
            "loss\n",
            "lost\n",
            "lot\n",
            "loud\n",
            "love\n",
            "loved\n",
            "lovely\n",
            "lover\n",
            "loving\n",
            "low\n",
            "lucky\n",
            "machine\n",
            "mad\n",
            "made\n",
            "magic\n",
            "main\n",
            "mainly\n",
            "major\n",
            "maker\n",
            "making\n",
            "male\n",
            "man\n",
            "manage\n",
            "managed\n",
            "manages\n",
            "manner\n",
            "many\n",
            "mark\n",
            "marriage\n",
            "married\n",
            "martin\n",
            "mary\n",
            "master\n",
            "masterpiece\n",
            "match\n",
            "material\n",
            "matter\n",
            "may\n",
            "maybe\n",
            "mean\n",
            "meaning\n",
            "meant\n",
            "mediocre\n",
            "medium\n",
            "meet\n",
            "member\n",
            "memorable\n",
            "memory\n",
            "men\n",
            "mental\n",
            "mention\n",
            "mentioned\n",
            "merely\n",
            "mess\n",
            "message\n",
            "met\n",
            "michael\n",
            "mid\n",
            "middle\n",
            "might\n",
            "mile\n",
            "military\n",
            "million\n",
            "mind\n",
            "mine\n",
            "minor\n",
            "minute\n",
            "miss\n",
            "missed\n",
            "missing\n",
            "mission\n",
            "mistake\n",
            "mix\n",
            "mixed\n",
            "model\n",
            "modern\n",
            "mom\n",
            "moment\n",
            "money\n",
            "monster\n",
            "month\n",
            "mood\n",
            "moral\n",
            "mostly\n",
            "mother\n",
            "motion\n",
            "mouth\n",
            "move\n",
            "moved\n",
            "movement\n",
            "moving\n",
            "mr\n",
            "much\n",
            "murder\n",
            "music\n",
            "musical\n",
            "must\n",
            "mysterious\n",
            "mystery\n",
            "n\n",
            "na\n",
            "naked\n",
            "name\n",
            "named\n",
            "narrative\n",
            "nasty\n",
            "natural\n",
            "naturally\n",
            "nature\n",
            "near\n",
            "nearly\n",
            "necessary\n",
            "need\n",
            "needed\n",
            "negative\n",
            "neither\n",
            "never\n",
            "new\n",
            "news\n",
            "next\n",
            "nice\n",
            "nicely\n",
            "night\n",
            "nightmare\n",
            "nobody\n",
            "noir\n",
            "non\n",
            "none\n",
            "nonsense\n",
            "normal\n",
            "normally\n",
            "note\n",
            "nothing\n",
            "notice\n",
            "noticed\n",
            "novel\n",
            "nowhere\n",
            "nudity\n",
            "number\n",
            "numerous\n",
            "obvious\n",
            "obviously\n",
            "odd\n",
            "offer\n",
            "office\n",
            "officer\n",
            "often\n",
            "oh\n",
            "ok\n",
            "okay\n",
            "old\n",
            "older\n",
            "onto\n",
            "open\n",
            "opening\n",
            "opera\n",
            "opinion\n",
            "opportunity\n",
            "opposite\n",
            "order\n",
            "original\n",
            "originally\n",
            "oscar\n",
            "others\n",
            "otherwise\n",
            "outside\n",
            "outstanding\n",
            "overall\n",
            "owner\n",
            "p\n",
            "pace\n",
            "paced\n",
            "pacing\n",
            "page\n",
            "paid\n",
            "pain\n",
            "painful\n",
            "paper\n",
            "parent\n",
            "park\n",
            "part\n",
            "particular\n",
            "particularly\n",
            "partner\n",
            "party\n",
            "pas\n",
            "passion\n",
            "past\n",
            "pathetic\n",
            "paul\n",
            "pay\n",
            "people\n",
            "perfect\n",
            "perfectly\n",
            "performance\n",
            "perhaps\n",
            "period\n",
            "person\n",
            "personal\n",
            "personality\n",
            "personally\n",
            "perspective\n",
            "peter\n",
            "phone\n",
            "photography\n",
            "physical\n",
            "pick\n",
            "picked\n",
            "picture\n",
            "piece\n",
            "place\n",
            "plain\n",
            "plan\n",
            "plane\n",
            "planet\n",
            "play\n",
            "played\n",
            "player\n",
            "playing\n",
            "please\n",
            "pleasure\n",
            "plenty\n",
            "plot\n",
            "plus\n",
            "point\n",
            "pointless\n",
            "police\n",
            "political\n",
            "poor\n",
            "poorly\n",
            "pop\n",
            "popular\n",
            "porn\n",
            "portray\n",
            "portrayal\n",
            "portrayed\n",
            "positive\n",
            "possible\n",
            "possibly\n",
            "post\n",
            "potential\n",
            "power\n",
            "powerful\n",
            "pre\n",
            "predictable\n",
            "premise\n",
            "presence\n",
            "present\n",
            "presented\n",
            "pretty\n",
            "previous\n",
            "price\n",
            "prison\n",
            "probably\n",
            "problem\n",
            "process\n",
            "produce\n",
            "produced\n",
            "producer\n",
            "product\n",
            "production\n",
            "prof\n",
            "professional\n",
            "project\n",
            "promise\n",
            "protagonist\n",
            "prove\n",
            "provide\n",
            "provides\n",
            "public\n",
            "pull\n",
            "pulled\n",
            "pure\n",
            "purpose\n",
            "put\n",
            "putting\n",
            "quality\n",
            "queen\n",
            "question\n",
            "quick\n",
            "quickly\n",
            "quiet\n",
            "quite\n",
            "r\n",
            "race\n",
            "radio\n",
            "random\n",
            "range\n",
            "rape\n",
            "rare\n",
            "rarely\n",
            "rate\n",
            "rated\n",
            "rather\n",
            "rating\n",
            "ray\n",
            "reach\n",
            "reaction\n",
            "read\n",
            "reading\n",
            "ready\n",
            "real\n",
            "realism\n",
            "realistic\n",
            "reality\n",
            "realize\n",
            "realized\n",
            "reason\n",
            "received\n",
            "recent\n",
            "recently\n",
            "recommend\n",
            "recommended\n",
            "record\n",
            "red\n",
            "redeeming\n",
            "reference\n",
            "regular\n",
            "relationship\n",
            "release\n",
            "released\n",
            "remains\n",
            "remake\n",
            "remarkable\n",
            "remember\n",
            "reminded\n",
            "reminds\n",
            "rent\n",
            "rented\n",
            "respect\n",
            "responsible\n",
            "rest\n",
            "result\n",
            "return\n",
            "revenge\n",
            "review\n",
            "reviewer\n",
            "rich\n",
            "richard\n",
            "ride\n",
            "ridiculous\n",
            "right\n",
            "ring\n",
            "rip\n",
            "rise\n",
            "river\n",
            "road\n",
            "robert\n",
            "rock\n",
            "role\n",
            "roll\n",
            "romance\n",
            "romantic\n",
            "room\n",
            "round\n",
            "rubbish\n",
            "ruin\n",
            "rule\n",
            "run\n",
            "running\n",
            "sad\n",
            "sadly\n",
            "said\n",
            "sam\n",
            "sat\n",
            "save\n",
            "saved\n",
            "saving\n",
            "saw\n",
            "say\n",
            "saying\n",
            "scare\n",
            "scared\n",
            "scary\n",
            "scene\n",
            "scenery\n",
            "school\n",
            "sci\n",
            "science\n",
            "scientist\n",
            "score\n",
            "scott\n",
            "scream\n",
            "screen\n",
            "screenplay\n",
            "script\n",
            "search\n",
            "season\n",
            "seat\n",
            "second\n",
            "secret\n",
            "seeing\n",
            "seek\n",
            "seem\n",
            "seemed\n",
            "seemingly\n",
            "seems\n",
            "seen\n",
            "self\n",
            "sell\n",
            "sense\n",
            "sent\n",
            "sequel\n",
            "sequence\n",
            "serial\n",
            "series\n",
            "serious\n",
            "seriously\n",
            "set\n",
            "setting\n",
            "seven\n",
            "several\n",
            "sex\n",
            "sexual\n",
            "sexy\n",
            "shadow\n",
            "shame\n",
            "share\n",
            "ship\n",
            "shock\n",
            "shocking\n",
            "shoot\n",
            "shooting\n",
            "short\n",
            "shot\n",
            "show\n",
            "showed\n",
            "showing\n",
            "shown\n",
            "sick\n",
            "side\n",
            "sight\n",
            "sign\n",
            "silent\n",
            "silly\n",
            "similar\n",
            "simple\n",
            "simply\n",
            "since\n",
            "singer\n",
            "singing\n",
            "single\n",
            "sister\n",
            "sit\n",
            "site\n",
            "sitting\n",
            "situation\n",
            "six\n",
            "skill\n",
            "skip\n",
            "slasher\n",
            "sleep\n",
            "slightly\n",
            "slow\n",
            "slowly\n",
            "small\n",
            "smart\n",
            "smile\n",
            "smith\n",
            "social\n",
            "society\n",
            "soft\n",
            "soldier\n",
            "solid\n",
            "somebody\n",
            "somehow\n",
            "someone\n",
            "something\n",
            "sometimes\n",
            "somewhat\n",
            "somewhere\n",
            "son\n",
            "song\n",
            "soon\n",
            "sorry\n",
            "sort\n",
            "soul\n",
            "sound\n",
            "soundtrack\n",
            "south\n",
            "space\n",
            "speak\n",
            "speaking\n",
            "special\n",
            "spend\n",
            "spent\n",
            "spirit\n",
            "spoiler\n",
            "spot\n",
            "stage\n",
            "stand\n",
            "standard\n",
            "star\n",
            "starring\n",
            "start\n",
            "started\n",
            "starting\n",
            "state\n",
            "station\n",
            "stay\n",
            "steal\n",
            "step\n",
            "stephen\n",
            "stereotype\n",
            "steve\n",
            "stick\n",
            "still\n",
            "stone\n",
            "stop\n",
            "store\n",
            "storyline\n",
            "straight\n",
            "strange\n",
            "street\n",
            "strength\n",
            "strong\n",
            "struggle\n",
            "stuck\n",
            "student\n",
            "studio\n",
            "study\n",
            "stuff\n",
            "stunning\n",
            "stupid\n",
            "style\n",
            "sub\n",
            "subject\n",
            "subtle\n",
            "success\n",
            "successful\n",
            "suck\n",
            "suddenly\n",
            "suggest\n",
            "suicide\n",
            "suit\n",
            "summer\n",
            "super\n",
            "superb\n",
            "superior\n",
            "support\n",
            "supporting\n",
            "suppose\n",
            "supposed\n",
            "supposedly\n",
            "sure\n",
            "surely\n",
            "surprise\n",
            "surprised\n",
            "surprising\n",
            "surprisingly\n",
            "suspect\n",
            "suspense\n",
            "sweet\n",
            "system\n",
            "take\n",
            "taken\n",
            "taking\n",
            "tale\n",
            "talent\n",
            "talented\n",
            "talk\n",
            "talking\n",
            "tape\n",
            "taste\n",
            "teacher\n",
            "team\n",
            "tear\n",
            "technical\n",
            "technique\n",
            "teen\n",
            "teenage\n",
            "teenager\n",
            "television\n",
            "tell\n",
            "telling\n",
            "ten\n",
            "tension\n",
            "term\n",
            "terrible\n",
            "terribly\n",
            "terrific\n",
            "thank\n",
            "thanks\n",
            "thats\n",
            "theater\n",
            "theatre\n",
            "theme\n",
            "therefore\n",
            "thin\n",
            "thing\n",
            "think\n",
            "thinking\n",
            "third\n",
            "thoroughly\n",
            "though\n",
            "thought\n",
            "thousand\n",
            "three\n",
            "thriller\n",
            "throughout\n",
            "throw\n",
            "thrown\n",
            "thus\n",
            "tired\n",
            "title\n",
            "today\n",
            "together\n",
            "told\n",
            "tom\n",
            "tone\n",
            "tony\n",
            "took\n",
            "top\n",
            "torture\n",
            "total\n",
            "totally\n",
            "touch\n",
            "touching\n",
            "tough\n",
            "toward\n",
            "towards\n",
            "town\n",
            "track\n",
            "tragedy\n",
            "tragic\n",
            "trailer\n",
            "train\n",
            "trash\n",
            "travel\n",
            "treat\n",
            "treated\n",
            "trick\n",
            "tried\n",
            "trip\n",
            "trouble\n",
            "true\n",
            "truly\n",
            "trust\n",
            "truth\n",
            "try\n",
            "trying\n",
            "turn\n",
            "turned\n",
            "turning\n",
            "tv\n",
            "twenty\n",
            "twice\n",
            "twist\n",
            "two\n",
            "type\n",
            "typical\n",
            "u\n",
            "ugly\n",
            "ultimately\n",
            "unbelievable\n",
            "understand\n",
            "understanding\n",
            "unfortunately\n",
            "unique\n",
            "unknown\n",
            "unless\n",
            "unlike\n",
            "unnecessary\n",
            "unusual\n",
            "upon\n",
            "ups\n",
            "us\n",
            "use\n",
            "used\n",
            "using\n",
            "usual\n",
            "usually\n",
            "utterly\n",
            "v\n",
            "value\n",
            "vampire\n",
            "van\n",
            "various\n",
            "vehicle\n",
            "version\n",
            "vhs\n",
            "victim\n",
            "video\n",
            "view\n",
            "viewer\n",
            "viewing\n",
            "villain\n",
            "violence\n",
            "violent\n",
            "vision\n",
            "visit\n",
            "visual\n",
            "voice\n",
            "vote\n",
            "wait\n",
            "waiting\n",
            "walk\n",
            "walking\n",
            "wall\n",
            "want\n",
            "wanted\n",
            "wanting\n",
            "war\n",
            "warning\n",
            "waste\n",
            "wasted\n",
            "watch\n",
            "watchable\n",
            "watched\n",
            "watching\n",
            "water\n",
            "way\n",
            "weak\n",
            "weapon\n",
            "wear\n",
            "wearing\n",
            "week\n",
            "weird\n",
            "went\n",
            "west\n",
            "western\n",
            "whatever\n",
            "whatsoever\n",
            "whenever\n",
            "whether\n",
            "white\n",
            "whole\n",
            "whose\n",
            "wide\n",
            "wife\n",
            "wild\n",
            "william\n",
            "willing\n",
            "win\n",
            "wind\n",
            "window\n",
            "winning\n",
            "wise\n",
            "wish\n",
            "wit\n",
            "witch\n",
            "within\n",
            "without\n",
            "witness\n",
            "witty\n",
            "woman\n",
            "wonder\n",
            "wonderful\n",
            "wonderfully\n",
            "wondering\n",
            "wood\n",
            "wooden\n",
            "word\n",
            "work\n",
            "worked\n",
            "working\n",
            "world\n",
            "worse\n",
            "worst\n",
            "worth\n",
            "worthy\n",
            "wow\n",
            "write\n",
            "writer\n",
            "writing\n",
            "written\n",
            "wrong\n",
            "wrote\n",
            "x\n",
            "yeah\n",
            "year\n",
            "yes\n",
            "yet\n",
            "york\n",
            "young\n",
            "younger\n",
            "youth\n",
            "zero\n",
            "zombie\n"
          ]
        }
      ],
      "source": [
        "for d in vectorizer.get_feature_names_out():\n",
        "  print(d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtSWJEDkgk8L"
      },
      "source": [
        "#### Without priors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3fca8e9-dee1-4d78-9dd0-b61205a62b5a",
        "id": "USu6Bo9hgk8L"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.84328\n"
          ]
        }
      ],
      "source": [
        "model=MultinomialNaiveBayes()\n",
        "params=model.fit(X_train, y_train, 0,0,0,0)\n",
        "y_prob=model.predict(X_test)\n",
        "y_pred = np.argmax(y_prob, 1)\n",
        "print(\"Accuracy:\",model.evaluate_acc(y_pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-P_TOfygk8M"
      },
      "source": [
        "#### With priors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69e31e2f-7d7b-42c2-8ba9-8865da77aabd",
        "id": "NBIWKuGTgk8M"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.84348\n"
          ]
        }
      ],
      "source": [
        "model=MultinomialNaiveBayes()\n",
        "params=model.fit(X_train, y_train, 1,1,10,10)\n",
        "y_prob=model.predict(X_test)\n",
        "y_pred = np.argmax(y_prob, 1)\n",
        "print(\"Accuracy:\",model.evaluate_acc(y_pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbaAxiRitFra"
      },
      "source": [
        "### With other limits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e5GiEu7tFra"
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer(stop_words=stopword_list, tokenizer=LemmaTokenizer(), min_df=0.001, max_df=0.65)\n",
        "X_train = vectorizer.fit_transform(x_train)\n",
        "dic= vectorizer.get_feature_names_out()\n",
        "X_test=vectorizer.transform(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOobH2-xtFra",
        "outputId": "56b191f3-fd29-4722-96f1-58c3b4f9117b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dictionary length: 9429\n"
          ]
        }
      ],
      "source": [
        "print(\"Dictionary length:\", len(vectorizer.get_feature_names_out()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnd-h2-TtFra",
        "outputId": "e6ee0750-de95-4482-d26f-36cf871efe0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "inferior\n",
            "infested\n",
            "infidelity\n",
            "infinitely\n",
            "inflicted\n",
            "influence\n",
            "influenced\n",
            "influential\n",
            "info\n",
            "inform\n",
            "information\n",
            "informative\n",
            "informed\n",
            "informs\n",
            "ing\n",
            "ingenious\n",
            "ingenuity\n",
            "ingredient\n",
            "ingrid\n",
            "inhabit\n",
            "inhabitant\n",
            "inhabited\n",
            "inherent\n",
            "inherently\n",
            "inheritance\n",
            "inherited\n",
            "initial\n",
            "initially\n",
            "inject\n",
            "injured\n",
            "injury\n",
            "injustice\n",
            "inmate\n",
            "inner\n",
            "innocence\n",
            "innocent\n",
            "innovation\n",
            "innovative\n",
            "innuendo\n",
            "insane\n",
            "insanely\n",
            "insanity\n",
            "insect\n",
            "insecure\n",
            "insecurity\n",
            "insert\n",
            "inserted\n",
            "inside\n",
            "insight\n",
            "insightful\n",
            "insignificant\n",
            "insipid\n",
            "insist\n",
            "insisted\n",
            "insists\n",
            "insomnia\n",
            "insomniac\n",
            "inspector\n",
            "inspiration\n",
            "inspirational\n",
            "inspire\n",
            "inspired\n",
            "inspires\n",
            "inspiring\n",
            "installment\n",
            "instance\n",
            "instant\n",
            "instantly\n",
            "instead\n",
            "instinct\n",
            "institution\n",
            "instruction\n",
            "instructor\n",
            "instrument\n",
            "instrumental\n",
            "insult\n",
            "insulted\n",
            "insulting\n",
            "insurance\n",
            "intact\n",
            "integral\n",
            "integrated\n",
            "integrity\n",
            "intellect\n",
            "intellectual\n",
            "intellectually\n",
            "intelligence\n",
            "intelligent\n",
            "intelligently\n",
            "intend\n",
            "intended\n",
            "intends\n",
            "intense\n",
            "intensely\n",
            "intensity\n",
            "intent\n",
            "intention\n",
            "intentional\n",
            "intentionally\n",
            "inter\n",
            "interact\n",
            "interacting\n",
            "interaction\n",
            "interest\n",
            "interested\n",
            "interesting\n",
            "interestingly\n",
            "interior\n",
            "interlude\n",
            "interminable\n",
            "internal\n",
            "international\n",
            "internet\n",
            "interplay\n",
            "interpret\n",
            "interpretation\n",
            "interpreted\n",
            "interrupt\n",
            "interrupted\n",
            "interspersed\n",
            "intertwined\n",
            "interval\n",
            "interview\n",
            "interviewed\n",
            "intestine\n",
            "intimacy\n",
            "intimate\n",
            "intimidating\n",
            "intricate\n",
            "intrigue\n",
            "intrigued\n",
            "intriguing\n",
            "intro\n",
            "introduce\n",
            "introduced\n",
            "introduces\n",
            "introducing\n",
            "introduction\n",
            "intrusive\n",
            "invariably\n",
            "invasion\n",
            "invent\n",
            "invented\n",
            "invention\n",
            "inventive\n",
            "inventor\n",
            "invest\n",
            "invested\n",
            "investigate\n",
            "investigates\n",
            "investigating\n",
            "investigation\n",
            "investigator\n",
            "investment\n",
            "invincible\n",
            "invisible\n",
            "invitation\n",
            "invite\n",
            "invited\n",
            "inviting\n",
            "involve\n",
            "involved\n",
            "involvement\n",
            "involves\n",
            "involving\n",
            "iq\n",
            "ira\n",
            "iran\n",
            "iranian\n",
            "iraq\n",
            "ireland\n",
            "irene\n",
            "irish\n",
            "iron\n",
            "ironic\n",
            "ironically\n",
            "irony\n",
            "irrational\n",
            "irrelevant\n",
            "irresistible\n",
            "irresponsible\n",
            "irreverent\n",
            "irritated\n",
            "irritating\n",
            "irving\n",
            "isabelle\n",
            "ish\n",
            "island\n",
            "isolated\n",
            "isolation\n",
            "israel\n",
            "israeli\n",
            "issue\n",
            "italian\n",
            "italy\n",
            "item\n",
            "iv\n",
            "ivan\n",
            "ivory\n",
            "ivy\n",
            "izzard\n",
            "j\n",
            "jack\n",
            "jackass\n",
            "jacket\n",
            "jackie\n",
            "jackson\n",
            "jacob\n",
            "jacques\n",
            "jaded\n",
            "jagger\n",
            "jail\n",
            "jake\n",
            "jam\n",
            "james\n",
            "jameson\n",
            "jamie\n",
            "jan\n",
            "jane\n",
            "janet\n",
            "janitor\n",
            "january\n",
            "japan\n",
            "japanese\n",
            "jar\n",
            "jared\n",
            "jarring\n",
            "jason\n",
            "jaw\n",
            "jay\n",
            "jazz\n",
            "je\n",
            "jealous\n",
            "jealousy\n",
            "jean\n",
            "jedi\n",
            "jeep\n",
            "jeff\n",
            "jeffrey\n",
            "jenna\n",
            "jennifer\n",
            "jenny\n",
            "jeremy\n",
            "jerk\n",
            "jerky\n",
            "jerry\n",
            "jersey\n",
            "jess\n",
            "jesse\n",
            "jessica\n",
            "jesus\n",
            "jet\n",
            "jew\n",
            "jewel\n",
            "jewelry\n",
            "jewish\n",
            "jill\n",
            "jim\n",
            "jimmy\n",
            "jo\n",
            "joan\n",
            "joanna\n",
            "job\n",
            "jock\n",
            "jodie\n",
            "joe\n",
            "joel\n",
            "joey\n",
            "johansson\n",
            "john\n",
            "johnny\n",
            "johnson\n",
            "join\n",
            "joined\n",
            "joining\n",
            "joint\n",
            "joke\n",
            "joker\n",
            "joking\n",
            "jolie\n",
            "jolly\n",
            "jolt\n",
            "jon\n",
            "jonathan\n",
            "jones\n",
            "jordan\n",
            "jose\n",
            "joseph\n",
            "josh\n",
            "journalist\n",
            "journey\n",
            "joy\n",
            "joyous\n",
            "jr\n",
            "juan\n",
            "judd\n",
            "jude\n",
            "judge\n",
            "judged\n",
            "judgement\n",
            "judging\n",
            "judgment\n",
            "judy\n",
            "juice\n",
            "juicy\n",
            "jules\n",
            "julia\n",
            "julian\n",
            "julie\n",
            "juliet\n",
            "juliette\n",
            "july\n",
            "jumbo\n",
            "jump\n",
            "jumped\n",
            "jumping\n",
            "june\n",
            "jungle\n",
            "junior\n",
            "junk\n",
            "jurassic\n",
            "jury\n",
            "justice\n",
            "justification\n",
            "justified\n",
            "justify\n",
            "justin\n",
            "juvenile\n",
            "k\n",
            "kane\n",
            "kansa\n",
            "kapoor\n",
            "karate\n",
            "karen\n",
            "karl\n",
            "karloff\n",
            "kate\n",
            "kathleen\n",
            "kathryn\n",
            "kathy\n",
            "kaufman\n",
            "kay\n",
            "kazan\n",
            "keanu\n",
            "keaton\n",
            "keen\n",
            "keep\n",
            "keeper\n",
            "keeping\n",
            "keitel\n",
            "keith\n",
            "kelly\n",
            "ken\n",
            "kennedy\n",
            "kenneth\n",
            "kenny\n",
            "kent\n",
            "kentucky\n",
            "kept\n",
            "kevin\n",
            "key\n",
            "keyboard\n",
            "khan\n",
            "kick\n",
            "kicked\n",
            "kicking\n",
            "kid\n",
            "kiddie\n",
            "kidding\n",
            "kiddy\n",
            "kidman\n",
            "kidnap\n",
            "kidnapped\n",
            "kidnapping\n",
            "kidnaps\n",
            "kill\n",
            "killed\n",
            "killer\n",
            "killing\n",
            "kilmer\n",
            "kim\n",
            "kind\n",
            "kinda\n",
            "kindly\n",
            "kindness\n",
            "king\n",
            "kingdom\n",
            "kingsley\n",
            "kinky\n",
            "kinnear\n",
            "kinski\n",
            "kirk\n",
            "kirsten\n",
            "kiss\n",
            "kissing\n",
            "kitchen\n",
            "kitten\n",
            "kitty\n",
            "kline\n",
            "knack\n",
            "knee\n",
            "knew\n",
            "knife\n",
            "knight\n",
            "knightley\n",
            "knock\n",
            "knocked\n",
            "knocking\n",
            "know\n",
            "knowing\n",
            "knowledge\n",
            "known\n",
            "kong\n",
            "korea\n",
            "korean\n",
            "kris\n",
            "kristofferson\n",
            "krueger\n",
            "kubrick\n",
            "kudos\n",
            "kumar\n",
            "kung\n",
            "kurosawa\n",
            "kurt\n",
            "kyle\n",
            "l\n",
            "la\n",
            "lab\n",
            "label\n",
            "labeled\n",
            "labor\n",
            "laboratory\n",
            "labyrinth\n",
            "laced\n",
            "lack\n",
            "lacked\n",
            "lacking\n",
            "lackluster\n",
            "lad\n",
            "ladder\n",
            "laden\n",
            "lady\n",
            "laid\n",
            "lake\n",
            "lam\n",
            "lamb\n",
            "lame\n",
            "lampoon\n",
            "lance\n",
            "land\n",
            "landed\n",
            "landing\n",
            "landlord\n",
            "landmark\n",
            "landscape\n",
            "lane\n",
            "lang\n",
            "lange\n",
            "language\n",
            "lap\n",
            "lapse\n",
            "lara\n",
            "large\n",
            "largely\n",
            "larger\n",
            "largest\n",
            "larry\n",
            "lars\n",
            "laser\n",
            "last\n",
            "lasted\n",
            "lasting\n",
            "lastly\n",
            "late\n",
            "lately\n",
            "later\n",
            "latest\n",
            "latin\n",
            "latino\n",
            "latter\n",
            "laugh\n",
            "laughable\n",
            "laughably\n",
            "laughed\n",
            "laughing\n",
            "laughter\n",
            "launch\n",
            "launched\n",
            "laura\n",
            "laurel\n",
            "lauren\n",
            "laurence\n",
            "laurie\n",
            "lavish\n",
            "law\n",
            "lawn\n",
            "lawrence\n",
            "lawyer\n",
            "lay\n",
            "layer\n",
            "layered\n",
            "laying\n",
            "lazy\n",
            "le\n",
            "lead\n",
            "leader\n",
            "leadership\n",
            "leading\n",
            "leaf\n",
            "league\n",
            "lean\n",
            "leaning\n",
            "leap\n",
            "learn\n",
            "learned\n",
            "learning\n",
            "learns\n",
            "leary\n",
            "least\n",
            "leather\n",
            "leave\n",
            "leaving\n",
            "lecture\n",
            "led\n",
            "ledger\n",
            "lee\n",
            "left\n",
            "leg\n",
            "legacy\n",
            "legal\n",
            "legend\n",
            "legendary\n",
            "legion\n",
            "legitimate\n",
            "leigh\n",
            "lemmon\n",
            "lena\n",
            "lend\n",
            "lends\n",
            "length\n",
            "lengthy\n",
            "lennon\n",
            "lens\n",
            "lent\n",
            "leo\n",
            "leon\n",
            "leonard\n",
            "leonardo\n",
            "lesbian\n",
            "leslie\n",
            "lesser\n",
            "lesson\n",
            "lester\n",
            "let\n",
            "letdown\n",
            "lethal\n",
            "letter\n",
            "letting\n",
            "level\n",
            "levy\n",
            "lewis\n",
            "lex\n",
            "li\n",
            "liam\n",
            "liar\n",
            "liberal\n",
            "liberty\n",
            "librarian\n",
            "library\n",
            "license\n",
            "lick\n",
            "lie\n",
            "lieutenant\n",
            "life\n",
            "lifeless\n",
            "lifelong\n",
            "lifestyle\n",
            "lifetime\n",
            "lift\n",
            "lifted\n",
            "light\n",
            "lighten\n",
            "lighter\n",
            "lighthearted\n",
            "lighting\n",
            "lightly\n",
            "lightning\n",
            "lightweight\n",
            "likable\n",
            "like\n",
            "likeable\n",
            "liked\n",
            "likely\n",
            "likewise\n",
            "liking\n",
            "lil\n",
            "lili\n",
            "lily\n",
            "limb\n",
            "limit\n",
            "limitation\n",
            "limited\n",
            "limp\n",
            "lin\n",
            "lincoln\n",
            "linda\n",
            "lindsay\n",
            "line\n",
            "linear\n",
            "liner\n",
            "lingering\n",
            "link\n",
            "linked\n",
            "lion\n",
            "lionel\n",
            "liotta\n",
            "lip\n",
            "liquor\n",
            "lisa\n",
            "list\n",
            "listed\n",
            "listen\n",
            "listened\n",
            "listener\n",
            "listening\n",
            "listing\n",
            "lit\n",
            "literal\n",
            "literally\n",
            "literary\n",
            "literate\n",
            "literature\n",
            "little\n",
            "liu\n",
            "live\n",
            "lived\n",
            "lively\n",
            "living\n",
            "liz\n",
            "liza\n",
            "lizard\n",
            "lloyd\n",
            "lo\n",
            "load\n",
            "loaded\n",
            "loathing\n",
            "local\n",
            "locale\n",
            "locate\n",
            "located\n",
            "location\n",
            "lock\n",
            "locke\n",
            "locked\n",
            "locker\n",
            "log\n",
            "logan\n",
            "logic\n",
            "logical\n",
            "logo\n",
            "lois\n",
            "lol\n",
            "lola\n",
            "london\n",
            "lone\n",
            "loneliness\n",
            "lonely\n",
            "loner\n",
            "long\n",
            "longer\n",
            "longest\n",
            "longing\n",
            "longoria\n",
            "longtime\n",
            "look\n",
            "looked\n",
            "looking\n",
            "looney\n",
            "loony\n",
            "loop\n",
            "loose\n",
            "loosely\n",
            "lopez\n",
            "lord\n",
            "lore\n",
            "loren\n",
            "loretta\n",
            "lorre\n",
            "los\n",
            "lose\n",
            "loser\n",
            "loses\n",
            "losing\n",
            "loss\n",
            "lost\n",
            "lot\n",
            "lotr\n",
            "lou\n",
            "loud\n",
            "loudly\n",
            "louis\n",
            "louise\n",
            "lousy\n",
            "lovable\n",
            "love\n",
            "loved\n",
            "lovely\n",
            "lover\n",
            "loving\n",
            "lovingly\n",
            "low\n",
            "lowe\n",
            "lower\n",
            "lowered\n",
            "lowest\n",
            "loy\n",
            "loyal\n",
            "loyalty\n",
            "lt\n",
            "lubitsch\n",
            "luc\n",
            "lucas\n",
            "lucille\n",
            "lucio\n",
            "luck\n",
            "luckily\n",
            "lucky\n",
            "lucy\n",
            "ludicrous\n",
            "ludicrously\n",
            "lugosi\n",
            "luis\n",
            "lukas\n",
            "luke\n",
            "lumet\n",
            "lump\n",
            "lunatic\n",
            "lunch\n",
            "lundgren\n",
            "lung\n",
            "lure\n",
            "lured\n",
            "lurid\n",
            "lurking\n",
            "luscious\n",
            "lush\n",
            "lust\n",
            "luxury\n",
            "lying\n",
            "lynch\n",
            "lyric\n",
            "lyrical\n",
            "mac\n",
            "macabre\n",
            "macbeth\n",
            "macdonald\n",
            "machination\n",
            "machine\n",
            "machinery\n",
            "macho\n",
            "mack\n",
            "macy\n",
            "mad\n",
            "made\n",
            "madeleine\n",
            "madison\n",
            "madly\n",
            "madman\n",
            "madness\n",
            "madonna\n",
            "madsen\n",
            "mae\n",
            "mafia\n",
            "magazine\n",
            "maggie\n",
            "magic\n",
            "magical\n",
            "magically\n",
            "magician\n",
            "magnificent\n",
            "magnificently\n",
            "magnolia\n",
            "maguire\n",
            "maid\n",
            "maiden\n",
            "mail\n",
            "main\n",
            "mainly\n",
            "mainstream\n",
            "maintain\n",
            "maintained\n",
            "maintaining\n",
            "maintains\n",
            "majestic\n",
            "major\n",
            "majority\n",
            "make\n",
            "maker\n",
            "makeup\n",
            "making\n",
            "malcolm\n",
            "malden\n",
            "male\n",
            "malevolent\n",
            "mall\n",
            "malone\n",
            "mama\n",
            "man\n",
            "manage\n",
            "managed\n",
            "management\n",
            "manager\n",
            "manages\n",
            "managing\n",
            "mandatory\n",
            "manga\n",
            "manhattan\n",
            "maniac\n",
            "maniacal\n",
            "manic\n",
            "manipulate\n",
            "manipulated\n",
            "manipulation\n",
            "manipulative\n",
            "mankind\n",
            "mann\n",
            "manner\n",
            "mannered\n",
            "mannerism\n",
            "manos\n",
            "mansion\n",
            "manson\n",
            "manufactured\n",
            "many\n",
            "map\n",
            "mar\n",
            "marathon\n",
            "marc\n",
            "marcel\n",
            "march\n",
            "margaret\n",
            "marginally\n",
            "maria\n",
            "marie\n",
            "marijuana\n",
            "marilyn\n",
            "marine\n",
            "mario\n",
            "marion\n",
            "marisa\n",
            "marital\n",
            "mark\n",
            "marked\n",
            "market\n",
            "marketed\n",
            "marketing\n",
            "marlon\n",
            "marred\n",
            "marriage\n",
            "married\n",
            "marries\n",
            "marry\n",
            "marrying\n",
            "marshall\n",
            "mart\n",
            "martha\n",
            "martial\n",
            "martian\n",
            "martin\n",
            "marty\n",
            "marvel\n",
            "marvellous\n",
            "marvelous\n",
            "marvelously\n",
            "marvin\n",
            "marx\n",
            "mary\n",
            "masculine\n",
            "mash\n",
            "mask\n",
            "masked\n",
            "mason\n",
            "masquerading\n",
            "mass\n",
            "massacre\n",
            "massey\n",
            "massive\n",
            "master\n",
            "masterful\n",
            "masterfully\n",
            "mastermind\n",
            "masterpiece\n",
            "masterson\n",
            "mastery\n",
            "mastroianni\n",
            "masturbation\n",
            "match\n",
            "matched\n",
            "matching\n",
            "mate\n",
            "material\n",
            "math\n",
            "matin\n",
            "matrix\n",
            "matt\n",
            "matter\n",
            "matthau\n",
            "matthew\n",
            "mature\n",
            "maturity\n",
            "maureen\n",
            "maurice\n",
            "maverick\n",
            "max\n",
            "maximum\n",
            "may\n",
            "maybe\n",
            "mayhem\n",
            "mayor\n",
            "maze\n",
            "mccarthy\n",
            "mccoy\n",
            "mcdermott\n",
            "mcdonald\n",
            "mcdowell\n",
            "mclaglen\n",
            "mcqueen\n",
            "meadow\n",
            "meal\n",
            "mean\n",
            "meander\n",
            "meandering\n",
            "meaning\n",
            "meaningful\n",
            "meaningless\n",
            "meant\n",
            "meantime\n",
            "meanwhile\n",
            "measure\n",
            "measured\n",
            "meat\n",
            "mechanic\n",
            "mechanical\n",
            "mechanism\n",
            "medal\n",
            "medical\n",
            "medicine\n",
            "medieval\n",
            "mediocre\n",
            "mediocrity\n",
            "meditation\n",
            "medium\n",
            "meek\n",
            "meet\n",
            "meeting\n",
            "meg\n",
            "mega\n",
            "mel\n",
            "melancholy\n",
            "melbourne\n",
            "melissa\n",
            "melodrama\n",
            "melodramatic\n",
            "melody\n",
            "melt\n",
            "melting\n",
            "melvyn\n",
            "member\n",
            "memoir\n",
            "memorable\n",
            "memory\n",
            "men\n",
            "menace\n",
            "menacing\n",
            "mental\n",
            "mentality\n",
            "mentally\n",
            "mention\n",
            "mentioned\n",
            "mentioning\n",
            "mentor\n",
            "menu\n",
            "mercenary\n",
            "merchant\n",
            "mercifully\n",
            "mercilessly\n",
            "mercy\n",
            "mere\n",
            "meredith\n",
            "merely\n",
            "merit\n",
            "mermaid\n",
            "merry\n",
            "meryl\n",
            "mesmerizing\n",
            "mess\n",
            "message\n",
            "messed\n",
            "messing\n",
            "messy\n",
            "met\n",
            "metal\n",
            "metaphor\n",
            "metaphysical\n",
            "meteor\n",
            "meter\n",
            "method\n",
            "metropolis\n",
            "mexican\n",
            "mexico\n",
            "meyer\n",
            "meyers\n",
            "mgm\n",
            "mi\n",
            "mia\n",
            "miami\n",
            "michael\n",
            "micheal\n",
            "michel\n",
            "michelle\n",
            "mick\n",
            "mickey\n",
            "microphone\n",
            "mid\n",
            "middle\n",
            "midget\n",
            "midnight\n",
            "midst\n",
            "midway\n",
            "might\n",
            "mighty\n",
            "miike\n",
            "mike\n",
            "mild\n",
            "mildly\n",
            "mildred\n",
            "mile\n",
            "milestone\n",
            "military\n",
            "milk\n",
            "mill\n",
            "millennium\n",
            "miller\n",
            "million\n",
            "millionaire\n",
            "milo\n",
            "mimic\n",
            "min\n",
            "mind\n",
            "minded\n",
            "mindless\n",
            "mindset\n",
            "mine\n",
            "miner\n",
            "mini\n",
            "miniature\n",
            "minimal\n",
            "minimum\n",
            "minion\n",
            "miniseries\n",
            "minister\n",
            "minnelli\n",
            "minor\n",
            "minority\n",
            "minus\n",
            "minute\n",
            "mira\n",
            "miracle\n",
            "miraculously\n",
            "miranda\n",
            "mirror\n",
            "misadventure\n",
            "miscast\n",
            "mischievous\n",
            "miserable\n",
            "miserably\n",
            "misery\n",
            "misfire\n",
            "misfit\n",
            "misfortune\n",
            "misguided\n",
            "mish\n",
            "mishap\n",
            "misleading\n",
            "mismatched\n",
            "misogynistic\n",
            "misplaced\n",
            "miss\n",
            "missed\n",
            "missile\n",
            "missing\n",
            "mission\n",
            "mist\n",
            "mistake\n",
            "mistaken\n",
            "mister\n",
            "mistress\n",
            "misty\n",
            "misunderstanding\n",
            "misunderstood\n",
            "mitch\n",
            "mitchell\n",
            "mitchum\n",
            "mix\n",
            "mixed\n",
            "mixing\n",
            "mixture\n",
            "miyazaki\n",
            "mo\n",
            "moan\n",
            "mob\n",
            "mobile\n",
            "mobster\n",
            "mock\n",
            "mockery\n",
            "mocking\n",
            "mode\n",
            "model\n",
            "moderate\n",
            "moderately\n",
            "modern\n",
            "modest\n",
            "modesty\n",
            "moe\n",
            "mol\n",
            "mold\n",
            "mole\n",
            "molly\n",
            "mom\n",
            "moment\n",
            "momentum\n",
            "mommy\n",
            "mon\n",
            "monday\n",
            "money\n",
            "monica\n",
            "monitor\n",
            "monk\n",
            "monkey\n",
            "monologue\n",
            "monotone\n",
            "monotonous\n",
            "monroe\n",
            "monster\n",
            "monstrosity\n",
            "monstrous\n",
            "montage\n",
            "montana\n",
            "month\n",
            "monty\n",
            "monumental\n",
            "mood\n",
            "moody\n",
            "moon\n",
            "moore\n",
            "moral\n",
            "morale\n",
            "morality\n",
            "morally\n",
            "morbid\n",
            "moreover\n",
            "morgan\n",
            "morgue\n",
            "mormon\n",
            "morning\n",
            "moron\n",
            "moronic\n",
            "morris\n",
            "morse\n",
            "mortal\n",
            "moss\n",
            "mostly\n",
            "motel\n",
            "mother\n",
            "motif\n",
            "motion\n",
            "motivated\n",
            "motivation\n",
            "motive\n",
            "motor\n",
            "motorcycle\n",
            "mount\n",
            "mountain\n",
            "mourning\n",
            "mouse\n",
            "mouth\n",
            "mouthed\n",
            "move\n",
            "moved\n",
            "movement\n",
            "movie\n",
            "moviegoer\n",
            "moving\n",
            "mpaa\n",
            "mr\n",
            "mst3k\n",
            "mtv\n",
            "much\n",
            "mud\n",
            "muddled\n",
            "muddy\n",
            "multi\n",
            "multiple\n",
            "mum\n",
            "mummy\n",
            "mundane\n",
            "muppet\n",
            "muppets\n",
            "murder\n",
            "murdered\n",
            "murderer\n",
            "murdering\n",
            "murderous\n",
            "murky\n",
            "murphy\n",
            "murray\n",
            "muscle\n",
            "muscular\n",
            "museum\n",
            "music\n",
            "musical\n",
            "musician\n",
            "muslim\n",
            "must\n",
            "mustache\n",
            "muster\n",
            "mutant\n",
            "mutated\n",
            "mute\n",
            "muted\n",
            "mutilated\n",
            "mutual\n",
            "myers\n",
            "myriad\n",
            "myrna\n",
            "mysterious\n",
            "mysteriously\n",
            "mystery\n",
            "mystic\n",
            "mystical\n",
            "mysticism\n",
            "myth\n",
            "mythical\n",
            "mythology\n",
            "n\n",
            "na\n",
            "nail\n",
            "nailed\n",
            "naive\n",
            "naked\n",
            "name\n",
            "named\n",
            "namely\n",
            "naming\n",
            "nancy\n",
            "nanny\n",
            "napoleon\n",
            "narcissistic\n",
            "narrated\n",
            "narration\n",
            "narrative\n",
            "narrator\n",
            "narrow\n",
            "nasty\n",
            "natali\n",
            "natalie\n",
            "natasha\n",
            "nathan\n",
            "nation\n",
            "national\n",
            "native\n",
            "natural\n",
            "naturalistic\n",
            "naturally\n",
            "nature\n",
            "natured\n",
            "naughty\n",
            "nauseating\n",
            "naval\n",
            "navy\n",
            "nazi\n",
            "nbc\n",
            "nc\n",
            "ne\n",
            "neal\n",
            "near\n",
            "nearby\n",
            "nearest\n",
            "nearly\n",
            "neat\n",
            "neatly\n",
            "necessarily\n",
            "necessary\n",
            "necessity\n",
            "neck\n",
            "ned\n",
            "need\n",
            "needed\n",
            "needing\n",
            "needle\n",
            "needlessly\n",
            "neeson\n",
            "negative\n",
            "neglect\n",
            "neglected\n",
            "neighbor\n",
            "neighborhood\n",
            "neighbour\n",
            "neil\n",
            "neill\n",
            "neither\n",
            "nelson\n",
            "nemesis\n",
            "neo\n",
            "neon\n",
            "nephew\n",
            "nerd\n",
            "nerdy\n",
            "nerve\n",
            "nervous\n",
            "nest\n",
            "net\n",
            "netflix\n",
            "network\n",
            "neurotic\n",
            "neutral\n",
            "never\n",
            "nevertheless\n",
            "new\n",
            "newcomer\n",
            "newer\n",
            "newly\n",
            "newman\n",
            "news\n",
            "newspaper\n",
            "next\n",
            "nice\n",
            "nicely\n",
            "niche\n",
            "nicholas\n",
            "nicholson\n",
            "nick\n",
            "nicolas\n",
            "nicole\n",
            "niece\n",
            "nielsen\n",
            "nifty\n",
            "night\n",
            "nightclub\n",
            "nightmare\n",
            "nightmarish\n",
            "nina\n",
            "nine\n",
            "ninety\n",
            "ninja\n",
            "nipple\n",
            "niro\n",
            "niven\n",
            "noble\n",
            "nobody\n",
            "nod\n",
            "noir\n",
            "noise\n",
            "nolan\n",
            "nolte\n",
            "nominated\n",
            "nomination\n",
            "nominee\n",
            "non\n",
            "none\n",
            "nonetheless\n",
            "nonexistent\n",
            "nonsense\n",
            "nonsensical\n",
            "nope\n",
            "nora\n",
            "norm\n",
            "normal\n",
            "normally\n",
            "norman\n",
            "north\n",
            "northam\n",
            "northern\n",
            "northwest\n",
            "norton\n",
            "nose\n",
            "nostalgia\n",
            "nostalgic\n",
            "notable\n",
            "notably\n",
            "notch\n",
            "note\n",
            "noted\n",
            "noteworthy\n",
            "nothing\n",
            "notice\n",
            "noticeable\n",
            "noticeably\n",
            "noticed\n",
            "noticing\n",
            "noting\n",
            "notion\n",
            "notoriety\n",
            "notorious\n",
            "notwithstanding\n",
            "novak\n",
            "novel\n",
            "novelist\n",
            "novelty\n",
            "november\n",
            "novice\n",
            "nowadays\n",
            "nowhere\n",
            "nt\n",
            "nuance\n",
            "nuanced\n",
            "nuclear\n",
            "nude\n",
            "nudity\n",
            "number\n",
            "numbing\n",
            "numbingly\n",
            "numerous\n",
            "nun\n",
            "nurse\n",
            "nut\n",
            "nutshell\n",
            "nutty\n",
            "ny\n",
            "nyc\n",
            "object\n",
            "objection\n",
            "objective\n",
            "obligation\n",
            "obligatory\n",
            "oblivion\n",
            "oblivious\n",
            "obnoxious\n",
            "obscene\n",
            "obscure\n",
            "obscurity\n",
            "observation\n",
            "observe\n",
            "observed\n",
            "observer\n",
            "observing\n",
            "obsessed\n",
            "obsession\n",
            "obsessive\n",
            "obstacle\n",
            "obtain\n",
            "obvious\n",
            "obviously\n",
            "occasion\n",
            "occasional\n",
            "occasionally\n",
            "occult\n",
            "occupation\n",
            "occupied\n",
            "occur\n",
            "occurred\n",
            "occurrence\n",
            "occurring\n",
            "occurs\n",
            "ocean\n",
            "october\n",
            "odd\n",
            "oddball\n",
            "oddity\n",
            "oddly\n",
            "odds\n",
            "odyssey\n",
            "offbeat\n",
            "offend\n",
            "offended\n",
            "offender\n",
            "offense\n",
            "offensive\n",
            "offer\n",
            "offered\n",
            "offering\n",
            "office\n",
            "officer\n",
            "official\n",
            "officially\n",
            "offs\n",
            "offspring\n",
            "often\n",
            "oh\n",
            "ohio\n",
            "oil\n",
            "ok\n",
            "okay\n",
            "ol\n",
            "old\n",
            "older\n",
            "oldest\n",
            "oliver\n",
            "olivia\n",
            "olivier\n",
            "ollie\n",
            "olsen\n",
            "olympia\n",
            "olympic\n",
            "omar\n",
            "omen\n",
            "ominous\n",
            "one\n",
            "ongoing\n",
            "online\n",
            "onto\n",
            "oops\n",
            "ooze\n",
            "open\n",
            "opened\n",
            "opener\n",
            "opening\n",
            "openly\n",
            "opera\n",
            "operate\n",
            "operatic\n",
            "operating\n",
            "operation\n",
            "opinion\n",
            "opponent\n",
            "opportunity\n",
            "opposed\n",
            "opposing\n",
            "opposite\n",
            "opposition\n",
            "oppression\n",
            "oppressive\n",
            "oprah\n",
            "optimism\n",
            "optimistic\n",
            "option\n",
            "opus\n",
            "orange\n",
            "orchestra\n",
            "orchestral\n",
            "ordeal\n",
            "order\n",
            "ordered\n",
            "ordinary\n",
            "organ\n",
            "organization\n",
            "organized\n",
            "orgy\n",
            "oriental\n",
            "oriented\n",
            "origin\n",
            "original\n",
            "originality\n",
            "originally\n",
            "orlando\n",
            "orleans\n",
            "orphan\n",
            "orphanage\n",
            "orson\n",
            "oscar\n",
            "ostensibly\n",
            "othello\n",
            "others\n",
            "otherwise\n",
            "otto\n",
            "ought\n",
            "ounce\n",
            "outbreak\n",
            "outburst\n",
            "outcast\n",
            "outcome\n",
            "outdated\n",
            "outdoor\n",
            "outer\n",
            "outfit\n",
            "outing\n",
            "outlandish\n",
            "outlaw\n",
            "outlet\n",
            "outline\n",
            "outlook\n",
            "output\n",
            "outrage\n",
            "outrageous\n",
            "outrageously\n",
            "outright\n",
            "outset\n",
            "outside\n",
            "outsider\n",
            "outstanding\n",
            "outta\n",
            "overacting\n",
            "overacts\n",
            "overall\n",
            "overbearing\n",
            "overblown\n",
            "overboard\n",
            "overcome\n",
            "overdone\n",
            "overlong\n",
            "overlook\n",
            "overlooked\n",
            "overly\n",
            "overnight\n",
            "overrated\n",
            "overseas\n",
            "overshadowed\n",
            "overt\n",
            "overtly\n",
            "overtone\n",
            "overused\n",
            "overweight\n",
            "overwhelmed\n",
            "overwhelming\n",
            "overwrought\n",
            "owe\n",
            "owen\n",
            "owes\n",
            "owl\n",
            "owned\n",
            "owner\n",
            "owning\n",
            "owns\n",
            "oz\n",
            "p\n",
            "pa\n",
            "pace\n",
            "paced\n",
            "pacific\n",
            "pacing\n",
            "pacino\n",
            "pack\n",
            "package\n",
            "packed\n",
            "pad\n",
            "padded\n",
            "padding\n",
            "page\n",
            "paid\n",
            "pain\n",
            "painful\n",
            "painfully\n",
            "paint\n",
            "painted\n",
            "painter\n",
            "painting\n",
            "pair\n",
            "paired\n",
            "pairing\n",
            "pal\n",
            "palace\n",
            "palance\n",
            "pale\n",
            "palestinian\n",
            "palette\n",
            "palm\n",
            "palma\n",
            "palpable\n",
            "paltrow\n",
            "pamela\n",
            "pan\n",
            "panic\n",
            "pant\n",
            "panty\n",
            "paper\n",
            "par\n",
            "parade\n",
            "paradise\n",
            "paragraph\n",
            "parallel\n",
            "paramount\n",
            "paranoia\n",
            "paranoid\n",
            "pardon\n",
            "parent\n",
            "paris\n",
            "parisian\n",
            "park\n",
            "parker\n",
            "parking\n",
            "parody\n",
            "parrot\n",
            "parson\n",
            "part\n",
            "partial\n",
            "partially\n",
            "participant\n",
            "participate\n",
            "participating\n",
            "participation\n",
            "particular\n",
            "particularly\n",
            "partly\n",
            "partner\n",
            "party\n",
            "partying\n",
            "pas\n",
            "pass\n",
            "passable\n",
            "passage\n",
            "passed\n",
            "passenger\n",
            "passing\n",
            "passion\n",
            "passionate\n",
            "passive\n",
            "past\n",
            "pastiche\n",
            "pat\n",
            "patch\n",
            "path\n",
            "pathetic\n",
            "pathos\n",
            "patience\n",
            "patient\n",
            "patriarch\n",
            "patricia\n",
            "patrick\n",
            "patriot\n",
            "patriotic\n",
            "patriotism\n",
            "patron\n",
            "pattern\n",
            "patty\n",
            "paul\n",
            "paula\n",
            "paulie\n",
            "pause\n",
            "pawn\n",
            "paxton\n",
            "pay\n",
            "paycheck\n",
            "paying\n",
            "payment\n",
            "payoff\n",
            "paz\n",
            "pb\n",
            "pc\n",
            "peace\n",
            "peaceful\n",
            "peak\n",
            "pearl\n",
            "peasant\n",
            "peck\n",
            "peckinpah\n",
            "peculiar\n",
            "pedestrian\n",
            "pee\n",
            "peek\n",
            "peer\n",
            "pegg\n",
            "pen\n",
            "penalty\n",
            "penchant\n",
            "penelope\n",
            "penguin\n",
            "penis\n",
            "penn\n",
            "penned\n",
            "penny\n",
            "people\n",
            "pepper\n",
            "per\n",
            "perceive\n",
            "perceived\n",
            "percent\n",
            "percentage\n",
            "perception\n",
            "perfect\n",
            "perfected\n",
            "perfection\n",
            "perfectly\n",
            "perform\n",
            "performance\n",
            "performed\n",
            "performer\n",
            "performing\n",
            "performs\n",
            "perhaps\n",
            "peril\n",
            "period\n",
            "perkins\n",
            "perky\n",
            "permanent\n",
            "permanently\n",
            "perpetrator\n",
            "perry\n",
            "person\n",
            "persona\n",
            "personal\n",
            "personality\n",
            "personally\n",
            "personnel\n",
            "perspective\n",
            "persuade\n",
            "pertwee\n",
            "perverse\n",
            "perversion\n",
            "pervert\n",
            "perverted\n",
            "pet\n",
            "pete\n",
            "peter\n",
            "petty\n",
            "pfeiffer\n",
            "pg\n",
            "phantom\n",
            "phase\n",
            "phenomenal\n",
            "phenomenon\n",
            "phil\n",
            "philadelphia\n",
            "philip\n",
            "phillip\n",
            "phillips\n",
            "philo\n",
            "philosophical\n",
            "philosophy\n",
            "phoenix\n",
            "phone\n",
            "phony\n",
            "photo\n",
            "photograph\n",
            "photographed\n",
            "photographer\n",
            "photography\n",
            "phrase\n",
            "physic\n",
            "physical\n",
            "physically\n",
            "pi\n",
            "pianist\n",
            "piano\n",
            "pic\n",
            "pick\n",
            "picked\n",
            "picking\n",
            "pickup\n",
            "picnic\n",
            "picture\n",
            "picturesque\n",
            "pie\n",
            "piece\n",
            "pierce\n",
            "pierre\n",
            "pig\n",
            "pile\n",
            "pill\n",
            "pilot\n",
            "pimp\n",
            "pin\n",
            "pink\n",
            "pinnacle\n",
            "pioneer\n",
            "pipe\n",
            "pirate\n",
            "pistol\n",
            "pit\n",
            "pitch\n",
            "pitched\n",
            "pitiful\n",
            "pitt\n",
            "pity\n",
            "pivotal\n",
            "pixar\n",
            "pizza\n",
            "place\n",
            "placed\n",
            "placement\n",
            "placing\n",
            "plague\n",
            "plagued\n",
            "plain\n",
            "plan\n",
            "plane\n",
            "planet\n",
            "planned\n",
            "planning\n",
            "plant\n",
            "planted\n",
            "plastic\n",
            "plate\n",
            "platform\n",
            "platoon\n",
            "plausibility\n",
            "plausible\n",
            "play\n",
            "playboy\n",
            "played\n",
            "player\n",
            "playful\n",
            "playing\n",
            "playwright\n",
            "plea\n",
            "pleasant\n",
            "pleasantly\n",
            "please\n",
            "pleased\n",
            "pleasing\n",
            "pleasure\n",
            "plenty\n",
            "plight\n",
            "plod\n",
            "plodding\n",
            "plot\n",
            "plotline\n",
            "plotted\n",
            "plotting\n",
            "ploy\n",
            "plug\n",
            "plummer\n",
            "plunge\n",
            "plus\n",
            "pocket\n",
            "poe\n",
            "poem\n",
            "poet\n",
            "poetic\n",
            "poetry\n",
            "poignancy\n",
            "poignant\n",
            "point\n",
            "pointed\n",
            "pointing\n",
            "pointless\n",
            "poison\n",
            "poke\n",
            "pokemon\n",
            "poker\n",
            "poking\n",
            "polanski\n",
            "polar\n",
            "pole\n",
            "police\n",
            "policeman\n",
            "policy\n",
            "polish\n",
            "polished\n",
            "polite\n",
            "political\n",
            "politically\n",
            "politician\n",
            "politics\n",
            "polly\n",
            "pompous\n",
            "pond\n",
            "ponder\n",
            "ponderous\n",
            "poo\n",
            "pool\n",
            "poor\n",
            "poorest\n",
            "poorly\n",
            "pop\n",
            "popcorn\n",
            "popped\n",
            "popping\n",
            "popular\n",
            "popularity\n",
            "populated\n",
            "population\n",
            "porn\n",
            "porno\n",
            "pornographic\n",
            "pornography\n",
            "port\n",
            "porter\n",
            "portion\n",
            "portman\n",
            "portrait\n",
            "portray\n",
            "portrayal\n",
            "portrayed\n",
            "portraying\n",
            "portrays\n",
            "pose\n",
            "posed\n",
            "posey\n",
            "posh\n",
            "posing\n",
            "position\n",
            "positive\n",
            "positively\n",
            "posse\n",
            "possessed\n",
            "possession\n",
            "possibility\n",
            "possible\n",
            "possibly\n",
            "post\n",
            "posted\n",
            "poster\n",
            "postman\n",
            "pot\n",
            "potato\n",
            "potent\n",
            "potential\n",
            "potentially\n",
            "potter\n",
            "pound\n",
            "pour\n",
            "pouring\n",
            "pov\n",
            "poverty\n",
            "pow\n",
            "powell\n",
            "power\n",
            "powered\n",
            "powerful\n",
            "powerfully\n",
            "practical\n",
            "practically\n",
            "practice\n",
            "practicing\n",
            "praise\n",
            "praised\n",
            "prank\n",
            "pray\n",
            "prayer\n",
            "praying\n",
            "pre\n",
            "preach\n",
            "preacher\n",
            "preaching\n",
            "preachy\n",
            "precious\n",
            "precise\n",
            "precisely\n",
            "precision\n",
            "predator\n",
            "predecessor\n",
            "predicament\n",
            "predict\n",
            "predictability\n",
            "predictable\n",
            "predictably\n",
            "predicted\n",
            "prefer\n",
            "preferably\n",
            "preference\n",
            "preferred\n",
            "prefers\n",
            "pregnancy\n",
            "pregnant\n",
            "prehistoric\n",
            "prejudice\n",
            "premier\n",
            "premiere\n",
            "premiered\n",
            "preminger\n",
            "premise\n",
            "preparation\n",
            "prepare\n",
            "prepared\n",
            "preparing\n",
            "preposterous\n",
            "prequel\n",
            "presence\n",
            "present\n",
            "presentation\n",
            "presented\n",
            "presenting\n",
            "preserve\n",
            "preserved\n",
            "president\n",
            "presidential\n",
            "press\n",
            "pressed\n",
            "pressure\n",
            "prestigious\n",
            "preston\n",
            "presumably\n",
            "presume\n",
            "presumed\n",
            "pretend\n",
            "pretending\n",
            "pretense\n",
            "pretension\n",
            "pretentious\n",
            "pretty\n",
            "prevalent\n",
            "prevent\n",
            "prevented\n",
            "preventing\n",
            "prevents\n",
            "preview\n",
            "previous\n",
            "previously\n",
            "prey\n",
            "price\n",
            "priceless\n",
            "pride\n",
            "priest\n",
            "primal\n",
            "primarily\n",
            "primary\n",
            "prime\n",
            "primitive\n",
            "prince\n",
            "princess\n",
            "principal\n",
            "principle\n",
            "print\n",
            "prior\n",
            "priority\n",
            "prison\n",
            "prisoner\n",
            "private\n",
            "privilege\n",
            "prize\n",
            "pro\n",
            "probably\n",
            "problem\n",
            "procedure\n",
            "proceed\n",
            "proceeding\n",
            "proceeds\n",
            "process\n",
            "proclaimed\n",
            "produce\n",
            "produced\n",
            "producer\n",
            "producing\n",
            "product\n",
            "production\n",
            "prof\n",
            "profanity\n",
            "profession\n",
            "professional\n",
            "professionally\n",
            "professor\n",
            "profile\n",
            "profit\n",
            "profound\n",
            "profoundly\n",
            "program\n",
            "programme\n",
            "programmer\n",
            "programming\n",
            "progress\n",
            "progressed\n",
            "progression\n",
            "progressive\n",
            "progressively\n",
            "project\n",
            "projected\n",
            "projection\n",
            "prolific\n",
            "prologue\n",
            "prolonged\n",
            "prom\n",
            "prominent\n",
            "promise\n",
            "promised\n",
            "promising\n",
            "promote\n",
            "promoted\n",
            "promoting\n",
            "promotion\n",
            "promotional\n",
            "promptly\n",
            "prone\n",
            "pronounced\n",
            "proof\n",
            "prop\n",
            "propaganda\n",
            "proper\n",
            "properly\n",
            "property\n",
            "prophecy\n",
            "prophet\n",
            "prophetic\n",
            "proportion\n",
            "prospect\n",
            "prostitute\n",
            "prostitution\n",
            "protagonist\n",
            "protect\n",
            "protecting\n",
            "protection\n",
            "protective\n",
            "protest\n",
            "prototype\n",
            "proud\n",
            "proudly\n",
            "prove\n",
            "proved\n",
            "proverbial\n",
            "provide\n",
            "provided\n",
            "provides\n",
            "providing\n",
            "proving\n",
            "provocative\n",
            "provoke\n",
            "provoking\n",
            "pseudo\n",
            "psyche\n",
            "psychedelic\n",
            "psychiatric\n",
            "psychiatrist\n",
            "psychic\n",
            "psycho\n",
            "psychological\n",
            "psychologically\n",
            "psychologist\n",
            "psychology\n",
            "psychopath\n",
            "psychotic\n",
            "pub\n",
            "public\n",
            "publicity\n",
            "published\n",
            "puerile\n",
            "puke\n",
            "pull\n",
            "pulled\n",
            "pulling\n",
            "pulp\n",
            "pulse\n",
            "pump\n",
            "pun\n",
            "punch\n",
            "punched\n",
            "punchline\n",
            "punished\n",
            "punishment\n",
            "punk\n",
            "pupil\n",
            "puppet\n",
            "puppy\n",
            "purchase\n",
            "purchased\n",
            "pure\n",
            "purely\n",
            "purist\n",
            "purple\n",
            "purpose\n",
            "purposely\n",
            "purse\n",
            "pursue\n",
            "pursued\n",
            "pursues\n",
            "pursuing\n",
            "pursuit\n",
            "push\n",
            "pushed\n",
            "pushing\n",
            "put\n",
            "putrid\n",
            "putting\n",
            "puzzle\n",
            "puzzled\n",
            "puzzling\n",
            "python\n",
            "q\n",
            "quaid\n",
            "quaint\n",
            "qualified\n",
            "qualifies\n",
            "qualify\n",
            "quality\n",
            "quantum\n",
            "quarter\n",
            "quasi\n",
            "queen\n",
            "quentin\n",
            "quest\n",
            "question\n",
            "questionable\n",
            "questioned\n",
            "questioning\n",
            "quibble\n",
            "quick\n",
            "quickly\n",
            "quiet\n",
            "quietly\n",
            "quinn\n",
            "quintessential\n",
            "quip\n",
            "quirk\n",
            "quirky\n",
            "quit\n",
            "quite\n",
            "quotable\n",
            "quote\n",
            "quoting\n",
            "r\n",
            "rabbit\n",
            "rabid\n",
            "race\n",
            "rachel\n",
            "racial\n",
            "racing\n",
            "racism\n",
            "racist\n",
            "rack\n",
            "radar\n",
            "radiation\n",
            "radical\n",
            "radio\n",
            "rag\n",
            "rage\n",
            "raging\n",
            "raid\n",
            "raider\n",
            "rail\n",
            "railroad\n",
            "rain\n",
            "rainbow\n",
            "raines\n",
            "rainy\n",
            "raise\n",
            "raised\n",
            "raising\n",
            "raj\n",
            "ralph\n",
            "ram\n",
            "ramble\n",
            "rambling\n",
            "rambo\n",
            "ramones\n",
            "rampage\n",
            "rampant\n",
            "ran\n",
            "ranch\n",
            "randall\n",
            "randolph\n",
            "random\n",
            "randomly\n",
            "randy\n",
            "range\n",
            "ranger\n",
            "ranging\n",
            "rank\n",
            "ranked\n",
            "ranking\n",
            "ransom\n",
            "rant\n",
            "ranting\n",
            "rap\n",
            "rape\n",
            "raped\n",
            "rapid\n",
            "rapidly\n",
            "raping\n",
            "rapist\n",
            "rapper\n",
            "rapture\n",
            "rare\n",
            "rarely\n",
            "rarity\n",
            "rat\n",
            "rate\n",
            "rated\n",
            "rather\n",
            "rating\n",
            "ratio\n",
            "rational\n",
            "raunchy\n",
            "rave\n",
            "raven\n",
            "raving\n",
            "ravishing\n",
            "raw\n",
            "ray\n",
            "raymond\n",
            "razor\n",
            "razzie\n",
            "rea\n",
            "reach\n",
            "reached\n",
            "reaching\n",
            "react\n",
            "reaction\n",
            "read\n",
            "reader\n",
            "readily\n",
            "reading\n",
            "ready\n",
            "reagan\n",
            "real\n",
            "realise\n",
            "realised\n",
            "realises\n",
            "realism\n",
            "realist\n",
            "realistic\n",
            "realistically\n",
            "reality\n",
            "realization\n",
            "realize\n",
            "realized\n",
            "realizes\n",
            "realizing\n",
            "really\n",
            "realm\n",
            "rear\n",
            "reason\n",
            "reasonable\n",
            "reasonably\n",
            "reasoning\n",
            "rebecca\n",
            "rebel\n",
            "rebellion\n",
            "rebellious\n",
            "recall\n",
            "recap\n",
            "recapture\n",
            "receive\n",
            "received\n",
            "receives\n",
            "receiving\n",
            "recent\n",
            "recently\n",
            "reception\n",
            "recipe\n",
            "reckless\n",
            "reckon\n",
            "recognise\n",
            "recognised\n",
            "recognition\n",
            "recognizable\n",
            "recognize\n",
            "recognized\n",
            "recommend\n",
            "recommendation\n",
            "recommended\n",
            "recommending\n",
            "record\n",
            "recorded\n",
            "recording\n",
            "recover\n",
            "recovered\n",
            "recovering\n",
            "recreate\n",
            "recreation\n",
            "recruit\n",
            "recurring\n",
            "recycled\n",
            "red\n",
            "redeem\n",
            "redeemed\n",
            "redeeming\n",
            "redemption\n",
            "redford\n",
            "redgrave\n",
            "redneck\n",
            "reduced\n",
            "redundant\n",
            "reed\n",
            "reef\n",
            "reek\n",
            "reel\n",
            "reese\n",
            "reeve\n",
            "refer\n",
            "reference\n",
            "referred\n",
            "referring\n",
            "refers\n",
            "refined\n",
            "reflect\n",
            "reflected\n",
            "reflecting\n",
            "reflection\n",
            "reflects\n",
            "refrain\n",
            "refreshing\n",
            "refuge\n",
            "refugee\n",
            "refund\n",
            "refuse\n",
            "refused\n",
            "refusing\n",
            "regain\n",
            "regard\n",
            "regarded\n",
            "regarding\n",
            "regardless\n",
            "regime\n",
            "reginald\n",
            "region\n",
            "register\n",
            "regret\n",
            "regular\n",
            "regularly\n",
            "rehash\n",
            "rehearsal\n",
            "reid\n",
            "reign\n",
            "reincarnation\n",
            "reiser\n",
            "reject\n",
            "rejected\n",
            "rejection\n",
            "relate\n",
            "related\n",
            "relates\n",
            "relating\n",
            "relation\n",
            "relationship\n",
            "relative\n",
            "relatively\n",
            "relax\n",
            "relaxed\n",
            "relaxing\n",
            "release\n",
            "released\n",
            "releasing\n",
            "relentless\n",
            "relentlessly\n",
            "relevance\n",
            "relevant\n",
            "reliable\n",
            "relic\n",
            "relied\n",
            "relief\n",
            "relies\n",
            "relieved\n",
            "religion\n",
            "religious\n",
            "relish\n",
            "reluctant\n",
            "reluctantly\n",
            "rely\n",
            "relying\n",
            "remade\n",
            "remain\n",
            "remainder\n",
            "remained\n",
            "remaining\n",
            "remains\n",
            "remake\n",
            "remark\n",
            "remarkable\n",
            "remarkably\n",
            "remember\n",
            "remembered\n",
            "remembering\n",
            "remembers\n",
            "remind\n",
            "reminded\n",
            "reminder\n",
            "reminding\n",
            "reminds\n",
            "reminiscent\n",
            "remorse\n",
            "remote\n",
            "remotely\n",
            "remove\n",
            "removed\n",
            "renaissance\n",
            "render\n",
            "rendered\n",
            "rendering\n",
            "rendition\n",
            "reno\n",
            "renowned\n",
            "rent\n",
            "rental\n",
            "rented\n",
            "renting\n",
            "repair\n",
            "repeat\n",
            "repeated\n",
            "repeatedly\n",
            "repeating\n",
            "repellent\n",
            "repertoire\n",
            "repetition\n",
            "repetitive\n",
            "replace\n",
            "replaced\n",
            "replacement\n",
            "replacing\n",
            "replay\n",
            "reply\n",
            "report\n",
            "reported\n",
            "reportedly\n",
            "reporter\n",
            "represent\n",
            "representation\n",
            "representative\n",
            "represented\n",
            "representing\n",
            "represents\n",
            "repressed\n",
            "republic\n",
            "republican\n",
            "repulsive\n",
            "reputation\n",
            "request\n",
            "require\n",
            "required\n",
            "requirement\n",
            "requires\n",
            "requisite\n",
            "rerun\n",
            "rescue\n",
            "rescued\n",
            "research\n",
            "researched\n",
            "researcher\n",
            "researching\n",
            "resemblance\n",
            "resemble\n",
            "resembled\n",
            "resembles\n",
            "resembling\n",
            "reservation\n",
            "reserved\n",
            "residence\n",
            "resident\n",
            "resist\n",
            "resistance\n",
            "resolution\n",
            "resolve\n",
            "resolved\n",
            "resonance\n",
            "resort\n",
            "resorting\n",
            "resource\n",
            "resourceful\n",
            "respect\n",
            "respectable\n",
            "respected\n",
            "respectful\n",
            "respective\n",
            "respectively\n",
            "respond\n",
            "responds\n",
            "response\n",
            "responsibility\n",
            "responsible\n",
            "rest\n",
            "restaurant\n",
            "restless\n",
            "restoration\n",
            "restore\n",
            "restored\n",
            "restrained\n",
            "restraint\n",
            "restricted\n",
            "restriction\n",
            "result\n",
            "resulted\n",
            "resulting\n",
            "resume\n",
            "resurrected\n",
            "resurrection\n",
            "retain\n",
            "retains\n",
            "retarded\n",
            "retelling\n",
            "retire\n",
            "retired\n",
            "retirement\n",
            "retread\n",
            "retreat\n",
            "retrieve\n",
            "retro\n",
            "retrospect\n",
            "return\n",
            "returned\n",
            "returning\n",
            "reunion\n",
            "reunite\n",
            "reunited\n",
            "reveal\n",
            "revealed\n",
            "revealing\n",
            "reveals\n",
            "revel\n",
            "revelation\n",
            "revenge\n",
            "reversal\n",
            "reverse\n",
            "reversed\n",
            "review\n",
            "reviewed\n",
            "reviewer\n",
            "reviewing\n",
            "revival\n",
            "revive\n",
            "revolt\n",
            "revolting\n",
            "revolution\n",
            "revolutionary\n",
            "revolve\n",
            "revolver\n",
            "revolves\n",
            "revolving\n",
            "reward\n",
            "rewarded\n",
            "rewarding\n",
            "rewind\n",
            "rewrite\n",
            "rex\n",
            "reynolds\n",
            "rhyme\n",
            "rhys\n",
            "rhythm\n",
            "rice\n",
            "rich\n",
            "richard\n",
            "richards\n",
            "richardson\n",
            "richly\n",
            "rick\n",
            "ricky\n",
            "rid\n",
            "ridden\n",
            "ride\n",
            "rider\n",
            "ridicule\n",
            "ridiculous\n",
            "ridiculously\n",
            "riding\n",
            "riff\n",
            "rifle\n",
            "right\n",
            "righteous\n",
            "rightfully\n",
            "rightly\n",
            "rigid\n",
            "ring\n",
            "ringing\n",
            "riot\n",
            "rip\n",
            "ripoff\n",
            "ripped\n",
            "ripper\n",
            "ripping\n",
            "rise\n",
            "rising\n",
            "risk\n",
            "risky\n",
            "rita\n",
            "ritchie\n",
            "ritter\n",
            "ritual\n",
            "rival\n",
            "rivalry\n",
            "river\n",
            "riveting\n",
            "rizzo\n",
            "rko\n",
            "roach\n",
            "road\n",
            "roaming\n",
            "roar\n",
            "roaring\n",
            "rob\n",
            "robbed\n",
            "robber\n",
            "robbery\n",
            "robbing\n",
            "robbins\n",
            "robert\n",
            "robertson\n",
            "robin\n",
            "robinson\n",
            "robot\n",
            "robotic\n",
            "rochester\n",
            "rock\n",
            "rocker\n",
            "rocket\n",
            "rocky\n",
            "rod\n",
            "rodriguez\n",
            "roger\n",
            "rogers\n",
            "rogue\n",
            "roland\n",
            "role\n",
            "roll\n",
            "rolled\n",
            "roller\n",
            "rolling\n",
            "rom\n",
            "roman\n",
            "romance\n",
            "romania\n",
            "romanian\n",
            "romantic\n",
            "rome\n",
            "romeo\n",
            "romero\n",
            "romp\n",
            "ron\n",
            "ronald\n",
            "roof\n",
            "rookie\n",
            "room\n",
            "roommate\n",
            "rooney\n",
            "root\n",
            "rooted\n",
            "rooting\n",
            "rope\n",
            "rosario\n",
            "rose\n",
            "rosemary\n",
            "ross\n",
            "rot\n",
            "roth\n",
            "rotten\n",
            "rotting\n",
            "rouge\n",
            "rough\n",
            "roughly\n",
            "round\n",
            "rounded\n",
            "rourke\n",
            "rousing\n",
            "route\n",
            "routine\n",
            "row\n",
            "rowlands\n",
            "roy\n",
            "royal\n",
            "royalty\n",
            "rub\n",
            "rubber\n",
            "rubbish\n",
            "ruby\n",
            "rudd\n",
            "rude\n",
            "rugged\n",
            "ruin\n",
            "ruined\n",
            "ruining\n",
            "rule\n",
            "ruled\n",
            "ruler\n",
            "rumor\n",
            "run\n",
            "runaway\n",
            "runner\n",
            "running\n",
            "runtime\n",
            "rupert\n",
            "rural\n",
            "rus\n",
            "rush\n",
            "rushed\n",
            "russell\n",
            "russia\n",
            "russian\n",
            "russo\n",
            "rusty\n",
            "ruth\n",
            "ruthless\n",
            "ryan\n",
            "sabotage\n",
            "sabrina\n",
            "sack\n",
            "sacred\n",
            "sacrifice\n",
            "sacrificed\n",
            "sacrificing\n",
            "sad\n",
            "saddest\n",
            "saddle\n",
            "sadistic\n",
            "sadly\n",
            "sadness\n",
            "safe\n",
            "safely\n",
            "safety\n",
            "saga\n",
            "said\n",
            "sailor\n",
            "saint\n",
            "sake\n",
            "sale\n",
            "salesman\n",
            "sally\n",
            "salman\n",
            "saloon\n",
            "salt\n",
            "salvage\n",
            "salvation\n",
            "sam\n",
            "samantha\n",
            "sammy\n",
            "sample\n",
            "samuel\n",
            "samurai\n",
            "san\n",
            "sand\n",
            "sander\n",
            "sandler\n",
            "sandra\n",
            "sandy\n",
            "sane\n",
            "sang\n",
            "sanity\n",
            "sans\n",
            "santa\n",
            "sap\n",
            "sappy\n",
            "sara\n",
            "sarah\n",
            "sarandon\n",
            "sarcasm\n",
            "sarcastic\n",
            "sardonic\n",
            "sassy\n",
            "sat\n",
            "satan\n",
            "satanic\n",
            "satellite\n",
            "satire\n",
            "satirical\n",
            "satisfaction\n",
            "satisfactory\n",
            "satisfied\n",
            "satisfy\n",
            "satisfying\n",
            "saturated\n",
            "saturday\n",
            "savage\n",
            "save\n",
            "saved\n",
            "saving\n",
            "savior\n",
            "savvy\n",
            "saw\n",
            "say\n",
            "saying\n",
            "scale\n",
            "scam\n",
            "scan\n",
            "scandal\n",
            "scantily\n",
            "scar\n",
            "scarcely\n",
            "scare\n",
            "scarecrow\n",
            "scared\n",
            "scarface\n",
            "scarier\n",
            "scariest\n",
            "scaring\n",
            "scarlett\n",
            "scarred\n",
            "scary\n",
            "scattered\n",
            "scenario\n",
            "scene\n",
            "scenery\n",
            "scenic\n",
            "schedule\n",
            "scheme\n",
            "scheming\n",
            "schlock\n",
            "schneider\n",
            "school\n",
            "schtick\n",
            "schwarzenegger\n",
            "sci\n",
            "science\n",
            "scientific\n",
            "scientist\n",
            "scifi\n",
            "scooby\n",
            "scoop\n",
            "scope\n",
            "score\n",
            "scored\n",
            "scoring\n",
            "scorpion\n",
            "scorsese\n",
            "scotland\n",
            "scott\n",
            "scottish\n",
            "scratch\n",
            "scratching\n",
            "scream\n",
            "screamed\n",
            "screaming\n",
            "screen\n",
            "screened\n",
            "screening\n",
            "screenplay\n",
            "screenwriter\n",
            "screw\n",
            "screwball\n",
            "screwed\n",
            "screwing\n",
            "script\n",
            "scripted\n",
            "scripting\n",
            "scriptwriter\n",
            "scrooge\n",
            "scum\n",
            "se\n",
            "sea\n",
            "seagal\n",
            "seal\n",
            "sean\n",
            "search\n",
            "searched\n",
            "searching\n",
            "season\n",
            "seasoned\n",
            "seat\n",
            "seattle\n",
            "sebastian\n",
            "secluded\n",
            "second\n",
            "secondary\n",
            "secondly\n",
            "secret\n",
            "secretary\n",
            "secretly\n",
            "section\n",
            "secure\n",
            "security\n",
            "seduce\n",
            "seduced\n",
            "seducing\n",
            "seduction\n",
            "seductive\n",
            "see\n",
            "seed\n",
            "seedy\n",
            "seeing\n",
            "seek\n",
            "seeking\n",
            "seem\n",
            "seemed\n",
            "seeming\n",
            "seemingly\n",
            "seems\n",
            "seen\n",
            "segal\n",
            "segment\n",
            "seinfeld\n",
            "seldom\n",
            "selected\n",
            "selection\n",
            "self\n",
            "selfish\n",
            "sell\n",
            "seller\n",
            "selling\n",
            "semblance\n",
            "semi\n",
            "sen\n",
            "senator\n",
            "send\n",
            "sending\n",
            "sends\n",
            "senior\n",
            "sens\n",
            "sensation\n",
            "sensational\n",
            "sense\n",
            "senseless\n",
            "sensibility\n",
            "sensible\n",
            "sensitive\n",
            "sensitivity\n",
            "sensual\n",
            "sent\n",
            "sentence\n",
            "sentenced\n",
            "sentiment\n",
            "sentimental\n",
            "sentimentality\n",
            "sentinel\n",
            "separate\n",
            "separated\n",
            "separation\n",
            "september\n",
            "sequel\n",
            "sequence\n",
            "serf\n",
            "sergeant\n",
            "sergio\n",
            "serial\n",
            "series\n",
            "serious\n",
            "seriously\n",
            "seriousness\n",
            "serum\n",
            "servant\n",
            "serve\n",
            "served\n",
            "service\n",
            "serviceable\n",
            "serving\n",
            "session\n",
            "set\n",
            "setting\n",
            "settle\n",
            "settled\n",
            "setup\n",
            "seuss\n",
            "seven\n",
            "seventh\n",
            "seventy\n",
            "several\n",
            "severe\n",
            "severed\n",
            "severely\n",
            "sewer\n",
            "sex\n",
            "sexist\n",
            "sexual\n",
            "sexuality\n",
            "sexually\n",
            "sexy\n",
            "seymour\n",
            "sf\n",
            "sfx\n",
            "sgt\n",
            "sh\n",
            "shack\n",
            "shade\n",
            "shadow\n",
            "shadowy\n",
            "shady\n",
            "shaft\n",
            "shah\n",
            "shake\n",
            "shakespeare\n",
            "shakespearean\n",
            "shaking\n",
            "shaky\n",
            "shall\n",
            "shallow\n",
            "shamble\n",
            "shame\n",
            "shameful\n",
            "shameless\n",
            "shamelessly\n",
            "shanghai\n",
            "shannon\n",
            "shape\n",
            "shaped\n",
            "share\n",
            "shared\n",
            "sharing\n",
            "shark\n",
            "sharon\n",
            "sharp\n",
            "shattered\n",
            "shattering\n",
            "shaun\n",
            "shaw\n",
            "shawshank\n",
            "shed\n",
            "sheen\n",
            "sheep\n",
            "sheer\n",
            "sheet\n",
            "shelf\n",
            "shell\n",
            "shelley\n",
            "shelter\n",
            "shenanigan\n",
            "shepard\n",
            "shepherd\n",
            "sheridan\n",
            "sheriff\n",
            "sherlock\n",
            "shield\n",
            "shift\n",
            "shifting\n",
            "shine\n",
            "shining\n",
            "shiny\n",
            "ship\n",
            "shirley\n",
            "shirt\n",
            "shiver\n",
            "shock\n",
            "shocked\n",
            "shocker\n",
            "shocking\n",
            "shockingly\n",
            "shoddy\n",
            "shoe\n",
            "shoestring\n",
            "shook\n",
            "shoot\n",
            "shooter\n",
            "shooting\n",
            "shootout\n",
            "shop\n",
            "shopping\n",
            "shore\n",
            "short\n",
            "shortcoming\n",
            "shorter\n",
            "shortly\n",
            "shot\n",
            "shotgun\n",
            "shoulder\n",
            "shout\n",
            "shouting\n",
            "shove\n",
            "shoved\n",
            "show\n",
            "showcase\n",
            "showdown\n",
            "showed\n",
            "shower\n",
            "showgirl\n",
            "showing\n",
            "shown\n",
            "showtime\n",
            "shred\n",
            "shrek\n",
            "shrill\n",
            "shrink\n",
            "shtick\n",
            "shudder\n",
            "shue\n",
            "shut\n",
            "shy\n",
            "sibling\n",
            "sick\n",
            "sickening\n",
            "sickly\n",
            "sickness\n",
            "sid\n",
            "side\n",
            "sided\n",
            "sidekick\n",
            "sidewalk\n",
            "sidney\n",
            "siege\n",
            "sigh\n",
            "sight\n",
            "sign\n",
            "signal\n",
            "signature\n",
            "signed\n",
            "significance\n",
            "significant\n",
            "significantly\n",
            "silence\n",
            "silent\n",
            "silhouette\n",
            "silliness\n",
            "silly\n",
            "silver\n",
            "silverman\n",
            "similar\n",
            "similarity\n",
            "similarly\n",
            "simmons\n",
            "simon\n",
            "simple\n",
            "simpler\n",
            "simplest\n",
            "simplicity\n",
            "simplistic\n",
            "simply\n",
            "simpson\n",
            "simultaneously\n",
            "sin\n",
            "sinatra\n",
            "since\n",
            "sincere\n",
            "sincerely\n",
            "sincerity\n",
            "sing\n",
            "singer\n",
            "singing\n",
            "single\n",
            "sings\n",
            "singular\n",
            "sinister\n",
            "sink\n",
            "sinking\n",
            "sir\n",
            "siren\n",
            "sirk\n",
            "sissy\n",
            "sister\n",
            "sit\n",
            "sitcom\n",
            "site\n",
            "sits\n",
            "sitting\n",
            "situation\n",
            "six\n",
            "sixteen\n",
            "sixth\n",
            "sixty\n",
            "size\n",
            "sized\n",
            "skeleton\n",
            "skeptical\n",
            "sketch\n",
            "skill\n",
            "skilled\n",
            "skillfully\n",
            "skimpy\n",
            "skin\n",
            "skinned\n",
            "skinny\n",
            "skip\n",
            "skipped\n",
            "skipping\n",
            "skirt\n",
            "skit\n",
            "skull\n",
            "sky\n",
            "slack\n",
            "slam\n",
            "slap\n",
            "slapped\n",
            "slapping\n",
            "slapstick\n",
            "slash\n",
            "slasher\n",
            "slaughter\n",
            "slaughtered\n",
            "slave\n",
            "slavery\n",
            "sleaze\n",
            "sleazy\n",
            "sleep\n",
            "sleeper\n",
            "sleeping\n",
            "sleepy\n",
            "sleeve\n",
            "slept\n",
            "sleuth\n",
            "slew\n",
            "slice\n",
            "slick\n",
            "slide\n",
            "slight\n",
            "slightest\n",
            "slightly\n",
            "slim\n",
            "slimy\n",
            "slip\n",
            "slipped\n",
            "slob\n",
            "sloppy\n",
            "slot\n",
            "slow\n",
            "slower\n",
            "slowly\n",
            "slows\n",
            "slug\n",
            "slum\n",
            "slut\n",
            "slutty\n",
            "sly\n",
            "smack\n",
            "small\n",
            "smaller\n",
            "smallest\n",
            "smarmy\n",
            "smart\n",
            "smarter\n",
            "smash\n",
            "smashed\n",
            "smashing\n",
            "smell\n",
            "smile\n",
            "smiling\n",
            "smirk\n",
            "smith\n",
            "smitten\n",
            "smoke\n",
            "smoking\n",
            "smooth\n",
            "smoothly\n",
            "smug\n",
            "smuggling\n",
            "snail\n",
            "snake\n",
            "snap\n",
            "snappy\n",
            "snatch\n",
            "sneak\n",
            "sneaking\n",
            "snipe\n",
            "sniper\n",
            "snippet\n",
            "snl\n",
            "snob\n",
            "snow\n",
            "snowman\n",
            "snowy\n",
            "snuff\n",
            "soap\n",
            "sober\n",
            "soccer\n",
            "social\n",
            "socialist\n",
            "socially\n",
            "societal\n",
            "society\n",
            "sociopath\n",
            "sock\n",
            "soderbergh\n",
            "soft\n",
            "sol\n",
            "sold\n",
            "soldier\n",
            "sole\n",
            "solely\n",
            "solid\n",
            "solo\n",
            "solution\n",
            "solve\n",
            "solved\n",
            "solving\n",
            "somber\n",
            "somebody\n",
            "someday\n",
            "somehow\n",
            "someone\n",
            "something\n",
            "somethings\n",
            "sometime\n",
            "sometimes\n",
            "somewhat\n",
            "somewhere\n",
            "son\n",
            "song\n",
            "sonny\n",
            "soon\n",
            "sooner\n",
            "soooo\n",
            "sophia\n",
            "sophie\n",
            "sophisticated\n",
            "sophistication\n",
            "sophomoric\n",
            "soprano\n",
            "sorcery\n",
            "sordid\n",
            "sore\n",
            "sorely\n",
            "sorrow\n",
            "sorry\n",
            "sort\n",
            "sorta\n",
            "sorvino\n",
            "sought\n",
            "soul\n",
            "soulless\n",
            "sound\n",
            "sounded\n",
            "sounding\n",
            "soundtrack\n",
            "soup\n",
            "sour\n",
            "source\n",
            "south\n",
            "southern\n",
            "soviet\n",
            "sox\n",
            "space\n",
            "spaceship\n",
            "spacey\n",
            "spade\n",
            "spaghetti\n",
            "spain\n",
            "span\n",
            "spanish\n",
            "spare\n",
            "spared\n",
            "spark\n",
            "sparkle\n",
            "sparse\n",
            "spawn\n",
            "spawned\n",
            "speak\n",
            "speaker\n",
            "speaking\n",
            "speaks\n",
            "spear\n",
            "special\n",
            "specially\n",
            "specie\n",
            "specific\n",
            "specifically\n",
            "spectacle\n",
            "spectacular\n",
            "spectacularly\n",
            "spectator\n",
            "spectrum\n",
            "speech\n",
            "speechless\n",
            "speed\n",
            "speeding\n",
            "spell\n",
            "spelled\n",
            "spelling\n",
            "spencer\n",
            "spend\n",
            "spending\n",
            "spends\n",
            "spent\n",
            "spice\n",
            "spider\n",
            "spielberg\n",
            "spike\n",
            "spill\n",
            "spin\n",
            "spinal\n",
            "spine\n",
            "spinning\n",
            "spinster\n",
            "spiral\n",
            "spirit\n",
            "spirited\n",
            "spiritual\n",
            "spit\n",
            "spite\n",
            "spitting\n",
            "splash\n",
            "splatter\n",
            "splendid\n",
            "spliced\n",
            "split\n",
            "splitting\n",
            "spock\n",
            "spoil\n",
            "spoiled\n",
            "spoiler\n",
            "spoiling\n",
            "spoke\n",
            "spoken\n",
            "spontaneous\n",
            "spoof\n",
            "spooky\n",
            "spoon\n",
            "sport\n",
            "sporting\n",
            "spot\n",
            "spotlight\n",
            "spotted\n",
            "spouse\n",
            "spout\n",
            "spray\n",
            "spread\n",
            "spreading\n",
            "spree\n",
            "spring\n",
            "springer\n",
            "sprinkled\n",
            "spy\n",
            "squad\n",
            "square\n",
            "squeamish\n",
            "squeeze\n",
            "sr\n",
            "st\n",
            "stab\n",
            "stabbed\n",
            "stabbing\n",
            "stable\n",
            "stack\n",
            "stadium\n",
            "staff\n",
            "stage\n",
            "staged\n",
            "staggering\n",
            "staging\n",
            "stair\n",
            "staircase\n",
            "stake\n",
            "stale\n",
            "stalk\n",
            "stalked\n",
            "stalker\n",
            "stalking\n",
            "stallone\n",
            "stalwart\n",
            "stan\n",
            "stance\n",
            "stand\n",
            "standard\n",
            "standing\n",
            "standout\n",
            "standpoint\n",
            "stanley\n",
            "stanwyck\n",
            "staple\n",
            "star\n",
            "stardom\n",
            "stardust\n",
            "stare\n",
            "stargate\n",
            "staring\n",
            "stark\n",
            "starlet\n",
            "starred\n",
            "starring\n",
            "starship\n",
            "start\n",
            "started\n",
            "starter\n",
            "starting\n",
            "startling\n",
            "starving\n",
            "state\n",
            "stated\n",
            "statement\n",
            "static\n",
            "stating\n",
            "station\n",
            "statue\n",
            "stature\n",
            "status\n",
            "stay\n",
            "stayed\n",
            "staying\n",
            "steady\n",
            "steal\n",
            "stealing\n",
            "steam\n",
            "steaming\n",
            "steamy\n",
            "steel\n",
            "steele\n",
            "steer\n",
            "stella\n",
            "stellar\n",
            "stem\n",
            "step\n",
            "stepfather\n",
            "stephanie\n",
            "stephen\n",
            "stepmother\n",
            "stepped\n",
            "stepping\n",
            "stereo\n",
            "stereotype\n",
            "stereotyped\n",
            "stereotypical\n",
            "stereotyping\n",
            "sterile\n",
            "sterling\n",
            "stern\n",
            "steve\n",
            "steven\n",
            "stevens\n",
            "stewart\n",
            "stick\n",
            "sticking\n",
            "stiff\n",
            "stile\n",
            "still\n",
            "stiller\n",
            "stilted\n",
            "stimulating\n",
            "sting\n",
            "stink\n",
            "stinker\n",
            "stir\n",
            "stirring\n",
            "stitch\n",
            "stock\n",
            "stoic\n",
            "stole\n",
            "stolen\n",
            "stomach\n",
            "stone\n",
            "stoned\n",
            "stoner\n",
            "stood\n",
            "stooge\n",
            "stoop\n",
            "stop\n",
            "stopped\n",
            "stopping\n",
            "store\n",
            "storm\n",
            "story\n",
            "storyline\n",
            "storyteller\n",
            "storytelling\n",
            "straight\n",
            "straightforward\n",
            "strain\n",
            "strained\n",
            "strand\n",
            "stranded\n",
            "strange\n",
            "strangely\n",
            "stranger\n",
            "strangest\n",
            "strategy\n",
            "straw\n",
            "stray\n",
            "streak\n",
            "stream\n",
            "streep\n",
            "street\n",
            "streisand\n",
            "strength\n",
            "stress\n",
            "stretch\n",
            "stretched\n",
            "stretching\n",
            "stricken\n",
            "strict\n",
            "strictly\n",
            "stride\n",
            "strike\n",
            "striking\n",
            "strikingly\n",
            "string\n",
            "strip\n",
            "stripped\n",
            "stripper\n",
            "stroke\n",
            "strong\n",
            "stronger\n",
            "strongest\n",
            "strongly\n",
            "struck\n",
            "structure\n",
            "structured\n",
            "struggle\n",
            "struggled\n",
            "struggling\n",
            "strung\n",
            "stuart\n",
            "stubborn\n",
            "stuck\n",
            "stud\n",
            "student\n",
            "studied\n",
            "studio\n",
            "study\n",
            "studying\n",
            "stuff\n",
            "stuffed\n",
            "stumble\n",
            "stumbled\n",
            "stumbling\n",
            "stunk\n",
            "stunned\n",
            "stunning\n",
            "stunningly\n",
            "stunt\n",
            "stupid\n",
            "stupidest\n",
            "stupidity\n",
            "stupidly\n",
            "style\n",
            "styled\n",
            "stylish\n",
            "stylistic\n",
            "stylized\n",
            "suave\n",
            "sub\n",
            "subconscious\n",
            "subdued\n",
            "subject\n",
            "subjected\n",
            "sublime\n",
            "submarine\n",
            "submit\n",
            "subplot\n",
            "subplots\n",
            "subsequent\n",
            "subsequently\n",
            "substance\n",
            "substantial\n",
            "substitute\n",
            "subtext\n",
            "subtitle\n",
            "subtitled\n",
            "subtle\n",
            "subtlety\n",
            "subtly\n",
            "suburb\n",
            "suburban\n",
            "subversive\n",
            "subway\n",
            "succeed\n",
            "succeeded\n",
            "succeeding\n",
            "succeeds\n",
            "success\n",
            "successful\n",
            "successfully\n",
            "succession\n",
            "successor\n",
            "suck\n",
            "sucked\n",
            "sucker\n",
            "sucking\n",
            "sudden\n",
            "suddenly\n",
            "sue\n",
            "suffer\n",
            "suffered\n",
            "suffering\n",
            "suffers\n",
            "suffice\n",
            "sufficient\n",
            "sufficiently\n",
            "sugar\n",
            "suggest\n",
            "suggested\n",
            "suggesting\n",
            "suggestion\n",
            "suggestive\n",
            "suggests\n",
            "suicide\n",
            "suit\n",
            "suitable\n",
            "suitably\n",
            "suite\n",
            "suited\n",
            "sullivan\n",
            "sum\n",
            "summarize\n",
            "summary\n",
            "summed\n",
            "summer\n",
            "sun\n",
            "sundance\n",
            "sunday\n",
            "sung\n",
            "sunk\n",
            "sunny\n",
            "sunrise\n",
            "sunset\n",
            "sunshine\n",
            "super\n",
            "superb\n",
            "superbly\n",
            "superficial\n",
            "superfluous\n",
            "superhero\n",
            "superior\n",
            "superlative\n",
            "superman\n",
            "supermarket\n",
            "supernatural\n",
            "superstar\n",
            "supply\n",
            "support\n",
            "supported\n",
            "supporter\n",
            "supporting\n",
            "supportive\n",
            "suppose\n",
            "supposed\n",
            "supposedly\n",
            "supreme\n",
            "sure\n",
            "surely\n",
            "surf\n",
            "surface\n",
            "surfer\n",
            "surfing\n",
            "surgeon\n",
            "surgery\n",
            "surpassed\n",
            "surpasses\n",
            "surprise\n",
            "surprised\n",
            "surprising\n",
            "surprisingly\n",
            "surreal\n",
            "surrealism\n",
            "surrender\n",
            "surrogate\n",
            "surround\n",
            "surrounded\n",
            "surrounding\n",
            "surroundings\n",
            "survival\n",
            "survive\n",
            "survived\n",
            "survives\n",
            "surviving\n",
            "survivor\n",
            "susan\n",
            "suspect\n",
            "suspected\n",
            "suspend\n",
            "suspended\n",
            "suspense\n",
            "suspenseful\n",
            "suspension\n",
            "suspicion\n",
            "suspicious\n",
            "sustain\n",
            "sutherland\n",
            "swallow\n",
            "swamp\n",
            "swear\n",
            "swearing\n",
            "sweat\n",
            "sweden\n",
            "swedish\n",
            "sweep\n",
            "sweeping\n",
            "sweet\n",
            "sweetheart\n",
            "sweetness\n",
            "swell\n",
            "swept\n",
            "swift\n",
            "swim\n",
            "swimming\n",
            "swing\n",
            "swinging\n",
            "swiss\n",
            "switch\n",
            "switched\n",
            "switching\n",
            "switzerland\n",
            "sword\n",
            "sydney\n",
            "sykes\n",
            "sylvia\n",
            "symbol\n",
            "symbolic\n",
            "symbolism\n",
            "sympathetic\n",
            "sympathize\n",
            "sympathy\n",
            "sync\n",
            "syndrome\n",
            "synopsis\n",
            "system\n",
            "ta\n",
            "table\n",
            "tabloid\n",
            "taboo\n",
            "tacked\n",
            "tackle\n",
            "tacky\n",
            "tactic\n",
            "tad\n",
            "tag\n",
            "tail\n",
            "tailor\n",
            "takashi\n",
            "take\n",
            "taken\n",
            "taking\n",
            "tale\n",
            "talent\n",
            "talented\n",
            "talentless\n",
            "talk\n",
            "talked\n",
            "talkie\n",
            "talking\n",
            "talky\n",
            "tall\n",
            "tame\n",
            "tango\n",
            "tank\n",
            "tap\n",
            "tape\n",
            "taped\n",
            "tara\n",
            "tarantino\n",
            "target\n",
            "targeted\n",
            "tarzan\n",
            "task\n",
            "taste\n",
            "tasteful\n",
            "tasteless\n",
            "tasty\n",
            "tattoo\n",
            "taught\n",
            "taut\n",
            "tax\n",
            "taxi\n",
            "taylor\n",
            "tcm\n",
            "te\n",
            "tea\n",
            "teach\n",
            "teacher\n",
            "teaching\n",
            "team\n",
            "teamed\n",
            "tear\n",
            "tearing\n",
            "tease\n",
            "tech\n",
            "technical\n",
            "technically\n",
            "technicolor\n",
            "technique\n",
            "technological\n",
            "technology\n",
            "ted\n",
            "teddy\n",
            "tedious\n",
            "tedium\n",
            "teen\n",
            "teenage\n",
            "teenager\n",
            "teeth\n",
            "telegraphed\n",
            "telephone\n",
            "television\n",
            "tell\n",
            "teller\n",
            "telling\n",
            "telly\n",
            "temper\n",
            "tempered\n",
            "temple\n",
            "temporary\n",
            "temptation\n",
            "tempted\n",
            "ten\n",
            "tenant\n",
            "tend\n",
            "tended\n",
            "tendency\n",
            "tender\n",
            "tenderness\n",
            "tends\n",
            "tense\n",
            "tension\n",
            "tent\n",
            "tepid\n",
            "term\n",
            "terminal\n",
            "terminator\n",
            "terrible\n",
            "terribly\n",
            "terrific\n",
            "terrified\n",
            "terrifying\n",
            "territory\n",
            "terror\n",
            "terrorism\n",
            "terrorist\n",
            "terry\n",
            "test\n",
            "testament\n",
            "tested\n",
            "testimony\n",
            "testing\n",
            "texas\n",
            "text\n",
            "textbook\n",
            "texture\n",
            "th\n",
            "thailand\n",
            "thank\n",
            "thankful\n",
            "thankfully\n",
            "thanks\n",
            "thats\n",
            "theater\n",
            "theatre\n",
            "theatrical\n",
            "theft\n",
            "thelma\n",
            "thematic\n",
            "thematically\n",
            "theme\n",
            "themed\n",
            "theodore\n",
            "theory\n",
            "therapist\n",
            "therapy\n",
            "thereafter\n",
            "thereby\n",
            "therefore\n",
            "therein\n",
            "thereof\n",
            "thespian\n",
            "thick\n",
            "thief\n",
            "thin\n",
            "thing\n",
            "think\n",
            "thinking\n",
            "thinly\n",
            "third\n",
            "thirteen\n",
            "thirty\n",
            "tho\n",
            "thomas\n",
            "thompson\n",
            "thorn\n",
            "thoroughly\n",
            "though\n",
            "thought\n",
            "thoughtful\n",
            "thousand\n",
            "thread\n",
            "threat\n",
            "threaten\n",
            "threatened\n",
            "threatening\n",
            "threatens\n",
            "three\n",
            "threw\n",
            "thrill\n",
            "thrilled\n",
            "thriller\n",
            "thrilling\n",
            "throat\n",
            "throne\n",
            "throughout\n",
            "throw\n",
            "throwaway\n",
            "throwback\n",
            "throwing\n",
            "thrown\n",
            "thru\n",
            "thrust\n",
            "thug\n",
            "thumb\n",
            "thunder\n",
            "thurman\n",
            "thus\n",
            "tick\n",
            "ticket\n",
            "tide\n",
            "tie\n",
            "tied\n",
            "tierney\n",
            "tiger\n",
            "tight\n",
            "tighter\n",
            "tightly\n",
            "til\n",
            "till\n",
            "tim\n",
            "timberlake\n",
            "time\n",
            "timed\n",
            "timeless\n",
            "timer\n",
            "timing\n",
            "timon\n",
            "timothy\n",
            "tin\n",
            "tina\n",
            "tiny\n",
            "tip\n",
            "tire\n",
            "tired\n",
            "tiresome\n",
            "tissue\n",
            "tit\n",
            "titanic\n",
            "title\n",
            "titled\n",
            "titular\n",
            "tobe\n",
            "toby\n",
            "today\n",
            "todd\n",
            "toddler\n",
            "toe\n",
            "together\n",
            "toilet\n",
            "token\n",
            "tokyo\n",
            "told\n",
            "tolerable\n",
            "tolerance\n",
            "tolerate\n",
            "toll\n",
            "tom\n",
            "tomato\n",
            "tomb\n",
            "tomei\n",
            "tommy\n",
            "tomorrow\n",
            "ton\n",
            "tone\n",
            "toned\n",
            "tongue\n",
            "toni\n",
            "tonight\n",
            "tony\n",
            "took\n",
            "tool\n",
            "toole\n",
            "tooth\n",
            "top\n",
            "topic\n",
            "topless\n",
            "topped\n",
            "torch\n",
            "torment\n",
            "tormented\n",
            "torn\n",
            "toro\n",
            "toronto\n",
            "torso\n",
            "torture\n",
            "tortured\n",
            "torturing\n",
            "toss\n",
            "tossed\n",
            "total\n",
            "totally\n",
            "touch\n",
            "touched\n",
            "touching\n",
            "tough\n",
            "tour\n",
            "tourist\n",
            "tow\n",
            "toward\n",
            "towards\n",
            "tower\n",
            "town\n",
            "townsfolk\n",
            "townspeople\n",
            "toxic\n",
            "toy\n",
            "trace\n",
            "track\n",
            "tracking\n",
            "tracy\n",
            "trade\n",
            "trademark\n",
            "trader\n",
            "tradition\n",
            "traditional\n",
            "traffic\n",
            "tragedy\n",
            "tragic\n",
            "tragically\n",
            "trail\n",
            "trailer\n",
            "train\n",
            "trained\n",
            "trainer\n",
            "training\n",
            "trait\n",
            "traitor\n",
            "tramp\n",
            "transcends\n",
            "transfer\n",
            "transferred\n",
            "transform\n",
            "transformation\n",
            "transformed\n",
            "transformer\n",
            "transforms\n",
            "transition\n",
            "translate\n",
            "translated\n",
            "translation\n",
            "transparent\n",
            "transplant\n",
            "transport\n",
            "transported\n",
            "transvestite\n",
            "trap\n",
            "trapped\n",
            "trapping\n",
            "trash\n",
            "trashed\n",
            "trashy\n",
            "trauma\n",
            "traumatic\n",
            "traumatized\n",
            "travel\n",
            "traveled\n",
            "traveler\n",
            "traveling\n",
            "travelling\n",
            "travesty\n",
            "travis\n",
            "travolta\n",
            "treasure\n",
            "treat\n",
            "treated\n",
            "treating\n",
            "treatment\n",
            "tree\n",
            "trek\n",
            "tremendous\n",
            "tremendously\n",
            "trend\n",
            "trendy\n",
            "trial\n",
            "triangle\n",
            "tribe\n",
            "tribulation\n",
            "tribute\n",
            "trick\n",
            "tricked\n",
            "tricky\n",
            "tried\n",
            "trier\n",
            "trigger\n",
            "trilogy\n",
            "trio\n",
            "trip\n",
            "tripe\n",
            "triple\n",
            "trite\n",
            "triumph\n",
            "trivia\n",
            "trivial\n",
            "troma\n",
            "troop\n",
            "trooper\n",
            "trouble\n",
            "troubled\n",
            "truck\n",
            "true\n",
            "truly\n",
            "truman\n",
            "trump\n",
            "trumpet\n",
            "trunk\n",
            "trust\n",
            "trusted\n",
            "truth\n",
            "truthful\n",
            "try\n",
            "trying\n",
            "tub\n",
            "tube\n",
            "tune\n",
            "tuned\n",
            "tunnel\n",
            "turd\n",
            "turgid\n",
            "turkey\n",
            "turkish\n",
            "turmoil\n",
            "turn\n",
            "turned\n",
            "turner\n",
            "turning\n",
            "turtle\n",
            "tv\n",
            "twelve\n",
            "twentieth\n",
            "twenty\n",
            "twice\n",
            "twilight\n",
            "twin\n",
            "twist\n",
            "twisted\n",
            "twisting\n",
            "two\n",
            "tyler\n",
            "type\n",
            "typical\n",
            "typically\n",
            "u\n",
            "uber\n",
            "ugh\n",
            "ugly\n",
            "uh\n",
            "uk\n",
            "ultimate\n",
            "ultimately\n",
            "ultimatum\n",
            "ultra\n",
            "um\n",
            "uma\n",
            "un\n",
            "unable\n",
            "unanswered\n",
            "unappealing\n",
            "unattractive\n",
            "unaware\n",
            "unbearable\n",
            "unbearably\n",
            "unbelievable\n",
            "unbelievably\n",
            "uncanny\n",
            "uncertain\n",
            "uncertainty\n",
            "uncle\n",
            "unclear\n",
            "uncomfortable\n",
            "unconscious\n",
            "unconventional\n",
            "unconvincing\n",
            "uncover\n",
            "uncredited\n",
            "uncut\n",
            "undead\n",
            "undeniable\n",
            "undeniably\n",
            "undercover\n",
            "underdeveloped\n",
            "underdog\n",
            "underground\n",
            "underlying\n",
            "underneath\n",
            "underrated\n",
            "understand\n",
            "understandable\n",
            "understandably\n",
            "understanding\n",
            "understands\n",
            "understated\n",
            "understatement\n",
            "understood\n",
            "undertone\n",
            "underused\n",
            "underwater\n",
            "underwear\n",
            "underworld\n",
            "undeveloped\n",
            "undoubtedly\n",
            "uneasy\n",
            "uneducated\n",
            "unemployed\n",
            "uneven\n",
            "unexpected\n",
            "unexpectedly\n",
            "unexplained\n",
            "unfair\n",
            "unfairly\n",
            "unfaithful\n",
            "unfamiliar\n",
            "unfinished\n",
            "unfold\n",
            "unfolding\n",
            "unfolds\n",
            "unforgettable\n",
            "unforgivable\n",
            "unfortunate\n",
            "unfortunately\n",
            "unfunny\n",
            "unhappy\n",
            "unhinged\n",
            "uniform\n",
            "uniformly\n",
            "unimaginative\n",
            "unimpressive\n",
            "uninspired\n",
            "uninspiring\n",
            "unintentional\n",
            "unintentionally\n",
            "uninteresting\n",
            "union\n",
            "unique\n",
            "uniquely\n",
            "unit\n",
            "united\n",
            "universal\n",
            "universally\n",
            "universe\n",
            "university\n",
            "unknown\n",
            "unleashed\n",
            "unless\n",
            "unlikable\n",
            "unlike\n",
            "unlikeable\n",
            "unlikely\n",
            "unlucky\n",
            "unnamed\n",
            "unnatural\n",
            "unnecessarily\n",
            "unnecessary\n",
            "unnerving\n",
            "unoriginal\n",
            "unpleasant\n",
            "unpredictable\n",
            "unrated\n",
            "unravel\n",
            "unreal\n",
            "unrealistic\n",
            "unrelated\n",
            "unremarkable\n",
            "unsatisfied\n",
            "unsatisfying\n",
            "unseen\n",
            "unsettling\n",
            "unstable\n",
            "unstoppable\n",
            "unsuccessful\n",
            "unsure\n",
            "unsuspecting\n",
            "unsympathetic\n",
            "untalented\n",
            "untrue\n",
            "unusual\n",
            "unusually\n",
            "unwatchable\n",
            "unwilling\n",
            "unwittingly\n",
            "upbeat\n",
            "upbringing\n",
            "upcoming\n",
            "update\n",
            "updated\n",
            "uplifting\n",
            "upon\n",
            "upper\n",
            "ups\n",
            "upset\n",
            "upsetting\n",
            "upside\n",
            "upstairs\n",
            "uptight\n",
            "urban\n",
            "urge\n",
            "urgency\n",
            "us\n",
            "usa\n",
            "usage\n",
            "use\n",
            "used\n",
            "useful\n",
            "useless\n",
            "user\n",
            "using\n",
            "ustinov\n",
            "usual\n",
            "usually\n",
            "utilized\n",
            "utter\n",
            "uttered\n",
            "utterly\n",
            "uwe\n",
            "v\n",
            "vacation\n",
            "vacuous\n",
            "vader\n",
            "vague\n",
            "vaguely\n",
            "vain\n",
            "val\n",
            "valentine\n",
            "valerie\n",
            "valid\n",
            "valley\n",
            "valuable\n",
            "value\n",
            "vampire\n",
            "van\n",
            "vance\n",
            "vanessa\n",
            "vanilla\n",
            "vanishes\n",
            "vanishing\n",
            "vanity\n",
            "vapid\n",
            "variation\n",
            "varied\n",
            "variety\n",
            "various\n",
            "varying\n",
            "vast\n",
            "vastly\n",
            "vaudeville\n",
            "vaughn\n",
            "vault\n",
            "vcr\n",
            "vega\n",
            "vehicle\n",
            "vein\n",
            "velvet\n",
            "vengeance\n",
            "vengeful\n",
            "venom\n",
            "venture\n",
            "vera\n",
            "verbal\n",
            "verbally\n",
            "verdict\n",
            "verge\n",
            "verhoeven\n",
            "versa\n",
            "versatile\n",
            "verse\n",
            "version\n",
            "versus\n",
            "vertigo\n",
            "vessel\n",
            "vet\n",
            "veteran\n",
            "vhs\n",
            "via\n",
            "vibe\n",
            "vibrant\n",
            "vice\n",
            "vicious\n",
            "victim\n",
            "victor\n",
            "victoria\n",
            "victorian\n",
            "victory\n",
            "video\n",
            "videotape\n",
            "vietnam\n",
            "view\n",
            "viewed\n",
            "viewer\n",
            "viewing\n",
            "viewpoint\n",
            "viggo\n",
            "vigilante\n",
            "vignette\n",
            "viking\n",
            "vile\n",
            "villa\n",
            "village\n",
            "villager\n",
            "villain\n",
            "villainous\n",
            "vince\n",
            "vincent\n",
            "vincenzo\n",
            "vintage\n",
            "violence\n",
            "violent\n",
            "violently\n",
            "violin\n",
            "virgin\n",
            "virginia\n",
            "virginity\n",
            "virtual\n",
            "virtually\n",
            "virtue\n",
            "virus\n",
            "visceral\n",
            "visible\n",
            "vision\n",
            "visionary\n",
            "visit\n",
            "visited\n",
            "visiting\n",
            "visitor\n",
            "vista\n",
            "visual\n",
            "visually\n",
            "visuals\n",
            "vital\n",
            "vivian\n",
            "vivid\n",
            "vividly\n",
            "vocal\n",
            "voice\n",
            "voiced\n",
            "void\n",
            "voight\n",
            "volume\n",
            "volunteer\n",
            "vomit\n",
            "von\n",
            "voodoo\n",
            "vote\n",
            "voted\n",
            "voter\n",
            "voting\n",
            "vow\n",
            "voyage\n",
            "voyager\n",
            "vu\n",
            "vulgar\n",
            "vulnerability\n",
            "vulnerable\n",
            "w\n",
            "wa\n",
            "wacky\n",
            "wage\n",
            "wagner\n",
            "wagon\n",
            "wait\n",
            "waited\n",
            "waiter\n",
            "waiting\n",
            "waitress\n",
            "wake\n",
            "waking\n",
            "wal\n",
            "walk\n",
            "walked\n",
            "walken\n",
            "walker\n",
            "walking\n",
            "wall\n",
            "wallace\n",
            "wallach\n",
            "wallet\n",
            "walsh\n",
            "walt\n",
            "walter\n",
            "wan\n",
            "wander\n",
            "wandering\n",
            "wanders\n",
            "wang\n",
            "wannabe\n",
            "want\n",
            "wanted\n",
            "wanting\n",
            "war\n",
            "ward\n",
            "warden\n",
            "wardrobe\n",
            "warehouse\n",
            "warfare\n",
            "warhol\n",
            "warm\n",
            "warming\n",
            "warmth\n",
            "warn\n",
            "warned\n",
            "warner\n",
            "warning\n",
            "warns\n",
            "warped\n",
            "warrant\n",
            "warren\n",
            "warrior\n",
            "wartime\n",
            "wash\n",
            "washed\n",
            "washing\n",
            "washington\n",
            "waste\n",
            "wasted\n",
            "wasteland\n",
            "wasting\n",
            "watch\n",
            "watchable\n",
            "watched\n",
            "watcher\n",
            "watching\n",
            "water\n",
            "watered\n",
            "waterfall\n",
            "waterfront\n",
            "watson\n",
            "wave\n",
            "waving\n",
            "wax\n",
            "way\n",
            "wayans\n",
            "wayne\n",
            "weak\n",
            "weaker\n",
            "weakest\n",
            "weakness\n",
            "wealth\n",
            "wealthy\n",
            "weapon\n",
            "wear\n",
            "wearing\n",
            "weary\n",
            "weather\n",
            "weave\n",
            "weaver\n",
            "web\n",
            "website\n",
            "wedding\n",
            "wee\n",
            "weed\n",
            "week\n",
            "weekend\n",
            "weekly\n",
            "weight\n",
            "weird\n",
            "weirdness\n",
            "weirdo\n",
            "welcome\n",
            "welcomed\n",
            "well\n",
            "welles\n",
            "wendy\n",
            "went\n",
            "werewolf\n",
            "werner\n",
            "wes\n",
            "wesley\n",
            "west\n",
            "western\n",
            "wet\n",
            "whacked\n",
            "whale\n",
            "whatever\n",
            "whats\n",
            "whatsoever\n",
            "wheel\n",
            "wheelchair\n",
            "whenever\n",
            "whereas\n",
            "wherein\n",
            "wherever\n",
            "whether\n",
            "whilst\n",
            "whim\n",
            "whimsical\n",
            "whine\n",
            "whining\n",
            "whiny\n",
            "whip\n",
            "whipped\n",
            "whisper\n",
            "white\n",
            "whoa\n",
            "whodunit\n",
            "whoever\n",
            "whole\n",
            "wholesome\n",
            "wholly\n",
            "whomever\n",
            "whoopi\n",
            "whore\n",
            "whose\n",
            "wicked\n",
            "wide\n",
            "widely\n",
            "wider\n",
            "widescreen\n",
            "widmark\n",
            "widow\n",
            "widowed\n",
            "widower\n",
            "wielding\n",
            "wife\n",
            "wig\n",
            "wild\n",
            "wilder\n",
            "wilderness\n",
            "wildly\n",
            "willard\n",
            "willed\n",
            "willem\n",
            "william\n",
            "williams\n",
            "willie\n",
            "willing\n",
            "willingly\n",
            "willis\n",
            "wilson\n",
            "wimp\n",
            "win\n",
            "winchester\n",
            "wind\n",
            "winded\n",
            "window\n",
            "wine\n",
            "wing\n",
            "winger\n",
            "wink\n",
            "winner\n",
            "winning\n",
            "winter\n",
            "wipe\n",
            "wiped\n",
            "wire\n",
            "wisdom\n",
            "wise\n",
            "wisely\n",
            "wiser\n",
            "wish\n",
            "wished\n",
            "wishing\n",
            "wit\n",
            "witch\n",
            "witchcraft\n",
            "witherspoon\n",
            "within\n",
            "without\n",
            "witless\n",
            "witness\n",
            "witnessed\n",
            "witnessing\n",
            "witted\n",
            "witty\n",
            "wizard\n",
            "woeful\n",
            "woefully\n",
            "woke\n",
            "wolf\n",
            "woman\n",
            "wonder\n",
            "wondered\n",
            "wonderful\n",
            "wonderfully\n",
            "wondering\n",
            "wonderland\n",
            "wondrous\n",
            "wong\n",
            "wont\n",
            "woo\n",
            "wood\n",
            "wooden\n",
            "woody\n",
            "word\n",
            "wore\n",
            "work\n",
            "worked\n",
            "worker\n",
            "working\n",
            "world\n",
            "worldwide\n",
            "worm\n",
            "worn\n",
            "worried\n",
            "worry\n",
            "worrying\n",
            "worse\n",
            "worship\n",
            "worst\n",
            "worth\n",
            "worthless\n",
            "worthwhile\n",
            "worthy\n",
            "would\n",
            "wound\n",
            "wounded\n",
            "woven\n",
            "wow\n",
            "wrap\n",
            "wrapped\n",
            "wrath\n",
            "wreck\n",
            "wrenching\n",
            "wrestler\n",
            "wrestling\n",
            "wretched\n",
            "wright\n",
            "wrist\n",
            "write\n",
            "writer\n",
            "writes\n",
            "writing\n",
            "written\n",
            "wrong\n",
            "wronged\n",
            "wrongly\n",
            "wrote\n",
            "wry\n",
            "wtf\n",
            "wu\n",
            "ww2\n",
            "wwi\n",
            "wwii\n",
            "www\n",
            "x\n",
            "ya\n",
            "yacht\n",
            "yahoo\n",
            "yankee\n",
            "yard\n",
            "yarn\n",
            "yawn\n",
            "yea\n",
            "yeah\n",
            "year\n",
            "yearning\n",
            "yell\n",
            "yelling\n",
            "yellow\n",
            "yep\n",
            "yes\n",
            "yesterday\n",
            "yet\n",
            "yikes\n",
            "york\n",
            "young\n",
            "younger\n",
            "youngest\n",
            "youngster\n",
            "youth\n",
            "youthful\n",
            "youtube\n",
            "yr\n",
            "yup\n",
            "yuppie\n",
            "z\n",
            "zane\n",
            "zany\n",
            "zealand\n",
            "zelah\n",
            "zero\n",
            "zeta\n",
            "zombie\n",
            "zone\n",
            "zoo\n",
            "zoom\n"
          ]
        }
      ],
      "source": [
        "for d in vectorizer.get_feature_names_out():\n",
        "  print(d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ux_nnFutFra"
      },
      "source": [
        "#### Without priors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "le7dJthgtFrb",
        "outputId": "32f647e5-f01e-463a-faf4-acb5520971da"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-44-3e54b638b994>:47: RuntimeWarning: divide by zero encountered in log\n",
            "  log_likelihood[n, c]= np.sum(np.log(np.power(prob,x_test)), axis=1)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.80976\n"
          ]
        }
      ],
      "source": [
        "model=MultinomialNaiveBayes()\n",
        "params=model.fit(X_train, y_train, 0,0,0,0)\n",
        "y_prob=model.predict(X_test)\n",
        "y_pred = np.argmax(y_prob, 1)\n",
        "print(\"Accuracy:\",model.evaluate_acc(y_pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Nk_oKs-tFrb"
      },
      "source": [
        "#### With priors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqS2942ytFrb",
        "outputId": "a62dcbf5-8f4b-4662-fdd0-2c2191179bc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.81364\n"
          ]
        }
      ],
      "source": [
        "model=MultinomialNaiveBayes()\n",
        "params=model.fit(X_train, y_train, 1,1,10,10)\n",
        "y_prob=model.predict(X_test)\n",
        "y_pred = np.argmax(y_prob, 1)\n",
        "print(\"Accuracy:\",model.evaluate_acc(y_pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#BERT model:"
      ],
      "metadata": {
        "id": "vaW_RKvhE7cP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tqI6f1WJhQR"
      },
      "source": [
        "Dataset load:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBoKIycUJgx9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "a0830fd63008425789c0c8c3f9927c58",
            "517873495e72416d83a2d895fc442745",
            "870b1c2f06fa47d48c5de2a17cb7061f",
            "b9ec0e0936ab44dca42acfe9ed86cc76",
            "54802b95bc4940af812cf11145ac6fdc",
            "9c65a3ac600047b4abb5087b1237e30c",
            "4c5b836002f54ecaaf5dfbac2aa19449",
            "98cdc804d62d48cca7b00c05dea058d0",
            "6406f4d0567a4a8bb196add3d174f72c",
            "80b7dbfbdcd64ca1a03747f558b13dda",
            "97f8e77feedd429c81b1c471e370a4d2",
            "5cf16c77459847e996be3491b77f501b",
            "224d54d31acb4cdebc4897a4272f035c",
            "91e24af853c7413c91357b68e1e3a6c3",
            "4dbda0f896e7460cb84819e649906034",
            "6d90e6666ea04a9d8789021a7f5e5d15",
            "c5db70e6643b4b7f847716b7f9472def",
            "6ad1ba3fd1b34e37814c7b6fee8d4b86",
            "e728cbd022324b6aadfd54ac15afcdf6",
            "bd6da42b12c84b1b85491ceaa0d38e0d",
            "0c410a1309d249208165ce06bbd2859d",
            "af40b1990b314bee808b43f11ef4d1e5",
            "5fcd121af809443b9dbff67e01fc2c22",
            "5e884fd397be49d896537256bf1e76fc",
            "2dd4b5a6bc5e4335a1c93d6af339981e",
            "b2431e6b4efd4a73b2f67fa68f965d2b",
            "eb66d832a3f24dfbbb2adebcfa7d5ceb",
            "d5608c1186f04541b90196b5f42b6a97",
            "15e2922530a74682939bcd022839290e",
            "0c2f3a56a87d449893c897176b031feb",
            "1842ff9b7eee43e1a65c4d1b302d0e27",
            "25bbb401783743aaae5212070e0acec3",
            "00e362aa147e49e4a9a1bff5897a4e5f",
            "beb8040d822d4c3f930f97c61ca8bff7",
            "a634199855e44a44bb66c698bfaa8b42",
            "b3787a1a968543ffaf7d45b2d64d6af5",
            "df525732c6eb4c09bdfd25e5835a64d0",
            "6e3c7d637aa64a088dbaf11185138f71",
            "17abc3b0f0cc41cfadb6604d9cc21a84",
            "3d827a556a60489bbc47803447560e50",
            "20b4f3559a6e435ab5a9adc01a2717d8",
            "dc221fd3f45a45d4a65dd40e539f5cfb",
            "059e901a25c84f7eb5e82d54df069d54",
            "15d2a1e8f21e42b8b5d75733648645f0",
            "b0c14e8ae49f4f7a950dfa0537f1ae7d",
            "deaae9ce221745b79c2a46f6fed7aef5",
            "f0fae9a67c7444018a9e3adec07bc923",
            "34d4e93183b04ef892300430cc3a9ad3",
            "767f38445eb641058e32aee450c23db1",
            "eb6b0689f87c4fbe86a6ae70c641c3a7",
            "a28f2b100c93494eabae7ac7d9b84a6f",
            "66c57555020f422bb06a06594b2003f3",
            "42a87a910216450987bd635b07a6cb57",
            "ed96f0ad4dde4a1a80fd4f9cf5ecb00e",
            "d2d20a7e2e4749b099562fff2e971247",
            "5db6b6a3e9bb45b79676c8809c1d0b5c",
            "2c8bde9f2e8f4de7896855211853fd19",
            "667a9b456e674725ad672fe33ee7094a",
            "f35f87d4663a42f2b9a14dc773a4c45a",
            "f427e6f3b3af433f897479381ef2cdad",
            "4bd671643875456491fcc3d8a1fdd8be",
            "4807675f959447ed8438566d38af4d7e",
            "eea065dc208649eaa43547ea201a55f2",
            "765c9f97cd3e4c20987e6f275852f2f6",
            "3a8df5039e1f4a39b3777b26d4fdf094",
            "ef0e5b0e8e194a85a0a8e29bdd2de342",
            "73eac624427147749a91b6912d0433e6",
            "ae68c05fc26c4654a2e91cf846d2ddcc",
            "8a19828f66624ea0bea2848d457c2a2e",
            "98443479c059496787ac8a64539415be",
            "2b7b52704c8849c39f2a02ea2c2352af",
            "9d4751404365457b9c17a62bf6f5b86b",
            "b6661a2279554f1e8e454389673a1ee8",
            "553359c1482e432abab93d29f44c6266",
            "973456b692dd4a578003f50c950ca31b",
            "1c8958478d8246b2ad4d240ca09331e4",
            "80c1dd865e614500bce5e230b354d868",
            "aa1a9c0f996f4dd5a057570e5c8630ef",
            "51ea123f017740ccb38f11a6ea54044f",
            "9d69c5d03697459480eb9a50a0f741b1",
            "5b33c147032f40b8a5795b4238c78fe6",
            "c98ce4fa103241edada82f3dbd7eec08",
            "42629c73bfe7448da84d60c28a217dd0",
            "8af02db3df9e442cb709444ddf7abe28",
            "cf0790a9998a42358fe53039f64d5db4",
            "e2e2595283db40ee895d55024871e966",
            "946fdd7118c043159e65394fb447c300",
            "f0fdc333f92c474b9c1c0c980bb10220",
            "1fce5e3cdbb04efba6c3b8b2dfc07bf0",
            "155eddcf6e774ee29058e31e1198c29b",
            "163112aa0be64a2c814a885dda182038",
            "c80e6ba39d874a01b618c092ea110bae",
            "989cfe1743e148329142d83ed819353f",
            "343b4767e9f44262be31ee16809e557b",
            "a1bd03da07fc4eed89771e0318ecb57c",
            "9cc0e4fb16784b4480ca3ff445066475",
            "3cbc9f3c92a0477eb2db26a73985e879",
            "63c6968aaf9d4d3c81ef9cba73f2fef8",
            "b2c38954a38546d4b3dca5ca95225d8f"
          ]
        },
        "outputId": "4e1a7f41-5c11-41ec-eb77-9b35bf8b40ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dl Completed...: 0 url [00:00, ? url/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a0830fd63008425789c0c8c3f9927c58"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dl Size...: 0 MiB [00:00, ? MiB/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5cf16c77459847e996be3491b77f501b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5fcd121af809443b9dbff67e01fc2c22"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train examples...: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "beb8040d822d4c3f930f97c61ca8bff7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Shuffling /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompletePUP4FX/imdb_reviews-train.tfrecord…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b0c14e8ae49f4f7a950dfa0537f1ae7d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test examples...: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5db6b6a3e9bb45b79676c8809c1d0b5c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Shuffling /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompletePUP4FX/imdb_reviews-test.tfrecord*…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "73eac624427147749a91b6912d0433e6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating unsupervised examples...: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aa1a9c0f996f4dd5a057570e5c8630ef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Shuffling /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompletePUP4FX/imdb_reviews-unsupervised.t…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1fce5e3cdbb04efba6c3b8b2dfc07bf0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset imdb_reviews downloaded and prepared to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\n"
          ]
        }
      ],
      "source": [
        "train_data, test_data = tfds.load(name=\"imdb_reviews\", split=[\"train\", \"test\"],\n",
        "                                  batch_size=-1, as_supervised=True)\n",
        "\n",
        "x_train, y_train = tfds.as_numpy(train_data)\n",
        "x_test, y_test = tfds.as_numpy(test_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55-_cjFvJmgt",
        "outputId": "0581deb8-c2e3-47bc-db1f-9db9d120e8a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input  Trainset shape:  (25000,)\n",
            "Output Trainset shape:  (25000,)\n",
            "Input  Testset  shape:  (25000,)\n",
            "Output Testset  shape:  (25000,)\n"
          ]
        }
      ],
      "source": [
        "print('Input  Trainset shape: ' ,x_train.shape)\n",
        "print('Output Trainset shape: ' ,y_train.shape)\n",
        "print('Input  Testset  shape: ' ,x_test.shape)\n",
        "print('Output Testset  shape: ' ,y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V14mntbdK5wk",
        "outputId": "be1223fa-2be8-4166-954d-b03f2b7268cb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\""
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "x_train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTeks-yFMsK-",
        "outputId": "eaa40a77-6a17-4cd6-dc4a-c4b3c7730b43"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "y_train[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSWokm4sRWn5"
      },
      "source": [
        "Use Kaggle code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sV0CD-M0RY1i",
        "outputId": "7e5a1894-92ad-457d-efcb-5dcb58a4ab6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (2.0.0+cu118)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch) (3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch) (3.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (16.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch_pretrained_bert\n",
            "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 KB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from pytorch_pretrained_bert) (2.27.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from pytorch_pretrained_bert) (1.22.4)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from pytorch_pretrained_bert) (2.0.0+cu118)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from pytorch_pretrained_bert) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from pytorch_pretrained_bert) (4.65.0)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.26.106-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 KB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (4.5.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.10.7)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (1.11.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (2.0.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=0.4.1->pytorch_pretrained_bert) (16.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=0.4.1->pytorch_pretrained_bert) (3.25.2)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 KB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting botocore<1.30.0,>=1.29.106\n",
            "  Downloading botocore-1.29.106-py3-none-any.whl (10.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.6/10.6 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->pytorch_pretrained_bert) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->pytorch_pretrained_bert) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->pytorch_pretrained_bert) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->pytorch_pretrained_bert) (2022.12.7)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.9/dist-packages (from botocore<1.30.0,>=1.29.106->boto3->pytorch_pretrained_bert) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=0.4.1->pytorch_pretrained_bert) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=0.4.1->pytorch_pretrained_bert) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.106->boto3->pytorch_pretrained_bert) (1.16.0)\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3, pytorch_pretrained_bert\n",
            "Successfully installed boto3-1.26.106 botocore-1.29.106 jmespath-1.0.1 pytorch_pretrained_bert-0.6.2 s3transfer-0.6.0\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import random as rn\n",
        "import pandas as pd\n",
        "!pip install torch\n",
        "import torch\n",
        "!pip install pytorch_pretrained_bert\n",
        "# from pytorch_pretrained_bert import BertModel\n",
        "from torch import nn\n",
        "# from torchnlp.datasets import imdb_dataset      # --> We are using our own uploaded dataset.\n",
        "from pytorch_pretrained_bert import BertTokenizer\n",
        "from keras import preprocessing\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.optim import Adam\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVaLMOg5RZNu"
      },
      "outputs": [],
      "source": [
        "#Initializing seed values to stabilize the outcomes.\n",
        "rn.seed(321)\n",
        "np.random.seed(321)\n",
        "torch.manual_seed(321)\n",
        "torch.cuda.manual_seed(321)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "TumQiHgGTOH7",
        "outputId": "48fa59b8-8f9c-4b0c-ad78-4c6130fdf631"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 700x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "sentences = [len(sent) for sent in x_train[0:2000]]\n",
        "\n",
        "plt.rcParams.update({'figure.figsize':(7,5), 'figure.dpi':100})\n",
        "plt.bar(range(1,2001), sentences, color = ['blue'])\n",
        "plt.gca().set(title='No. of characters in each sentence', xlabel='Number of sentence', ylabel='Number of Characters in each sentence');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85XXDSP5TfOh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97c0f4c5-9c6c-45a2-ccbf-31866632fe83"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1297.963"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "np.mean(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IenyfipU7c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a06ecf96-70b5-441a-f7af-1c0ebb5ee74c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 3986169.44B/s]\n"
          ]
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSZGsHAOlN0K"
      },
      "source": [
        "Prepare data for tokenize:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75uPYHcCaEJ4"
      },
      "outputs": [],
      "source": [
        "for i in range(len(x_train)):\n",
        "  x_train[i] = str(x_train[i])[2:-1]\n",
        "\n",
        "for i in range(len(x_test)):\n",
        "  x_test[i] = str(x_test[i])[2:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSKWSUQGZ-ZD"
      },
      "outputs": [],
      "source": [
        "train_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t) + ['[SEP]'], x_train))\n",
        "test_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t) + ['[SEP]'], x_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aV80trXrq_Gz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6c8f1be-ce80-4c2d-b6a9-0077f4d00673"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (803 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (626 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (771 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (821 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (755 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (886 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1430 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1436 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (640 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (635 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1526 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1643 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1108 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1061 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (603 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (759 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (517 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1313 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (902 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1087 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (805 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (515 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (616 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (538 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (775 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (602 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (770 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (831 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (613 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (656 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (667 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (626 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (784 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (696 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (741 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (792 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (806 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1039 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (900 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (590 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (658 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (536 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (994 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (673 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (576 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1076 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (907 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1330 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (606 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1006 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (539 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (514 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (731 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (755 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (591 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (934 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (710 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (592 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (750 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (780 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (589 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (786 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (631 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (802 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (594 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (647 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (773 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (668 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1468 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1724 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (541 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (800 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (785 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1190 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (514 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (922 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (573 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1211 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1028 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (525 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1125 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1085 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (794 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (533 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (534 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (740 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1163 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (603 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (558 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (756 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (810 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1000 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (643 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (525 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (584 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (523 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (944 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (728 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1040 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (964 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (938 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (521 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (569 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (543 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (641 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (808 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (640 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (755 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (555 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (643 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (552 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (620 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (577 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1055 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (637 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1318 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (776 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (583 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (540 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (625 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (603 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (696 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (726 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1270 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (515 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (788 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (841 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (529 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (565 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (532 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (520 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (631 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (829 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (718 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (650 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (847 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (666 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (707 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1324 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1061 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (842 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (595 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (585 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (699 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (756 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1178 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (746 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (960 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (630 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (722 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (684 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (919 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (556 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (515 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (667 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (672 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (516 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (935 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (698 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (661 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (714 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (556 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (783 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (591 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (616 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (672 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1338 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (516 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (665 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (618 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (676 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (771 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (798 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (824 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (939 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (728 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (571 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (655 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (744 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (885 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1132 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (521 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (595 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (614 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (829 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (643 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (571 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1240 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (694 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (585 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (776 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (672 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (545 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (531 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1254 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1120 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (832 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (740 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (552 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1108 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (534 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1394 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1063 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1181 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (769 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (575 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (733 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (723 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (777 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (537 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1428 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1248 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1262 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (626 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1144 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (832 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (632 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (860 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (882 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (741 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1278 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (538 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (624 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (845 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (560 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (790 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1113 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (732 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (728 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1334 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (589 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (573 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (523 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1029 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (985 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1195 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (599 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (925 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (521 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1173 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (784 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (549 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (588 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1171 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (737 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (621 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (709 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (751 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (817 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (581 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (566 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (582 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (774 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (532 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1313 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (946 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (575 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1115 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (691 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1530 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (577 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (895 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (642 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (761 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (890 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (642 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (609 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1030 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1327 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1214 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1238 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2130 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (529 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (572 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (605 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (723 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (547 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (836 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (986 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1010 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (731 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (660 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1161 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1002 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (669 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (682 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (590 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (596 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (541 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1429 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1315 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (541 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (525 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (557 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (787 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (697 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (953 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (856 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (733 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (602 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (620 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1211 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (964 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1252 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (692 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (569 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (903 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (536 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1261 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (707 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (561 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (589 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1237 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (514 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1069 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1514 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (716 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (969 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1239 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (755 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (529 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (902 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (591 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (543 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (650 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (518 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (583 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (690 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1210 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (566 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (533 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1168 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (560 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1047 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (689 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (527 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (802 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (530 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (939 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (841 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1393 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1029 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (594 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (634 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (613 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1108 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (726 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (789 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (641 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (692 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (542 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (551 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (647 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (689 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1111 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (976 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (648 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (831 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (881 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (763 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (776 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (638 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (604 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (893 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (889 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (652 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (523 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (667 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (649 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (597 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (753 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (601 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (742 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1681 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (692 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (688 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1372 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (828 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (795 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1144 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (560 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (652 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1343 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (654 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (675 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (648 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (827 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1879 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1590 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (665 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (901 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1158 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (992 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (611 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (759 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (595 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (568 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1218 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (699 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (623 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (870 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (570 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1388 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (906 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1122 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (702 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (542 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (980 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (878 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1129 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (577 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (921 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (838 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1332 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (754 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (650 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (518 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (986 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1121 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1189 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (537 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (671 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1206 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (620 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1316 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (693 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (600 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (567 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (803 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (963 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (849 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (600 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (626 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (742 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (670 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (809 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (539 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (687 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (739 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1110 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (577 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (936 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (585 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (696 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (582 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (551 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (547 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (765 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (580 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1021 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (805 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1258 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (588 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (556 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (686 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1147 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (575 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (549 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2353 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (527 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (574 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1368 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1683 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (914 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (941 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (564 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (543 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (637 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (536 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1005 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (640 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (587 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (592 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (876 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (611 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (613 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (602 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (675 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1002 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (563 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (548 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (675 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1072 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (582 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (576 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (833 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (936 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (694 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (993 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (522 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (548 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (541 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1230 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (614 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (590 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (597 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (587 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (547 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (830 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (604 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1162 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1108 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (620 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1255 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (702 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (625 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (810 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (639 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (600 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (695 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (687 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (931 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1169 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (787 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (665 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (539 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (577 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (816 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (633 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (632 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (531 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (762 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (561 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (834 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (748 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (812 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (517 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (948 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (564 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (766 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (756 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (523 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (706 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (624 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (816 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (537 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1036 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (955 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1013 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (983 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (513 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (830 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (660 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (740 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (815 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (607 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (584 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1019 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1300 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (791 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (555 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (549 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1238 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (799 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (845 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (543 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (529 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (992 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (721 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (650 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1121 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (616 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1002 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (574 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (718 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (526 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (748 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (891 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (515 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1038 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (836 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (899 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (575 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (966 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (580 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (641 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (551 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (887 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (553 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (560 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (534 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (611 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (537 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (550 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (651 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (550 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (629 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (569 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (791 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (651 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (611 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (669 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (728 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (796 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (614 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (612 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1024 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (744 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (803 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (954 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1063 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (602 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1230 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (691 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (582 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (819 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1049 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (587 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (519 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (692 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (551 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (723 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (689 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1298 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (670 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1020 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (656 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1222 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (750 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (973 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (976 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1341 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (831 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (908 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (778 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (865 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (596 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (633 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (640 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (874 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (651 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1103 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1231 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (615 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (517 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (706 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (869 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (717 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (916 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1517 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1158 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (697 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (863 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (673 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (522 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1300 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (785 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (796 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (526 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (950 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (588 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1340 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (539 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (692 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (661 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (539 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (657 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (584 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1221 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (532 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (967 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (776 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (994 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (620 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (960 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (557 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (695 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (544 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (702 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1251 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (757 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (515 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (759 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (683 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (734 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (517 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (704 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (543 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (727 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (747 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (624 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (518 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (555 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (535 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (659 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1440 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (893 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (580 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (793 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1221 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (940 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (719 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (900 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (784 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (892 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (643 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1071 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (661 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (850 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (708 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (652 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (615 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (563 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (687 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (737 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (824 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (637 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (515 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (578 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (588 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (866 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1137 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (623 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (722 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1344 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (534 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (555 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (660 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (600 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1226 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (660 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (731 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (669 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (955 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (597 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (637 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (792 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1010 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (807 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (591 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2490 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (588 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (561 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (728 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (666 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (607 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (732 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (780 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (730 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (720 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (901 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (538 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (611 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1069 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (646 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (791 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (750 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (516 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (615 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1179 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (788 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (742 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1126 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (944 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (639 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1084 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (737 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (542 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (758 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1145 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (957 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (833 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (660 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1377 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (612 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (764 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (568 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (541 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (543 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (515 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1154 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (747 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1398 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (764 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (728 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1238 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (716 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (636 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (554 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (725 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1311 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1481 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (776 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1120 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1020 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (869 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (936 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (762 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (972 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (517 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (574 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (888 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (665 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (705 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (888 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (569 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (514 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1260 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1047 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (640 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (564 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (613 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (734 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (531 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (793 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (960 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1023 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1262 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (719 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (570 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (525 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (753 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (893 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1273 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (599 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (617 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (647 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (926 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (775 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (518 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (622 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (908 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (561 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (629 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (615 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1075 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (534 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (635 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1020 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (638 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1157 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (721 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (742 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (708 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (970 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (675 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1266 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (729 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (689 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (779 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (876 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (632 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (713 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (615 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1298 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1090 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (527 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (680 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (521 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1191 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (747 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (590 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (552 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (540 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (837 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (517 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (821 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (539 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (585 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1172 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1025 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (517 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (925 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1106 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (591 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (553 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (600 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (527 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (525 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1090 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (673 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (844 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (541 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (572 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (751 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (707 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (636 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (518 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (526 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1412 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (550 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (651 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (645 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (565 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (936 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1175 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (548 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (622 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (592 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1356 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (522 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1052 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (610 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (688 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1055 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (589 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (974 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (631 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (891 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (759 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (606 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (575 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (828 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1316 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (593 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (559 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (694 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (986 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (775 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (541 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (527 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (524 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1072 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (584 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (608 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1067 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1009 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1046 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (694 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (818 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (685 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1327 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (518 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1354 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (555 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (523 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (646 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (742 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (749 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (778 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (911 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (562 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1369 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (802 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1029 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (583 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (660 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (975 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (519 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (640 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (862 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (818 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1042 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (651 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (556 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1296 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (531 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (943 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1245 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (522 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (947 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1082 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (881 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (850 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (538 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (523 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (732 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1102 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (759 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (695 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (595 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (659 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (707 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (522 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (994 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (721 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (581 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (629 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (658 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (837 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (738 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (640 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (550 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (559 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (706 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (812 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (706 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (549 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (756 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (996 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1000 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (947 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (555 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (612 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (803 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (557 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (600 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (727 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (712 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (626 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1284 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (532 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (900 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (695 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (748 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (553 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (775 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (869 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1200 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (594 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (556 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1254 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (751 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (637 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1192 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (814 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (750 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (627 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (591 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (550 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1426 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (524 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (660 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1140 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (852 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (621 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (847 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1219 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (624 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (906 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (824 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (850 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (820 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (848 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (690 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (963 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (693 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (734 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (953 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (638 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (658 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (819 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (635 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (622 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (602 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (890 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (773 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1436 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (601 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (622 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1122 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (679 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1006 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1193 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (652 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (797 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (891 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (540 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (582 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (978 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (585 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (705 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (932 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (651 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (946 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (547 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (529 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (850 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (793 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (960 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (690 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1192 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (570 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (515 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (813 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (959 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (589 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (752 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (568 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (556 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (622 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (546 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1339 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (556 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (977 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (577 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (900 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (786 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (523 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (779 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (539 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (681 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (518 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (620 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (643 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (673 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (836 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (866 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (724 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (532 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (902 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (947 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1292 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (545 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (587 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1152 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (975 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (518 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (526 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (632 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (811 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (802 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (627 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (514 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (671 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (603 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1205 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (981 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (599 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1232 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (751 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (584 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (610 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (802 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (941 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (525 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (514 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (831 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1151 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (538 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (811 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (532 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (528 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (972 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (670 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (679 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (863 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1035 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (789 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (831 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (742 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (562 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (664 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (632 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1363 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (942 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1109 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (618 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (555 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (661 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (646 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (748 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (521 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1299 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (576 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (640 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1009 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (517 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (697 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1015 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (648 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1195 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1005 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (889 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (873 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (579 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (838 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (815 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (729 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (553 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (893 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (633 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2266 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (674 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (578 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (704 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (774 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (613 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (698 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (516 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1250 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (635 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (590 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (529 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (837 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1074 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (774 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (516 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (549 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (540 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (629 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (545 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (682 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (957 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (655 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (642 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (819 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (651 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (695 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (604 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (632 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (631 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (793 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (559 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (586 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1139 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (687 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (729 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (797 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (597 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (832 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (649 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (729 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (534 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (972 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (670 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (619 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1225 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (599 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (591 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (625 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1071 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1164 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (686 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (560 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (666 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (563 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (821 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1336 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (584 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (898 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (949 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (985 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1223 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (659 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (540 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (612 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (737 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (722 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (593 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (999 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1370 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (601 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1234 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (574 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (518 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (834 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1374 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (902 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (851 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (825 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (551 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1095 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (589 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (566 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (596 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (581 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1074 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (882 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (887 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (747 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (537 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1295 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1253 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (936 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (600 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (749 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (651 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (519 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (724 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1361 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (694 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (846 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (629 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (554 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (662 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (837 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (595 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (557 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (619 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1034 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (583 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (545 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (748 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (562 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (616 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (561 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (671 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (687 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (777 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (527 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (590 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (570 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (831 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1045 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (765 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (666 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (566 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (629 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (580 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1305 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (541 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1278 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (676 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (535 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (950 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1092 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1299 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (544 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (544 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (645 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (563 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1727 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1213 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (651 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (532 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (785 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (716 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (621 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (629 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (536 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (931 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1204 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1272 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (615 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (692 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (639 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (579 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (686 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (661 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1227 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1217 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (542 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (710 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (574 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (616 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (883 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (661 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (655 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (726 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (629 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (517 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1277 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1144 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (716 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (864 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (568 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1483 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (594 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (910 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (616 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (576 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1019 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1101 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (734 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (885 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (566 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (527 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (924 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1063 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (563 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (843 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (845 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (710 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (775 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (714 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (718 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (606 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1031 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (734 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (706 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (567 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (827 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (692 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1145 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (713 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (546 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (697 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (818 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (542 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1453 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (515 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (541 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1211 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (537 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (632 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (984 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (708 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (825 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (641 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (541 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1084 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1250 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (768 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (920 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (708 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (641 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (903 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1281 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (820 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (576 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (716 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (844 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (526 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (806 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (714 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (585 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (854 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (668 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (662 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (818 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (763 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (872 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (554 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (640 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1019 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (925 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (635 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (770 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (582 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (674 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (772 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (527 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (643 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (902 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (835 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (978 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (556 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (699 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (826 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (553 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (658 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (586 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1190 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (803 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (673 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (593 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (566 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (729 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1365 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1272 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (590 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (621 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (554 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (974 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (606 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1099 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (535 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (779 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (559 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1241 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (529 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1023 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (908 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (668 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (570 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (601 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (686 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (638 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (856 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (586 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1121 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (676 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (561 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (723 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1044 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (713 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (700 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (547 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (729 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (622 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (593 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (532 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1204 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1019 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1387 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (518 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (743 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (539 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (531 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (759 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (674 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1169 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (603 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (627 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (513 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (624 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (970 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (829 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (596 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (519 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (861 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (860 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (737 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (784 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (595 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1390 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (558 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (534 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (968 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (722 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (539 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (555 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (634 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (710 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (534 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (529 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1065 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (521 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1271 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (822 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1166 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1038 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (927 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (895 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (563 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (882 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (527 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (624 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (696 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (969 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (648 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (565 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (547 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (766 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (897 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (556 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (672 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1104 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (529 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (549 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (614 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (904 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (854 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (643 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (776 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (746 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (719 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (734 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (811 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (529 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (594 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1179 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (749 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (522 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1039 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1034 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (774 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (825 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (514 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (553 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (590 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (754 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (571 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (628 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (764 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1323 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1421 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (802 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1135 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (971 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (945 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (561 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (806 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (628 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1290 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (636 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (598 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (690 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (556 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (635 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (544 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1264 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (579 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (519 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (700 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (964 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (605 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (691 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (874 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (988 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (730 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1098 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (671 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1146 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (554 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (833 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1005 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (567 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (567 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (547 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (619 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1161 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (676 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (636 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (526 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (525 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (571 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (544 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (526 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (525 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (667 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (623 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (708 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (603 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (538 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (749 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (701 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (602 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1079 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (669 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (531 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (942 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (720 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (557 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (774 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (709 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (632 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1361 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (979 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (794 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (734 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (643 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (818 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (578 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1347 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (810 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (554 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (822 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (520 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (635 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (698 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (601 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1186 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (540 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (788 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1191 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (517 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1190 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1119 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (806 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (606 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1289 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (799 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (552 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1219 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1164 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (726 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (578 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (520 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (604 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (554 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1047 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (669 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1282 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (602 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (561 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (645 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (572 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1012 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (649 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (559 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (759 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1111 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (643 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (543 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (793 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (637 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1019 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (951 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (623 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1366 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (637 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (567 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (740 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (678 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (592 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (616 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (730 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (587 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (520 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (607 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (690 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (517 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1420 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (567 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (798 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (620 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (517 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (898 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (525 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1179 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (781 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (529 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (607 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (540 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1381 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (589 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (852 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (638 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (751 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (723 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (989 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (887 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (529 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (762 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (577 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (662 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (735 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1234 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (606 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (561 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (931 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (807 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (660 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1153 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (834 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (552 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (522 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (667 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1009 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1080 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (761 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (513 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (597 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (575 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1312 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (900 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (522 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1430 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (519 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (577 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (542 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (557 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (728 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (528 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (611 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (742 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (900 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1277 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (624 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (543 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (660 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (993 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (518 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (943 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (568 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (553 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (677 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (531 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (906 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (704 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (784 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (579 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (722 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (923 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1159 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1123 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (736 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (712 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (517 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (755 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1420 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (689 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (875 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (859 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (785 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (738 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (599 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (540 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (534 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (573 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (844 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (520 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (898 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1202 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1317 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (850 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (622 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (960 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1214 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1027 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (889 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (562 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (644 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (704 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (686 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (539 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (608 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (723 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (888 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (743 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (524 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (649 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (520 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (515 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (692 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (615 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (633 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (532 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1335 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (831 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (721 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (541 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (613 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (682 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (612 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (669 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (675 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (612 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1146 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (979 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (890 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1149 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (905 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (667 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (570 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (868 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1138 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1309 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1197 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1368 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (646 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (847 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1033 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (814 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (931 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1299 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1014 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (518 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1400 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (690 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (932 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (728 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (721 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (642 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (544 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1200 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1310 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (594 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (592 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (572 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (758 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (816 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (742 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (987 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1005 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (697 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1258 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (796 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (675 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (800 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (796 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (817 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (572 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1264 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1122 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (527 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (563 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (616 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (596 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (531 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (700 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (518 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (618 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (690 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (686 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (593 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (963 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (662 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (974 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (524 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1083 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (706 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (690 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (579 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (562 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (559 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1388 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (556 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (515 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (586 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (555 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (623 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (689 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (766 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (536 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (755 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1165 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1037 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (729 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (521 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (773 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (971 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (929 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (746 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (544 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (848 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1059 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (657 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (991 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (929 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1309 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (824 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1096 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (569 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1499 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (532 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (920 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (561 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (654 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (672 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (726 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (783 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (588 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (831 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1094 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (535 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (791 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (857 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (640 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1001 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (761 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (738 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (521 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (714 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (963 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (954 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (538 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (920 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (734 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (513 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (600 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (598 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (876 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (748 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (558 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (777 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (571 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1020 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (540 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (674 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (719 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (522 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (651 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1414 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (558 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (525 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (959 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (602 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (703 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (581 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (579 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (612 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1010 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (965 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (685 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (559 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (594 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (603 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (619 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (784 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (746 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (592 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (520 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (874 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (623 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (601 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1129 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (540 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (765 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (624 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1176 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (664 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (631 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (583 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (655 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1006 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1163 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1239 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (576 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (600 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (959 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (572 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (961 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (586 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (758 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (524 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1283 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (935 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (533 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (797 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (948 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (519 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (545 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (759 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1399 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (578 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (621 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (616 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1008 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1448 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (928 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (714 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (892 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (578 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (914 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1428 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (540 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1210 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (642 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (897 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (525 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (583 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1318 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (615 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (636 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (642 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (694 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (791 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1072 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (642 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (597 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (756 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (658 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1260 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (877 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (648 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1247 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (586 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (585 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1110 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (561 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (529 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (528 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1172 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (658 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (572 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (723 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (808 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1229 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (566 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (598 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (549 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (547 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (761 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (809 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (784 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1014 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (516 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (702 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (975 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (861 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (940 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (954 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (645 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (520 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (522 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (583 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (654 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (760 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (580 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (633 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (517 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1382 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (696 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (529 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (749 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (675 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (918 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1436 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (695 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (880 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1357 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (544 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (527 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (626 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (530 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (618 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1139 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (742 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (766 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (803 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (561 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1313 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (885 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (813 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (629 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (598 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (587 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (524 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (602 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (829 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (532 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (753 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (694 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (514 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1057 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (834 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (529 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (558 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (638 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (562 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (617 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (682 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (697 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (596 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (516 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (846 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (633 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (691 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1718 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (679 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (633 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (533 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (550 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (714 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (533 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (728 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1408 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (804 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (553 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1273 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (976 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (789 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (513 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1040 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (721 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1133 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (546 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (854 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (617 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1471 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (714 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (672 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (661 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1145 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (687 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (996 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (528 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (647 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1274 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (870 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1118 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (822 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1382 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (543 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (825 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1362 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (583 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2326 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (745 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (677 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (573 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (605 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (532 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (794 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (673 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (637 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (517 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1411 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (523 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (956 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (769 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (877 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1193 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (546 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (855 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (681 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (616 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (628 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (571 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (930 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (617 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (623 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (549 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (651 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1091 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (679 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (603 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (534 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1123 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (984 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (687 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (631 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (791 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (727 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (658 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (738 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (827 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (772 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (693 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (583 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (652 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (515 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (752 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1251 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (691 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (561 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1062 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (672 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (957 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (573 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (766 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (699 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (850 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (690 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (811 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1067 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (688 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (528 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (907 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (846 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (516 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (519 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (575 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1104 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (581 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (697 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (975 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (719 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (991 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (630 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (671 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (591 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (945 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (654 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (603 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (580 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1405 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (631 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (776 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (560 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (577 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1304 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (620 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1398 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (750 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (996 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (548 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (542 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (961 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (546 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1300 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (680 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (626 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (564 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1332 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (797 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (668 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (628 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (813 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (653 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (534 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (895 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (833 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (658 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1010 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (525 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1000 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (925 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (577 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (688 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (645 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1110 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1078 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (634 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (642 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (865 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (726 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1065 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (608 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (621 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1293 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (998 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1116 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1317 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1292 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1068 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (669 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (920 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (745 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (551 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1413 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1044 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (829 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (533 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (830 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (647 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (920 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (548 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (891 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (598 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (514 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (813 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1072 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1209 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1429 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (651 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1078 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (705 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (866 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (632 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1906 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (527 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (857 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1000 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (523 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1150 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (576 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (768 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (950 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (673 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (671 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1364 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (598 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (615 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (633 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (526 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (630 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1038 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (534 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (580 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (642 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1552 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (552 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (686 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (565 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (534 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (597 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (666 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (656 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (622 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (687 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (616 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1129 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (515 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (613 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (604 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (626 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (749 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (674 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (693 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (733 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1370 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (569 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (533 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (622 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (658 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1317 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (761 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (543 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (810 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (815 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (582 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (928 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (797 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (730 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (826 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (646 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (753 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (825 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (537 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (813 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (589 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (762 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (904 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (531 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (850 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1124 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (755 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (717 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (753 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (676 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (692 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (631 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (614 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (518 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1217 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (740 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (642 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1033 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (863 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (891 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (732 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (580 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (696 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (713 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1321 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (601 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (804 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (523 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (690 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1201 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (755 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (550 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (536 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (572 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (880 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (729 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (554 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (587 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (603 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (608 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1161 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (513 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (920 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (557 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (573 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (646 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (520 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (621 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (538 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (831 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (558 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (766 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (648 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1106 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (847 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (531 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (546 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (752 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (992 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (679 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (708 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (602 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (543 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (672 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (614 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1088 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (550 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (861 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (902 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1294 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1265 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (653 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (929 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (549 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (700 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (662 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (661 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (600 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (780 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (786 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1163 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (762 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (637 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (575 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (768 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1130 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (730 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (848 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1052 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1305 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (667 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1085 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (556 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (554 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (736 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (673 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (656 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (900 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (747 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (607 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1032 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (729 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (833 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (772 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (523 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (612 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (716 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (572 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (521 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (689 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (637 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (589 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (755 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1034 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (560 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (956 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (804 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (515 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (686 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1274 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (866 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (613 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (611 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1270 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (730 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (544 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (519 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (854 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (572 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (607 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (596 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (571 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (861 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (712 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (744 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (734 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (927 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (610 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (564 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1144 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (541 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (644 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (542 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (735 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1238 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (764 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1391 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1141 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (575 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1325 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (550 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (520 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (673 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (698 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (996 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (661 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1089 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1410 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (605 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (515 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (538 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (637 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (683 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (822 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (535 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (560 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (808 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (518 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (727 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (645 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (792 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (680 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (683 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1200 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (541 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (575 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (595 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (662 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (609 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (928 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (589 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (516 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (548 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (856 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (659 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (844 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (527 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (618 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (574 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (530 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (522 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1006 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (691 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (665 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1018 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (908 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (735 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1056 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (532 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (854 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (614 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (943 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1100 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (663 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (674 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1380 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1087 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1064 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (571 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (976 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (943 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (536 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (815 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (583 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (536 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (685 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (582 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (833 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (538 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (574 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (626 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (518 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (708 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (873 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (688 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (561 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (620 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (779 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (519 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (549 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (619 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (543 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1274 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (652 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (924 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1204 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (535 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (590 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (971 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1138 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (863 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (784 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (539 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (764 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (729 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (584 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (569 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (847 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (535 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (588 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (575 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (647 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (558 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (647 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (812 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (872 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (761 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1345 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (625 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (765 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1070 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (612 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (677 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1067 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (730 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (997 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (821 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (997 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (696 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1053 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1318 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (585 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (547 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (584 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (590 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (691 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (874 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (777 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1260 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (696 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (668 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (622 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (900 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (647 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (724 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (953 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (762 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (834 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1222 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (702 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (899 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (540 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (568 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (576 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (888 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (748 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (932 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (522 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (540 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (766 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (551 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (956 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1062 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (834 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (913 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (613 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (677 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (675 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1072 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (539 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (522 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (876 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (697 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (515 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (641 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (964 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (651 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (641 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (831 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1555 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (575 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (845 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1046 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (669 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1148 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (602 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1154 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (919 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (576 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (823 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (526 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (778 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (538 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (592 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (885 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (700 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (595 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (944 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (587 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (541 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1229 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (621 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (635 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (648 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (731 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (982 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1224 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (546 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (596 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (644 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (605 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1007 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (588 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1026 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (517 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (521 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (618 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (589 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (591 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (570 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (532 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (656 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1020 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1156 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (584 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (579 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (556 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (987 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (574 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (753 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (605 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (565 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (539 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1110 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (543 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (932 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1046 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (638 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (564 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (907 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (682 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (552 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (688 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (775 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (765 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (780 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1177 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (550 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (911 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (556 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (765 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (915 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (808 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1021 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (679 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1077 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (636 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (801 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1267 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1114 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (973 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (521 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (568 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (734 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (522 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (998 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (963 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (624 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (624 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1360 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (588 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (712 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (935 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (934 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (529 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (668 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (798 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (854 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (715 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1192 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (572 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (971 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (602 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (693 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (817 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (692 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (773 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (816 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1423 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (761 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (842 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (538 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (521 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (524 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (535 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1168 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1493 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (549 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (631 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (661 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (614 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (541 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (642 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (930 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (3078 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (626 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (786 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (650 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (679 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1400 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (663 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (552 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (925 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (659 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1073 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (983 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (572 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (962 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1152 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (918 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (755 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (545 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (603 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1036 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (612 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (736 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (981 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (634 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (898 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (693 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (556 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (903 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (719 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (826 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (997 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1621 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (630 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (814 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (553 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (568 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (634 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (563 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (727 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (571 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (861 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (918 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (584 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (797 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (824 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (579 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (538 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (518 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (609 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (971 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (551 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (577 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (544 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1128 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (764 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1267 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (974 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (719 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (599 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (861 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (963 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (919 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (580 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (601 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (781 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (741 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (789 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2932 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (827 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (708 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (814 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (736 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (740 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (820 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (618 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (662 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (578 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1205 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1300 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (678 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (906 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (541 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (730 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (526 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (701 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (586 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1244 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1025 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (531 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (531 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (662 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (710 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (797 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (517 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (872 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (589 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (784 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1185 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (562 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (588 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (640 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (798 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (557 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (525 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1455 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (816 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (712 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (841 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (802 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (873 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (598 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (690 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (526 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (649 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (686 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (850 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1257 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (630 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (655 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (653 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (642 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1077 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (907 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (647 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (554 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (835 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1470 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1227 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (717 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (820 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1025 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (746 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (633 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (576 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (639 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (829 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (741 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (594 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (624 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (639 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (620 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (540 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1170 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (548 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (766 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1100 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1094 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (656 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (599 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (735 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1201 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (817 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (562 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (568 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (681 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (839 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1133 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1104 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (532 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (957 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (571 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (788 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (911 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1367 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (933 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (547 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (717 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1338 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (576 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1157 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1262 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (736 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1003 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1006 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (528 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (689 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (741 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1124 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (521 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (528 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (643 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (786 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (957 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (872 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (830 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (710 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (539 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (717 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (777 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (890 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (632 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (678 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1018 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (709 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (903 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (517 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (645 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1313 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (634 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1018 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (624 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (712 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1265 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (572 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (625 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (826 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (676 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (669 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1242 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (675 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (833 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (715 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (554 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1359 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (520 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (520 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (595 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (809 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (608 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (666 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (527 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (805 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (798 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (804 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1093 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (635 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (547 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (536 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (676 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (529 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (596 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (605 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (680 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (811 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1273 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1380 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1104 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1224 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1039 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (657 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (553 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (866 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1079 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (616 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (802 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (789 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (826 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (969 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (599 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (935 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (690 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (777 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (627 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (727 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (914 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (553 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (520 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (577 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (702 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (618 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (535 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (812 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (520 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (597 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (641 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1240 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (957 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1382 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1277 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1276 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1348 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (563 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (739 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (533 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (701 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (621 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (885 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (572 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1128 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (708 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (798 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (571 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (774 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1376 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (703 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (756 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (644 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1342 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (660 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1207 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1146 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (525 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (692 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (586 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (559 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (942 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (542 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (562 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (997 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (590 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (758 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (687 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1205 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (579 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1048 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1056 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (590 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (762 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1409 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (660 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (581 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (660 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1266 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (544 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (551 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (539 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (611 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (674 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (908 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (689 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (573 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (522 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (722 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (937 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (892 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (840 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (681 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (570 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (597 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (813 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (573 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (741 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (827 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (730 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (610 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (890 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (516 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (862 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (571 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (534 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (943 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (561 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (876 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (758 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (930 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (542 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1031 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (987 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (765 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1261 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (586 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (734 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (537 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (892 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (740 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (591 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (555 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (727 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (668 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1374 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (583 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (689 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (915 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (925 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (849 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (538 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (835 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (807 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (611 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1206 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (742 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1170 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (844 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (654 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (622 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (661 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (627 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (831 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (652 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (821 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (744 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (592 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (558 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (531 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1197 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (860 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1291 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (637 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1125 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (532 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (556 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (715 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (698 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (593 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (555 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (534 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (743 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (641 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (925 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (569 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (896 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (800 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (872 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (694 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (721 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (615 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (985 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1314 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (522 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (803 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (539 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (656 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (711 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (590 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (609 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1168 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (705 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (678 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (522 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (542 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (884 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1016 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (631 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (689 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (697 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1185 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (650 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (616 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (664 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (521 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (596 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (725 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (564 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (608 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (701 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (608 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (673 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (538 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (521 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (532 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (623 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (541 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (538 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (570 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (596 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (640 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (771 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1038 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (846 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (531 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (558 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (554 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (966 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (585 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (659 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1352 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (600 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1357 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (848 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1044 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (520 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (743 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (712 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (578 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (655 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (610 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (536 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (541 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (814 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1384 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (593 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (533 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1146 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (539 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1170 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (583 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (838 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (606 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (740 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (589 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (566 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1200 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (518 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (971 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (765 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1184 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1280 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (796 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (969 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (916 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1147 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1247 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1299 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (726 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (568 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1238 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (782 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (999 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1097 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (514 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (854 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (612 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (702 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (663 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (762 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (618 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1097 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (853 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (652 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (525 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (734 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1312 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (621 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (546 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (657 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (614 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (905 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (572 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1220 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (967 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (693 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (726 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (527 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (631 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1033 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (692 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (573 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1321 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (566 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (542 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (736 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (935 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (616 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (873 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (565 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (651 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (692 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (883 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (566 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (667 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1058 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (668 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1187 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (530 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (709 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (667 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (917 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (573 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1015 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (745 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (795 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (519 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1337 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (751 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (537 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (792 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (850 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (514 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (826 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1188 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (912 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (819 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (517 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (905 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (980 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (824 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (536 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (527 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (727 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (757 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1410 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1263 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (553 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (517 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (545 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (880 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (550 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (538 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (987 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1326 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (899 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (537 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (923 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (531 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (570 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (898 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (717 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (941 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1241 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (682 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (832 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (668 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (526 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (889 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (526 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1394 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (879 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (956 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (621 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1348 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (539 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (639 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (743 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (554 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1242 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (689 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (828 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (967 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (637 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (705 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (790 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (979 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (581 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (564 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1183 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (877 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (554 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (557 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (534 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (875 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (676 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (604 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (665 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (592 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (535 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (959 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (553 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (521 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1159 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (832 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1180 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (814 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (592 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (904 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1056 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (646 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1289 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (836 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (566 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (636 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (841 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (646 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (624 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (648 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (807 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (596 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (727 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (573 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1223 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (655 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (552 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (845 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (920 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (591 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (705 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (599 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (665 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (694 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (836 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (598 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (606 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (553 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (638 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (721 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (963 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (514 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (673 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (549 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (654 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (519 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (998 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (605 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (974 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (868 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (905 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (584 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (888 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (583 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (526 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (933 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (840 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1302 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (672 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (740 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (517 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (789 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (795 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (593 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (629 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (752 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (655 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (530 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1328 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (916 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (556 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (572 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (794 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (659 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1345 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (688 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (708 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1083 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1122 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (581 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (631 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (766 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (612 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (903 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (655 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (733 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (687 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (585 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (945 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1363 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (750 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (749 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (667 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (515 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (553 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (545 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (862 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (895 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (724 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (665 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (687 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (905 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (941 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (824 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (546 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (526 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1042 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1353 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (531 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (560 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (540 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (513 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1363 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1044 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (560 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (579 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (738 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1011 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (557 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (675 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (687 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (714 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (539 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (580 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (757 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (601 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (760 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (666 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (579 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (571 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (684 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (637 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1059 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (577 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (583 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (894 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (720 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1080 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (532 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (536 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (530 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1097 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (678 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (671 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (577 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (531 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (710 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1255 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1018 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (692 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (535 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1160 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (720 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (551 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1166 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (816 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (567 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (736 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (670 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (576 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (832 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (601 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (519 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (619 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1304 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (836 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (753 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (881 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (863 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (746 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (714 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (896 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (612 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1322 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (596 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (879 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (798 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (844 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (545 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (555 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1324 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1188 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1016 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (650 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (593 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (929 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (630 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (614 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (717 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (537 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (609 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1349 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (738 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (695 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (719 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (625 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (529 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (720 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (537 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (733 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1404 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (892 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (517 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1513 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (640 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (762 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (535 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (638 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1645 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1010 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (864 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (843 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (847 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (556 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (579 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (553 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (616 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (744 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (605 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (756 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1000 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (664 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1304 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (672 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1256 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (680 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (655 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (571 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (554 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (616 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (594 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (532 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (637 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (687 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (558 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1135 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1146 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (706 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (535 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (562 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (602 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (550 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (901 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (879 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1037 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (637 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (783 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (819 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (903 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (746 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (555 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (532 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1131 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (562 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (858 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (790 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (793 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (730 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1066 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1048 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (786 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (844 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (523 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (567 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (911 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (714 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (524 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1029 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (950 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (902 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (762 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (643 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (949 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (513 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (763 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (832 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1125 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (557 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (542 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (846 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (577 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1076 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (555 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (533 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1045 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (724 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (926 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (858 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1250 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (669 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1177 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (669 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (579 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1073 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (769 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1003 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (649 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1288 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (638 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (520 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1368 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1228 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (855 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (778 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (546 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (573 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1006 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (775 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (563 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (570 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (551 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (726 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (686 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (589 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (719 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (895 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (730 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (759 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (715 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (672 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (947 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (768 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (709 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (519 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (531 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1066 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (575 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (665 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (710 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (514 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (653 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1052 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (586 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (611 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (613 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (626 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (923 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1412 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (990 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (703 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1294 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (808 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (940 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (823 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1400 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (630 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (719 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (648 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (614 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (996 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (560 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (554 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (694 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1033 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (641 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1333 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (628 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (762 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (591 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (576 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (626 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (579 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1226 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (616 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (588 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (653 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (631 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1119 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (623 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (603 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1006 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (651 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (656 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (561 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (936 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (819 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (602 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (729 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1018 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (560 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (817 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (598 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (564 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (526 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (674 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (886 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (536 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (653 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1241 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (619 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (555 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (518 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (658 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (548 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (554 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (533 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (590 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (828 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1282 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (670 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (927 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (707 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (520 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (712 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (578 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (674 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1355 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (515 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1096 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (978 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (879 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (674 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (928 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (563 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (523 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1059 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1100 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (623 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (705 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (764 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (544 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (518 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1314 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (641 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (556 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (660 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1239 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (719 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (638 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (638 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1070 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (692 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (558 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (631 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1257 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (755 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (546 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (785 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (560 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (937 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (870 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (582 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (982 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1314 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (630 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (662 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (728 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (636 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (732 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (582 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1279 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (937 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (574 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (740 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (723 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (521 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (639 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (544 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (675 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (897 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (539 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (778 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1287 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (712 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (843 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (515 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (624 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (523 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (920 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1276 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1370 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (855 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (972 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1087 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (581 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (574 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (791 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1149 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (946 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1332 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (560 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (648 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (842 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (542 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (876 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (530 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (649 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (545 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (674 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (746 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (653 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (544 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (553 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (715 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1147 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (988 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (597 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (597 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (595 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (722 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (814 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1150 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (838 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (602 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1356 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (593 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1259 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (759 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (784 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (538 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (684 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (632 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (637 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (545 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (814 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1354 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (759 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1077 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (860 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (568 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (571 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (588 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (821 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (530 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1151 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1006 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (964 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1009 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1047 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1096 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (541 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (584 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (593 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (873 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (531 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (557 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (790 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1423 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (540 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1278 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (581 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (975 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (981 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (518 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (555 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1264 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (960 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1049 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (705 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (652 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (791 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (546 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (760 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (760 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (815 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (602 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (610 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (738 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (595 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (878 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (650 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (530 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (949 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (989 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (630 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (529 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (528 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (567 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (653 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (896 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1403 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1114 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1174 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (832 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (732 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (591 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (684 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (927 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (526 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (629 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (523 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (705 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (792 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (769 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1072 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (684 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (516 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (585 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (579 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (745 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (816 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (542 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (851 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (536 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1175 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (534 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (615 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (795 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (803 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (532 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (712 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1110 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (575 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (543 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (637 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (765 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (518 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (786 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (546 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1222 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (823 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (731 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (821 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (820 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (698 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (756 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (582 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (726 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (754 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (590 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (522 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1234 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (624 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (525 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (590 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1402 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (778 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1483 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (590 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (685 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (864 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (533 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1130 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (540 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (524 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (984 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (770 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (780 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1115 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (599 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (613 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (982 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (622 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (733 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1487 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (643 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (804 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1042 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (565 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (667 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1455 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (558 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (560 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (594 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (705 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (716 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (559 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (874 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (657 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (673 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (727 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (892 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (547 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (523 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (870 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (838 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (552 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (570 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (754 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (632 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (589 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (623 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (647 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (807 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (550 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (549 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (539 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (565 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (672 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (802 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (515 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (721 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (745 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1113 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (703 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (524 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (576 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1352 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (874 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (712 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (652 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (589 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1097 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (568 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1201 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1303 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (770 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (806 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (869 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (547 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1002 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1033 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1200 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (549 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (759 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (520 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1010 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (610 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (916 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1235 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (852 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (668 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (560 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (526 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (996 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (579 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (587 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (634 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (641 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (545 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (743 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (605 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (928 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (531 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (639 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (763 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (786 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (738 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (534 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (646 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (602 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (806 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (531 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1039 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (748 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1190 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (676 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (910 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (521 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (531 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (990 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (548 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (602 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (618 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (643 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1072 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1255 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (736 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (620 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (548 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (599 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (3225 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (637 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1395 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (647 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1108 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (640 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (528 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1108 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (616 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (651 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (635 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (646 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (636 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (891 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (791 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (722 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1322 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (649 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (576 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (547 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (619 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1183 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (585 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (652 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (577 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (522 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (568 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (558 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (582 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (579 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (781 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (987 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (663 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (560 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (633 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (518 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (534 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1137 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (715 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (520 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (936 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (751 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1251 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (578 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (532 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (619 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (610 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (704 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1227 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (586 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (692 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (849 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (643 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (674 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (541 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (647 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1025 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (816 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1059 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (694 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (850 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1433 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1174 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (596 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (745 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1007 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (618 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1110 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (578 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (918 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1088 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (514 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1141 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (535 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1090 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (830 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (863 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (676 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (624 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1075 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1191 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (643 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (544 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (811 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1063 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (645 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (629 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (536 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (669 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (699 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (630 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1146 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (580 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1037 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (697 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (691 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (730 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1248 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (518 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (612 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (926 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (657 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (704 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (632 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (955 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (559 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (792 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (527 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1001 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (786 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (738 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (533 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (784 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (954 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1645 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (549 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (846 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (926 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (531 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (607 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1049 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (659 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1123 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (595 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (822 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (587 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1120 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1420 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (528 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (555 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (721 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (659 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (728 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1054 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (568 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (601 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (609 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (648 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (543 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (702 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (741 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (741 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (902 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (513 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (695 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (539 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1296 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (775 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (843 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (968 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1826 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (782 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (637 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (916 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (752 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (637 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (539 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (781 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (546 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (663 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (694 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (749 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (537 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (995 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (656 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (825 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (715 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (695 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (553 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (774 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (583 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (679 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1220 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1119 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1224 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (672 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (564 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (805 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1283 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (588 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (707 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (595 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (978 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (530 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (809 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (534 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (576 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (769 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (589 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (637 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (726 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1310 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (763 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (625 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (713 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (622 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (649 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (538 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (740 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (639 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (549 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (989 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1009 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1044 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (791 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (634 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (815 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (628 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (758 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (597 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (639 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (558 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1249 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (802 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (527 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (542 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (610 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (940 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (647 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (723 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (644 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (878 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (791 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (609 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (600 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1012 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1195 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (528 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (716 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1134 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1028 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (819 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (609 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (657 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (563 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (743 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (597 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (910 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (639 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (586 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (748 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (694 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (651 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (637 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (777 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1170 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (851 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1090 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (830 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (521 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1056 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (642 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (762 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (569 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (729 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (619 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (608 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (750 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (839 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (592 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (715 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1008 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1110 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (833 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (697 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (736 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (730 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (719 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (621 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (597 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1147 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (760 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (589 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (570 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (742 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (517 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (900 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (571 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (784 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1389 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (759 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (604 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1010 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (955 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (746 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1348 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (556 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (786 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (592 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (881 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1218 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1279 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (646 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (924 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1011 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (540 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (561 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (815 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (585 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (531 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1105 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (746 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (754 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (622 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (515 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1019 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (542 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (546 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (638 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (614 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1197 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (567 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (650 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (645 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (588 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (773 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (751 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1159 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1454 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (787 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (767 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (624 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1109 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (657 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (544 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (908 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (551 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (530 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (705 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (591 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (547 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (948 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (746 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (563 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1385 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1282 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (626 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (674 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (692 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (613 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (527 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (588 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (525 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (588 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (737 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (514 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (576 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (599 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1295 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (803 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (804 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1062 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (536 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (635 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (563 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (857 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (764 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (567 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (602 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (890 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (726 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (821 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (572 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (761 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (740 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (720 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1003 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (608 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (754 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (789 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (773 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (582 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (629 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (701 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (966 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (674 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (687 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (871 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (631 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (533 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (519 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (693 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (657 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (543 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (912 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (602 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (832 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (859 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (668 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1258 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (551 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (608 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (921 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (767 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1046 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1042 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (523 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (621 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1470 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (940 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (648 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (529 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (844 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (518 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (725 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (545 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (581 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (637 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (752 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (783 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1111 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (704 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (717 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (546 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (938 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (789 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (589 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1251 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (826 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1160 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (936 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (807 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (604 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (549 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (621 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (578 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1375 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (617 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (574 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (717 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (543 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (545 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (708 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (674 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1117 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (623 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (951 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (514 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (764 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (777 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (626 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1079 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1063 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (629 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (618 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (559 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (841 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1009 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1068 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (935 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (579 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (755 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (863 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1157 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (578 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (746 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1015 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (595 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1225 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (579 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1431 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1030 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (735 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (927 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (558 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (742 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (870 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (877 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1332 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (566 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (732 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (690 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (758 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1338 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (540 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (573 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (515 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (553 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1091 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (580 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (564 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1040 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (515 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (522 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (986 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (805 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (517 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (541 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (591 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (652 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (780 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (569 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (613 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1075 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (569 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (671 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (829 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (751 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (919 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (615 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (980 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (890 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (802 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (517 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (657 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (867 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (530 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (567 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (576 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1032 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (670 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (738 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (774 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (591 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1284 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1266 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1129 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (905 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (683 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (649 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (531 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (835 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (902 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (820 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (577 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (567 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (583 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1382 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1121 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (550 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (794 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (530 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (897 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (827 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (545 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (523 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (699 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (852 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (791 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (638 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1174 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (781 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (699 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1187 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (683 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (586 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (562 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (575 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (774 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1374 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (710 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (730 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (529 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (793 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (783 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (616 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (929 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (956 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (682 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (530 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (809 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (557 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (906 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1145 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (616 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (552 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (638 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (749 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (541 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (694 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (543 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (650 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (710 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1209 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (551 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (518 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (767 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (587 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (605 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (838 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (959 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (551 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1130 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1059 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1114 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (970 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1414 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (717 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (791 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (538 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (774 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1006 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (655 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1097 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (540 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (920 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (581 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (566 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (907 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (529 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1214 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (830 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (630 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1195 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1305 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1222 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (787 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1322 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (809 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (562 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (594 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (601 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1003 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1050 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1375 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (708 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (537 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (754 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (547 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (880 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1122 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (685 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (738 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (539 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (892 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (747 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (739 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (943 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (834 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (518 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (670 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1167 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (853 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (959 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (731 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (691 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (540 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (902 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (986 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (519 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (697 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (687 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (720 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (553 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (532 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (617 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1172 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (611 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (739 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (764 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (784 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (537 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (870 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (691 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1241 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (692 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (713 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1141 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1306 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (634 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (698 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (810 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (619 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1540 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (852 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (822 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (562 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (799 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (710 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (578 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (553 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1090 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1044 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (744 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (564 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1250 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (991 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (782 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (863 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (649 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (826 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (913 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1159 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (571 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (654 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (591 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (558 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (865 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (547 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (587 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (554 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (617 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1075 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (532 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (580 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (760 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (835 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (523 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (547 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1047 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (594 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (656 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (849 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (915 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (531 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (898 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (515 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (620 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (927 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (793 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (648 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (650 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (632 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (660 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1312 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1047 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (536 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (525 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (553 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (725 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (514 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1288 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (554 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (575 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (634 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (731 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (682 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (652 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (576 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1081 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (524 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (714 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (515 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (539 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (664 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (690 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (546 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (766 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (624 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (852 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (712 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (529 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (599 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1019 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (616 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (750 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (869 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (611 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (552 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (599 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (665 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (575 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (805 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (690 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (525 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (701 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (852 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (941 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (682 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (646 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (855 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (787 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (790 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (514 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1048 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (513 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (960 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (568 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (569 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (549 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (572 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (712 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (626 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (735 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (657 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (667 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (582 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (761 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1344 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (549 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (857 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (686 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (900 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (601 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (542 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (702 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (555 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (615 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1145 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (535 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (561 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1166 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (819 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (765 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (618 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (634 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (723 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (702 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (835 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1255 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (928 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (839 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (659 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (536 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (863 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (553 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (588 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (582 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (572 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (657 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (576 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (922 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (564 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (539 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1306 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1298 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1426 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (685 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (800 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (927 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (639 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (586 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (646 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (577 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (695 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1070 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (655 > 512). Running this sequence through BERT will result in indexing errors\n"
          ]
        }
      ],
      "source": [
        "train_tokens_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, train_tokens)), maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
        "test_tokens_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, test_tokens)), maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRlqEzrasRiJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2eb12d1-5fa8-4cd3-ae71-20e66c863de9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(512,)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "train_tokens_ids[100].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFwfNyHNwgq5"
      },
      "outputs": [],
      "source": [
        "train_masks = [[float(i > 0) for i in ii] for ii in train_tokens_ids]\n",
        "test_masks =  [[float(i > 0) for i in ii] for ii in test_tokens_ids]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgKjqXao0HHX"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define BERT"
      ],
      "metadata": {
        "id": "1Ic587drDcjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bertviz\n",
        "from bertviz import head_view, model_view"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLtbRg7eEKhX",
        "outputId": "a7055a37-66d8-4b06-e523-a0e93fed8756"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bertviz\n",
            "  Downloading bertviz-1.4.0-py3-none-any.whl (157 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.6/157.6 KB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.0 in /usr/local/lib/python3.9/dist-packages (from bertviz) (2.0.0+cu118)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.9/dist-packages (from bertviz) (1.26.106)\n",
            "Collecting transformers>=2.0\n",
            "  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from bertviz) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from bertviz) (4.65.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from bertviz) (2022.10.31)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.0->bertviz) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.0->bertviz) (2.0.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.0->bertviz) (3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.0->bertviz) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.0->bertviz) (3.10.7)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.0->bertviz) (1.11.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.0->bertviz) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.0->bertviz) (16.0.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m96.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers>=2.0->bertviz) (23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers>=2.0->bertviz) (1.22.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers>=2.0->bertviz) (6.0)\n",
            "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from boto3->bertviz) (0.6.0)\n",
            "Requirement already satisfied: botocore<1.30.0,>=1.29.106 in /usr/local/lib/python3.9/dist-packages (from boto3->bertviz) (1.29.106)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.9/dist-packages (from boto3->bertviz) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->bertviz) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->bertviz) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->bertviz) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->bertviz) (2022.12.7)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.9/dist-packages (from botocore<1.30.0,>=1.29.106->boto3->bertviz) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.0->bertviz) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.0->bertviz) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.106->boto3->bertviz) (1.16.0)\n",
            "Installing collected packages: tokenizers, sentencepiece, huggingface-hub, transformers, bertviz\n",
            "Successfully installed bertviz-1.4.0 huggingface-hub-0.13.3 sentencepiece-0.1.97 tokenizers-0.13.3 transformers-4.27.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "from transformers import BertConfig, BertModel\n",
        "\n",
        "configuration = BertConfig(is_decoder = True)\n",
        "# Initializing a model (with random weights) from the bert-base-uncased style configuration\n",
        "model = BertModel.from_pretrained('bert-base-uncased', output_attentions=True)\n",
        "# Accessing the model configuration\n",
        "configuration = model.config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431,
          "referenced_widgets": [
            "69961b9ba66447f586b2d1cf2bb321b8",
            "6482373b405649cab9b9fae0d91c60b2",
            "d86734fef1184497a99c2a12ea213baf",
            "83f62b93d4324b17a4daf10bc7d1c301",
            "01607773d6e4474ea9caeb8598bb73c9",
            "ae8dce2ad10c421bad5e50f02b28822c",
            "0f155dcd788940f6837f17aa36da2c74",
            "d287ec25957d4759ab623f0cbb4f5b4e",
            "e7fbcb1fe66e42e9af9d7f26a7bef1bf",
            "81cbc62f59f9412482a947bf8273a19c",
            "11620abab2854942a367ca43142b8404",
            "bf315c3903c84cefbf281f3c394a19c6",
            "2bb9be2cd7674c4da88e080d14f93716",
            "920ac69f2c39496382b2f30c8f2fabc3",
            "2cc3686ca8bc44cfad437213ccdfe8ec",
            "9f2d20348c0649aba9c0989d1659d59f",
            "306b5dc379ba4543af094a23cbcf5707",
            "7caeb745362b4f89b661560b0ec33562",
            "704ddbcb46e74281a3f37604db11cb6f",
            "884ca4e0d7844f6995da59148b82598c",
            "404c716effee4d7f85ac4272e545063a",
            "e2000c408dd34b4ab5dad2b5b33c1913"
          ]
        },
        "id": "kXvA15eDLI1e",
        "outputId": "29eea4af-bf0a-4c1d-d7ea-36145682abd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "69961b9ba66447f586b2d1cf2bb321b8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bf315c3903c84cefbf281f3c394a19c6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ui9XNUa0Zod"
      },
      "outputs": [],
      "source": [
        "class BertBinaryClassifier(nn.Module):\n",
        "    def __init__(self, dropout=0.1):\n",
        "        super(BertBinaryClassifier, self).__init__()\n",
        "\n",
        "        #self.bert = BertModel.from_pretrained('bert-base-uncased', config = config)\n",
        "        self.bert = model\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear = nn.Linear(768, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, tokens, masks=None):\n",
        "        pooled_output = self.bert(tokens, attention_mask=masks) #, output_all_encoded_layers=True\n",
        "        dropout_output = self.dropout(pooled_output[1])\n",
        "        linear_output = self.linear(dropout_output)\n",
        "        proba = self.sigmoid(linear_output)\n",
        "        return proba, pooled_output[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9kJqIVP2RJE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03008b62-1308-423d-8b67-8be06c0f55da"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8Rh25Ei2ist"
      },
      "outputs": [],
      "source": [
        "bert_clf = BertBinaryClassifier()\n",
        "bert_clf = bert_clf.cuda()     # running BERT on CUDA_GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxiTrnUYZeA3"
      },
      "source": [
        "Tune BERT:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bl11_CHdbzbv"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 4\n",
        "EPOCHS = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0cJJjPnZfaM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "ef2f52de-1aca-4772-f7de-2504502e82f6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'439.07328M'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "train_tokens_tensor = torch.tensor(train_tokens_ids)\n",
        "train_y_tensor = torch.tensor(y_train.reshape(-1, 1)).float()\n",
        "\n",
        "test_tokens_tensor = torch.tensor(test_tokens_ids)\n",
        "test_y_tensor = torch.tensor(y_test.reshape(-1, 1)).float()\n",
        "\n",
        "train_masks_tensor = torch.tensor(train_masks)\n",
        "test_masks_tensor = torch.tensor(test_masks)\n",
        "\n",
        "str(torch.cuda.memory_allocated(device)/1000000 ) + 'M'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7f3oqP8bvZV"
      },
      "outputs": [],
      "source": [
        "train_dataset = TensorDataset(train_tokens_tensor, train_masks_tensor, train_y_tensor)\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "test_dataset = TensorDataset(test_tokens_tensor, test_masks_tensor, test_y_tensor)\n",
        "test_sampler = SequentialSampler(test_dataset)\n",
        "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVqY4MU-bxHe"
      },
      "outputs": [],
      "source": [
        "param_optimizer = list(bert_clf.sigmoid.named_parameters())\n",
        "optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUzYjY_Ob5P2"
      },
      "outputs": [],
      "source": [
        "optimizer = Adam(bert_clf.parameters(), lr=3e-6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQ37hWtsb7QN"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaNgaVi0b9gE"
      },
      "outputs": [],
      "source": [
        "for epoch_num in range(EPOCHS):\n",
        "    bert_clf.train()\n",
        "    train_loss = 0\n",
        "    for step_num, batch_data in enumerate(train_dataloader):\n",
        "        token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n",
        "        print(str(torch.cuda.memory_allocated(device)/1000000 ) + 'M')\n",
        "        logits, attention = bert_clf(token_ids, masks)\n",
        "\n",
        "        loss_func = nn.BCELoss()\n",
        "\n",
        "        batch_loss = loss_func(logits, labels)\n",
        "        train_loss += batch_loss.item()\n",
        "\n",
        "\n",
        "        bert_clf.zero_grad()\n",
        "        batch_loss.backward()\n",
        "\n",
        "\n",
        "        clip_grad_norm_(parameters=bert_clf.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        clear_output(wait=True)\n",
        "        print('Epoch: ', epoch_num + 1)\n",
        "        print(\"\\r\" + \"{0}/{1} loss: {2} \".format(step_num, len(train_data) / BATCH_SIZE, train_loss / (step_num + 1)))\n",
        "\n",
        "        # attention = torch.stack(attention, dim=0).squeeze()\n",
        "        # attention = attention.mean(dim=1)\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTFeLWVHcE3v"
      },
      "outputs": [],
      "source": [
        "bert_clf.eval()\n",
        "bert_predicted = []\n",
        "attention_eval = []\n",
        "all_logits = []\n",
        "j = 0\n",
        "with torch.no_grad():\n",
        "    for step_num, batch_data in enumerate(test_dataloader):\n",
        "\n",
        "        token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n",
        "\n",
        "        logits = bert_clf(token_ids, masks)\n",
        "        loss_func = nn.BCELoss()\n",
        "        loss = loss_func(logits[0], labels)\n",
        "        numpy_logits = logits[0].cpu().detach().numpy()\n",
        "\n",
        "        bert_predicted += list(numpy_logits[:, 0] > 0.5)\n",
        "\n",
        "        if j < 20:\n",
        "          for i in range(4):\n",
        "            attention_eval.append(logits[-1][-1][i][-3].cpu().detach().numpy())\n",
        "          j = j + 1\n",
        "\n",
        "        all_logits += list(numpy_logits[:, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Without Fine-tuning pre-trained weights:"
      ],
      "metadata": {
        "id": "J_OBZUw_RtfX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, bert_predicted))"
      ],
      "metadata": {
        "id": "Jd9J3f9-MVsH",
        "outputId": "8b297be3-ca29-4aa8-daf9-e55d0cc92f3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.93      0.65     12500\n",
            "           1       0.54      0.08      0.15     12500\n",
            "\n",
            "    accuracy                           0.51     25000\n",
            "   macro avg       0.52      0.51      0.40     25000\n",
            "weighted avg       0.52      0.51      0.40     25000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With fine-tuning (With batch size of 4 and 10 epoch):"
      ],
      "metadata": {
        "id": "mTThyFMeR5D9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zb8yGDBycLdx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75186a29-c741-40b6-d6ef-b1c74e4b2a0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.91      0.93     12500\n",
            "           1       0.91      0.95      0.93     12500\n",
            "\n",
            "    accuracy                           0.93     25000\n",
            "   macro avg       0.93      0.93      0.93     25000\n",
            "weighted avg       0.93      0.93      0.93     25000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_test, bert_predicted))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wrong predicted attention:"
      ],
      "metadata": {
        "id": "oXHKWotHfB5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(y_test)):\n",
        "  i = i + 41\n",
        "  if y_test[i] != bert_predicted[i]:\n",
        "    print(\"Not equal\", i)\n",
        "    print(\"real label is:\", y_test[i])\n",
        "    print(\"predicted label is:\", bert_predicted[i])\n",
        "    break"
      ],
      "metadata": {
        "id": "RxO5QTxQUTGQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e4ec0b6-22e8-40f4-976f-4d3e7d71b452"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not equal 43\n",
            "real label is: 1\n",
            "predicted label is: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# attention_eval[1]\n",
        "x_test[43]"
      ],
      "metadata": {
        "id": "9esSUcnYJ-ZK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "93bf7d4d-7582-4082-bdfe-c0d064dafd2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I find it sad that just because Edward Norton did not want to be in the film or have anything to do with it, people automatically think the movie sucks without even watching it or giving it a chance. I really hope Norton did not do this. He is a fine actor and all but he scared people away from a decent movie.<br /><br />I found it entertaining. It wasn't mind blowing or anything with crazy special effects, but it was not a bad. It was fun to watch. But yea, definitely not a bad/horrible movie.<br /><br />7/10\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.convert_ids_to_tokens(test_tokens_ids[43])[:180]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b538f789-004e-415d-e9e0-b803009b3288",
        "id": "OmzKKWV0Z0yP"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " 'i',\n",
              " 'find',\n",
              " 'it',\n",
              " 'sad',\n",
              " 'that',\n",
              " 'just',\n",
              " 'because',\n",
              " 'edward',\n",
              " 'norton',\n",
              " 'did',\n",
              " 'not',\n",
              " 'want',\n",
              " 'to',\n",
              " 'be',\n",
              " 'in',\n",
              " 'the',\n",
              " 'film',\n",
              " 'or',\n",
              " 'have',\n",
              " 'anything',\n",
              " 'to',\n",
              " 'do',\n",
              " 'with',\n",
              " 'it',\n",
              " ',',\n",
              " 'people',\n",
              " 'automatically',\n",
              " 'think',\n",
              " 'the',\n",
              " 'movie',\n",
              " 'sucks',\n",
              " 'without',\n",
              " 'even',\n",
              " 'watching',\n",
              " 'it',\n",
              " 'or',\n",
              " 'giving',\n",
              " 'it',\n",
              " 'a',\n",
              " 'chance',\n",
              " '.',\n",
              " 'i',\n",
              " 'really',\n",
              " 'hope',\n",
              " 'norton',\n",
              " 'did',\n",
              " 'not',\n",
              " 'do',\n",
              " 'this',\n",
              " '.',\n",
              " 'he',\n",
              " 'is',\n",
              " 'a',\n",
              " 'fine',\n",
              " 'actor',\n",
              " 'and',\n",
              " 'all',\n",
              " 'but',\n",
              " 'he',\n",
              " 'scared',\n",
              " 'people',\n",
              " 'away',\n",
              " 'from',\n",
              " 'a',\n",
              " 'decent',\n",
              " 'movie',\n",
              " '.',\n",
              " '<',\n",
              " 'br',\n",
              " '/',\n",
              " '>',\n",
              " '<',\n",
              " 'br',\n",
              " '/',\n",
              " '>',\n",
              " 'i',\n",
              " 'found',\n",
              " 'it',\n",
              " 'entertaining',\n",
              " '.',\n",
              " 'it',\n",
              " 'wasn',\n",
              " \"'\",\n",
              " 't',\n",
              " 'mind',\n",
              " 'blowing',\n",
              " 'or',\n",
              " 'anything',\n",
              " 'with',\n",
              " 'crazy',\n",
              " 'special',\n",
              " 'effects',\n",
              " ',',\n",
              " 'but',\n",
              " 'it',\n",
              " 'was',\n",
              " 'not',\n",
              " 'a',\n",
              " 'bad',\n",
              " '.',\n",
              " 'it',\n",
              " 'was',\n",
              " 'fun',\n",
              " 'to',\n",
              " 'watch',\n",
              " '.',\n",
              " 'but',\n",
              " 'ye',\n",
              " '##a',\n",
              " ',',\n",
              " 'definitely',\n",
              " 'not',\n",
              " 'a',\n",
              " 'bad',\n",
              " '/',\n",
              " 'horrible',\n",
              " 'movie',\n",
              " '.',\n",
              " '<',\n",
              " 'br',\n",
              " '/',\n",
              " '>',\n",
              " '<',\n",
              " 'br',\n",
              " '/',\n",
              " '>',\n",
              " '7',\n",
              " '/',\n",
              " '10',\n",
              " '[SEP]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]']"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "# Visualize the attention weights using a heatmap\n",
        "#attention.data.numpy().argmax()\n",
        "a =attention_eval[43][:180,:180]\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "tokens = tokenizer.convert_ids_to_tokens(test_tokens_ids[43])[:180]\n",
        "plt.figure(figsize=(len(tokens), len(tokens)))\n",
        "ax = sns.heatmap(a, cmap='Blues', xticklabels=tokens, yticklabels=tokens, annot=True, cbar=False)\n",
        "ax.set_title('Attention Map')\n",
        "ax.set_xlabel('Input Tokens')\n",
        "ax.set_ylabel('Input Tokens')\n",
        "plt.savefig('attention_wrong.pdf', format='pdf', bbox_inches='tight')\n",
        "files.download('attention_wrong.pdf')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SnVqyX0uec0c"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "# Visualize the attention weights using a heatmap\n",
        "#attention.data.numpy().argmax()\n",
        "a =attention_eval[43][0:1,:180]\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "xtokens = tokenizer.convert_ids_to_tokens(test_tokens_ids[43])[:180]\n",
        "ytokens = tokenizer.convert_ids_to_tokens(test_tokens_ids[43])[0:1]\n",
        "plt.figure(figsize=(len(tokens), len(tokens)))\n",
        "ax = sns.heatmap(a, cmap='Blues', xticklabels=xtokens, yticklabels=ytokens, annot=True, cbar=False)\n",
        "ax.set_title('Attention Map')\n",
        "ax.set_xlabel('Input Tokens')\n",
        "ax.set_ylabel('Input Tokens')\n",
        "plt.savefig('attention_wrong.pdf', format='pdf', bbox_inches='tight')\n",
        "files.download('attention_wrong.pdf')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KbmTfVVTaBow"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Right predicted attention:"
      ],
      "metadata": {
        "id": "DZ_8gdXlP4T2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(y_test)):\n",
        "  i = i + 5\n",
        "  if y_test[i] == bert_predicted[i]:\n",
        "    print(\"equal\", i)\n",
        "    print(\"real label is:\", y_test[i])\n",
        "    print(\"predicted label is:\", bert_predicted[i])\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7516212f-2404-457b-cc93-3b77916a81fd",
        "id": "R5xXS9TOXuGL"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "equal 5\n",
            "real label is: 1\n",
            "predicted label is: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.convert_ids_to_tokens(test_tokens_ids[5])[:190]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Vl0UluL4XOe",
        "outputId": "7d7ecb62-5e44-4aad-ad81-00aab46fe527"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " 'my',\n",
              " 'god',\n",
              " ',',\n",
              " 'ryan',\n",
              " 'go',\n",
              " '##sling',\n",
              " 'has',\n",
              " 'made',\n",
              " 'a',\n",
              " 'lot',\n",
              " 'of',\n",
              " 'deep',\n",
              " 'characters',\n",
              " 'in',\n",
              " 'his',\n",
              " 'career',\n",
              " ',',\n",
              " 'this',\n",
              " 'is',\n",
              " 'one',\n",
              " 'of',\n",
              " 'his',\n",
              " 'wonderful',\n",
              " 'acting',\n",
              " 'jobs',\n",
              " '.',\n",
              " 'for',\n",
              " 'me',\n",
              " 'this',\n",
              " 'is',\n",
              " 'a',\n",
              " 'very',\n",
              " 'deep',\n",
              " 'movie',\n",
              " ',',\n",
              " 'needs',\n",
              " 'a',\n",
              " 'lot',\n",
              " 'of',\n",
              " 'concentration',\n",
              " ',',\n",
              " 'not',\n",
              " 'because',\n",
              " 'is',\n",
              " 'difficult',\n",
              " 'to',\n",
              " 'watch',\n",
              " ',',\n",
              " 'just',\n",
              " 'because',\n",
              " 'you',\n",
              " 'understand',\n",
              " 'it',\n",
              " 'if',\n",
              " 'you',\n",
              " 'put',\n",
              " 'your',\n",
              " 'shoes',\n",
              " 'in',\n",
              " 'this',\n",
              " 'kid',\n",
              " ',',\n",
              " 'even',\n",
              " 'though',\n",
              " 'has',\n",
              " 'everything',\n",
              " 'and',\n",
              " 'has',\n",
              " 'famous',\n",
              " 'father',\n",
              " 'that',\n",
              " 'is',\n",
              " 'a',\n",
              " 'writer',\n",
              " ',',\n",
              " 'has',\n",
              " 'a',\n",
              " 'deeper',\n",
              " 'mind',\n",
              " ',',\n",
              " 'you',\n",
              " 'don',\n",
              " \"'\",\n",
              " 't',\n",
              " 'understand',\n",
              " 'why',\n",
              " 'he',\n",
              " 'kills',\n",
              " 'this',\n",
              " 'poor',\n",
              " 'kid',\n",
              " ',',\n",
              " 'until',\n",
              " 'you',\n",
              " 'really',\n",
              " 'heard',\n",
              " 'what',\n",
              " 'he',\n",
              " 'has',\n",
              " 'to',\n",
              " 'say',\n",
              " 'and',\n",
              " 'you',\n",
              " 'start',\n",
              " 'to',\n",
              " 'think',\n",
              " ',',\n",
              " 'at',\n",
              " 'least',\n",
              " 'to',\n",
              " 'me',\n",
              " ',',\n",
              " 'that',\n",
              " 'a',\n",
              " 'lot',\n",
              " 'of',\n",
              " 'things',\n",
              " 'that',\n",
              " 'he',\n",
              " 'says',\n",
              " 'is',\n",
              " 'true',\n",
              " '.',\n",
              " 'simple',\n",
              " 'kid',\n",
              " ',',\n",
              " 'sweet',\n",
              " ',',\n",
              " 'very',\n",
              " 'gentle',\n",
              " ',',\n",
              " 'in',\n",
              " 'a',\n",
              " 'way',\n",
              " 'normal',\n",
              " 'like',\n",
              " 'any',\n",
              " 'teenage',\n",
              " ',',\n",
              " 'but',\n",
              " 'inside',\n",
              " 'of',\n",
              " 'him',\n",
              " 'suffer',\n",
              " 'because',\n",
              " 'he',\n",
              " 'start',\n",
              " 'to',\n",
              " 'look',\n",
              " 'at',\n",
              " 'the',\n",
              " 'world',\n",
              " 'in',\n",
              " 'a',\n",
              " 'different',\n",
              " 'way',\n",
              " ',',\n",
              " 'then',\n",
              " 'you',\n",
              " 'understand',\n",
              " 'why',\n",
              " 'he',\n",
              " 'did',\n",
              " 'what',\n",
              " 'he',\n",
              " 'did',\n",
              " '.',\n",
              " 'i',\n",
              " 'recommend',\n",
              " 'this',\n",
              " 'movie',\n",
              " 'for',\n",
              " 'those',\n",
              " 'who',\n",
              " 'likes',\n",
              " 'deep',\n",
              " 'drama',\n",
              " '.',\n",
              " '[SEP]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]']"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "# Visualize the attention weights using a heatmap\n",
        "#attention.data.numpy().argmax()\n",
        "a =attention_eval[5][:190,:190]\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "tokens = tokenizer.convert_ids_to_tokens(test_tokens_ids[5])[:190]\n",
        "plt.figure(figsize=(len(tokens), len(tokens)))\n",
        "ax = sns.heatmap(a, cmap='Blues', xticklabels=tokens, yticklabels=tokens, annot=True, cbar=False)\n",
        "ax.set_title('Attention Map')\n",
        "ax.set_xlabel('Input Tokens')\n",
        "ax.set_ylabel('Input Tokens')\n",
        "plt.savefig('attention_right.pdf', format='pdf', bbox_inches='tight')\n",
        "files.download('attention_right.pdf')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rKLDG_80XuGL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "# Visualize the attention weights using a heatmap\n",
        "#attention.data.numpy().argmax()\n",
        "a =attention_eval[5][0:1,:190]\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "xtokens = tokenizer.convert_ids_to_tokens(test_tokens_ids[5])[:190]\n",
        "ytokens = tokenizer.convert_ids_to_tokens(test_tokens_ids[5])[0:1]\n",
        "plt.figure(figsize=(len(tokens), len(tokens)))\n",
        "ax = sns.heatmap(a, cmap='Blues', xticklabels=xtokens, yticklabels=ytokens, annot=True, cbar=False)\n",
        "ax.set_title('Attention Map')\n",
        "ax.set_xlabel('Input Tokens')\n",
        "ax.set_ylabel('Input Tokens')\n",
        "plt.savefig('attention_right.pdf', format='pdf', bbox_inches='tight')\n",
        "files.download('attention_right.pdf')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "F7PjV7AlcKZW"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "doc-Pj0kE9Yq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}